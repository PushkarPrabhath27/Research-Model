{
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GriceBench Phase 2: MRR Evaluation & Relation Repair Assessment\n",
                "\n",
                "This notebook evaluates the retrieval-based Relation repair system using:\n",
                "1. **MRR (Mean Reciprocal Rank)** - per morechanges.md lines 775-812\n",
                "2. **Relevance Scores** - semantic similarity metrics\n",
                "\n",
                "## Required Datasets to Add:\n",
                "\n",
                "Add these to your Kaggle notebook:\n",
                "\n",
                "1. **gricebench-scientific-fix** (your private dataset):\n",
                "   - `relation_eval_set.json` (from Phase 1)\n",
                "   - `topical_corpus.json`\n",
                "   - `repair_data/repair_test.json`\n",
                "\n",
                "2. **sentence-transformers/all-MiniLM-L6-v2** (HuggingFace model)\n",
                "\n",
                "Mount paths:\n",
                "- `/kaggle/input/gricebench-scientific-fix/`\n",
                "- `/kaggle/input/all-minilm-l6-v2/`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Install dependencies\n",
                "!pip install -q sentence-transformers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Configuration\n",
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "import random\n",
                "from pathlib import Path\n",
                "from typing import Dict, List\n",
                "import re\n",
                "\n",
                "# Paths\n",
                "DATA_INPUT = Path(\"/kaggle/input/gricebench-scientific-fix\")\n",
                "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
                "\n",
                "# Check data\n",
                "print(\"Checking datasets...\")\n",
                "for path in DATA_INPUT.iterdir():\n",
                "    print(f\"  - {path.name}\")\n",
                "\n",
                "# Model path (if using local model)\n",
                "MODEL_PATH = \"/kaggle/input/all-minilm-l6-v2\"\n",
                "if not Path(MODEL_PATH).exists():\n",
                "    MODEL_PATH = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
                "    print(f\"\\nUsing HuggingFace model: {MODEL_PATH}\")\n",
                "else:\n",
                "    print(f\"\\nUsing local model: {MODEL_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Load sentence encoder\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "print(\"Loading sentence encoder...\")\n",
                "encoder = SentenceTransformer(MODEL_PATH)\n",
                "print(\"‚úÖ Encoder loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Load evaluation data\n",
                "\n",
                "# Try to load relation eval set (from Phase 1)\n",
                "eval_set_path = DATA_INPUT / \"relation_eval_set.json\"\n",
                "if not eval_set_path.exists():\n",
                "    # Fallback: create from repair_test.json\n",
                "    print(\"relation_eval_set.json not found, creating from repair_test.json...\")\n",
                "    repair_test_path = DATA_INPUT / \"repair_data\" / \"repair_test.json\"\n",
                "    if not repair_test_path.exists():\n",
                "        repair_test_path = DATA_INPUT / \"repair_test.json\"\n",
                "    \n",
                "    with open(repair_test_path, 'r') as f:\n",
                "        test_data = json.load(f)\n",
                "    \n",
                "    # Filter Relation violations\n",
                "    relation_examples = []\n",
                "    for i, item in enumerate(test_data):\n",
                "        input_text = item.get(\"input_text\", \"\")\n",
                "        if \"[VIOLATION=RELATION]\" in input_text:\n",
                "            example = {\n",
                "                \"id\": f\"relation_eval_{i}\",\n",
                "                \"input_text\": input_text,\n",
                "                \"target_text\": item.get(\"target_text\", \"\"),\n",
                "            }\n",
                "            context_match = re.search(r'\\[CONTEXT\\](.*?)\\[', input_text, re.DOTALL)\n",
                "            response_match = re.search(r'\\[RESPONSE\\](.*?)$', input_text, re.DOTALL)\n",
                "            if context_match:\n",
                "                example[\"context\"] = context_match.group(1).strip()\n",
                "            if response_match:\n",
                "                example[\"response\"] = response_match.group(1).strip()\n",
                "            relation_examples.append(example)\n",
                "    \n",
                "    random.seed(42)\n",
                "    eval_data = random.sample(relation_examples, min(200, len(relation_examples)))\n",
                "    print(f\"  Created {len(eval_data)} examples\")\n",
                "else:\n",
                "    with open(eval_set_path, 'r') as f:\n",
                "        eval_data = json.load(f)\n",
                "    print(f\"Loaded {len(eval_data)} examples from relation_eval_set.json\")\n",
                "\n",
                "# Load corpus\n",
                "corpus_path = DATA_INPUT / \"topical_corpus.json\"\n",
                "with open(corpus_path, 'r') as f:\n",
                "    corpus = json.load(f)\n",
                "\n",
                "# Extract responses\n",
                "if isinstance(corpus[0], dict):\n",
                "    corpus_responses = [item.get('response', str(item)) for item in corpus]\n",
                "else:\n",
                "    corpus_responses = corpus\n",
                "\n",
                "print(f\"Corpus size: {len(corpus_responses)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Encode corpus (subsample for efficiency)\n",
                "\n",
                "MAX_CORPUS = 10000\n",
                "if len(corpus_responses) > MAX_CORPUS:\n",
                "    random.seed(42)\n",
                "    corpus_sample = random.sample(corpus_responses, MAX_CORPUS)\n",
                "else:\n",
                "    corpus_sample = corpus_responses\n",
                "\n",
                "print(f\"Encoding {len(corpus_sample)} corpus responses...\")\n",
                "corpus_embeddings = encoder.encode(\n",
                "    corpus_sample,\n",
                "    convert_to_numpy=True,\n",
                "    normalize_embeddings=True,\n",
                "    show_progress_bar=True,\n",
                "    batch_size=64\n",
                ")\n",
                "print(f\"‚úÖ Corpus encoded: {corpus_embeddings.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## MRR Evaluation\n",
                "\n",
                "Mean Reciprocal Rank measures how well the retrieval system finds relevant responses:\n",
                "- For each context, retrieve top-10 from corpus\n",
                "- Find rank of semantically similar response\n",
                "- MRR = mean(1/rank)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: MRR Evaluation\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"MRR EVALUATION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "mrr_scores = []\n",
                "top1_hits = 0\n",
                "top3_hits = 0\n",
                "top10_hits = 0\n",
                "\n",
                "for i, item in enumerate(eval_data):\n",
                "    if (i + 1) % 50 == 0:\n",
                "        print(f\"  Processed {i + 1}/{len(eval_data)}\")\n",
                "    \n",
                "    # Get context\n",
                "    context = item.get('context', '')\n",
                "    if not context and 'input_text' in item:\n",
                "        match = re.search(r'\\[CONTEXT\\](.*?)\\[', item['input_text'], re.DOTALL)\n",
                "        if match:\n",
                "            context = match.group(1).strip()\n",
                "    \n",
                "    if not context:\n",
                "        mrr_scores.append(0.0)\n",
                "        continue\n",
                "    \n",
                "    # Get true response\n",
                "    true_response = item.get('target_text', item.get('response', ''))\n",
                "    \n",
                "    # Encode context\n",
                "    context_embedding = encoder.encode(\n",
                "        [context],\n",
                "        convert_to_numpy=True,\n",
                "        normalize_embeddings=True\n",
                "    )[0]\n",
                "    \n",
                "    # Find top-10 from corpus\n",
                "    similarities = np.dot(corpus_embeddings, context_embedding)\n",
                "    top_indices = np.argsort(similarities)[-10:][::-1]\n",
                "    \n",
                "    # Encode true response\n",
                "    true_embedding = encoder.encode(\n",
                "        [true_response],\n",
                "        convert_to_numpy=True,\n",
                "        normalize_embeddings=True\n",
                "    )[0]\n",
                "    \n",
                "    # Find rank of similar response\n",
                "    rank = None\n",
                "    for j, idx in enumerate(top_indices):\n",
                "        candidate_embedding = corpus_embeddings[idx]\n",
                "        sim_to_true = np.dot(candidate_embedding, true_embedding)\n",
                "        if sim_to_true > 0.7:  # Threshold for \"relevant\"\n",
                "            rank = j + 1\n",
                "            break\n",
                "    \n",
                "    if rank:\n",
                "        mrr_scores.append(1.0 / rank)\n",
                "        if rank == 1:\n",
                "            top1_hits += 1\n",
                "        if rank <= 3:\n",
                "            top3_hits += 1\n",
                "        if rank <= 10:\n",
                "            top10_hits += 1\n",
                "    else:\n",
                "        mrr_scores.append(0.0)\n",
                "\n",
                "# Calculate metrics\n",
                "n = len(eval_data)\n",
                "mrr = np.mean(mrr_scores)\n",
                "\n",
                "results = {\n",
                "    'mrr': float(mrr),\n",
                "    'top1_accuracy': top1_hits / n if n > 0 else 0,\n",
                "    'top3_accuracy': top3_hits / n if n > 0 else 0,\n",
                "    'top10_accuracy': top10_hits / n if n > 0 else 0,\n",
                "    'n_examples': n\n",
                "}\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"MRR RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"MRR:          {results['mrr']:.4f}\")\n",
                "print(f\"Top-1:        {results['top1_accuracy']:.4f}\")\n",
                "print(f\"Top-3:        {results['top3_accuracy']:.4f}\")\n",
                "print(f\"Top-10:       {results['top10_accuracy']:.4f}\")\n",
                "print(f\"Examples:     {results['n_examples']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Interpretation & Verdict\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"VERDICT\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "if results['mrr'] >= 0.7:\n",
                "    verdict = \"EXCELLENT\"\n",
                "    action = \"Retrieval system is working well. Proceed to Phase 3.\"\n",
                "    emoji = \"‚úÖ\"\n",
                "elif results['mrr'] >= 0.5:\n",
                "    verdict = \"ACCEPTABLE\"\n",
                "    action = \"Retrieval acceptable but could improve. Consider better embeddings.\"\n",
                "    emoji = \"‚ö†Ô∏è\"\n",
                "else:\n",
                "    verdict = \"NEEDS IMPROVEMENT\"\n",
                "    action = \"Retrieval below threshold. Recommend: use all-mpnet-base-v2 or larger corpus.\"\n",
                "    emoji = \"‚ùå\"\n",
                "\n",
                "print(f\"\\n{emoji} {verdict}\")\n",
                "print(f\"\\nRecommendation: {action}\")\n",
                "\n",
                "# Decision point per morechanges.md\n",
                "print(\"\\n\" + \"-\" * 50)\n",
                "print(\"Decision Point:\")\n",
                "if results['mrr'] >= 0.5:\n",
                "    print(\"‚úÖ MRR >= 0.5: Proceed to Phase 3 (Annotation)\")\n",
                "else:\n",
                "    print(\"‚ùå MRR < 0.5: Fix retrieval before proceeding\")\n",
                "    print(\"   Options:\")\n",
                "    print(\"   1. Use better encoder (all-mpnet-base-v2)\")\n",
                "    print(\"   2. Expand corpus with more examples\")\n",
                "    print(\"   3. Use GPT-2 fallback for Relation repair\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Save results\n",
                "\n",
                "output_path = OUTPUT_DIR / \"relation_repair_mrr.json\"\n",
                "with open(output_path, 'w') as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "\n",
                "print(f\"\\n‚úÖ Results saved to {output_path}\")\n",
                "print(\"\\nüì• Download this file for your records.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sample Analysis\n",
                "\n",
                "View some example retrievals to understand performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: View sample retrievals\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"SAMPLE RETRIEVALS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Show 5 random examples\n",
                "sample_indices = random.sample(range(len(eval_data)), min(5, len(eval_data)))\n",
                "\n",
                "for idx in sample_indices:\n",
                "    item = eval_data[idx]\n",
                "    context = item.get('context', '')\n",
                "    if not context and 'input_text' in item:\n",
                "        match = re.search(r'\\[CONTEXT\\](.*?)\\[', item['input_text'], re.DOTALL)\n",
                "        if match:\n",
                "            context = match.group(1).strip()\n",
                "    \n",
                "    # Get top-3 retrievals\n",
                "    context_embedding = encoder.encode([context], normalize_embeddings=True)[0]\n",
                "    similarities = np.dot(corpus_embeddings, context_embedding)\n",
                "    top_indices = np.argsort(similarities)[-3:][::-1]\n",
                "    \n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"CONTEXT: {context[:200]}...\" if len(context) > 200 else f\"CONTEXT: {context}\")\n",
                "    print(f\"\\nTOP-3 RETRIEVALS:\")\n",
                "    for rank, corpus_idx in enumerate(top_indices, 1):\n",
                "        retrieved = corpus_sample[corpus_idx]\n",
                "        sim = similarities[corpus_idx]\n",
                "        print(f\"  {rank}. [sim={sim:.3f}] {retrieved[:100]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 10: Summary\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"PHASE 2 MRR EVALUATION COMPLETE\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(f\"\"\"\n",
                "Summary:\n",
                "--------\n",
                "MRR Score: {results['mrr']:.4f}\n",
                "Top-1 Accuracy: {results['top1_accuracy']:.2%}\n",
                "Top-3 Accuracy: {results['top3_accuracy']:.2%}\n",
                "Top-10 Accuracy: {results['top10_accuracy']:.2%}\n",
                "\n",
                "Next Steps:\n",
                "-----------\n",
                "1. Download relation_repair_mrr.json\n",
                "2. If MRR >= 0.5, proceed to human annotation (Phase 3)\n",
                "3. If MRR < 0.5, run improvement notebook first\n",
                "\"\"\")"
            ]
        }
    ]
}
