{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Synthetic Generation - LOCAL (No API Needed!)\n",
                "\n",
                "**100% Free - No Rate Limits - No API Key Needed**\n",
                "\n",
                "Runs Llama 3.2 3B directly on Kaggle GPU.\n",
                "\n",
                "**Setup:**\n",
                "1. Enable GPU: Settings â†’ Accelerator â†’ GPU T4 x2\n",
                "2. Upload scored_data.json dataset\n",
                "3. Run all cells"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install packages\n",
                "!pip install -q transformers torch accelerate bitsandbytes tqdm\n",
                "\n",
                "import os, json, time, torch\n",
                "from tqdm import tqdm\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
                "print(\"âœ… Ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Model LOCALLY (runs on Kaggle GPU)\n",
                "print(\"Loading Llama 3.2 3B locally...\")\n",
                "\n",
                "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
                "\n",
                "# 4-bit quantization for efficiency\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_compute_dtype=torch.float16\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "\n",
                "print(\"âœ… Model loaded on GPU\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "INPUT_FILE = None\n",
                "for p in [\"/kaggle/input/gricebench-scored/scored_data.json\", \"/kaggle/input/scored-data/scored_data.json\"]:\n",
                "    if os.path.exists(p): INPUT_FILE = p; break\n",
                "\n",
                "if not INPUT_FILE:\n",
                "    raise FileNotFoundError(\"Upload scored_data.json dataset!\")\n",
                "\n",
                "OUTPUT_FILE = \"/kaggle/working/synthetic_candidates.json\"\n",
                "print(f\"Input: {INPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generation Function\n",
                "SYSTEM = \"\"\"You are a Gricean Cooperative Dialogue Assistant specialized in generating maximally cooperative conversational responses.\n",
                "\n",
                "Your responses MUST strictly satisfy ALL FOUR Gricean Maxims:\n",
                "\n",
                "**1. QUANTITY** - Provide exactly the information needed\n",
                "**2. QUALITY** - Only state what you believe to be true  \n",
                "**3. RELATION** - Stay strictly on-topic\n",
                "**4. MANNER** - Be clear, concise, and polite\n",
                "\n",
                "Generate a response that would score positively on all four maxims.\"\"\"\n",
                "\n",
                "def generate_response(prompt):\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM},\n",
                "        {\"role\": \"user\", \"content\": prompt}\n",
                "    ]\n",
                "    \n",
                "    inputs = tokenizer.apply_chat_template(\n",
                "        messages,\n",
                "        return_tensors=\"pt\",\n",
                "        add_generation_prompt=True\n",
                "    ).to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            inputs,\n",
                "            max_new_tokens=1024,\n",
                "            temperature=0.7,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
                "    return response.strip()\n",
                "\n",
                "def get_failed(path):\n",
                "    with open(path) as f: data = json.load(f)\n",
                "    return [e for e in data if not all(e.get('margins',{}).get(m,0)>0 for m in ['quantity','quality','relation','manner'])]\n",
                "\n",
                "def run():\n",
                "    all_cands = get_failed(INPUT_FILE)\n",
                "    print(f\"Target: {len(all_cands)}\")\n",
                "    \n",
                "    done = []\n",
                "    if os.path.exists(OUTPUT_FILE):\n",
                "        try:\n",
                "            with open(OUTPUT_FILE) as f: done = json.load(f)\n",
                "            print(f\"Resume: {len(done)} done\")\n",
                "        except: pass\n",
                "    \n",
                "    done_prompts = {d['prompt'] for d in done}\n",
                "    todos = [c for c in all_cands if c['prompt'] not in done_prompts]\n",
                "    print(f\"Remaining: {len(todos)}\\nStarting...\\n\")\n",
                "\n",
                "    success = 0\n",
                "    errors = 0\n",
                "    \n",
                "    try:\n",
                "        for i, item in enumerate(tqdm(todos)):\n",
                "            try:\n",
                "                text = generate_response(item['prompt'])\n",
                "                \n",
                "                res = item.copy()\n",
                "                res['synthetic_chosen'] = text\n",
                "                res['original_chosen_failed'] = item['chosen']\n",
                "                res['chosen'] = text\n",
                "                done.append(res)\n",
                "                success += 1\n",
                "                \n",
                "                # Detailed logging every 50\n",
                "                if (i+1) % 50 == 0:\n",
                "                    print(f\"\\n{'='*80}\")\n",
                "                    print(f\"âœ… CHECKPOINT @ {i+1}/{len(todos)} ({(i+1)/len(todos)*100:.1f}%)\")\n",
                "                    print(f\"Success: {success} | Errors: {errors}\")\n",
                "                    print(f\"\\nSample output:\\n{text[:200]}...\")\n",
                "                    print(f\"{'='*80}\\n\")\n",
                "                \n",
                "                # Autosave\n",
                "                if (i+1) % 10 == 0:\n",
                "                    with open(OUTPUT_FILE, 'w') as f: json.dump(done, f, indent=2)\n",
                "                \n",
                "            except Exception as e:\n",
                "                errors += 1\n",
                "                print(f\"\\nâŒ ERROR at {i}: {str(e)[:100]}\")\n",
                "                if errors > 20:\n",
                "                    print(\"Too many errors, stopping.\")\n",
                "                    break\n",
                "                \n",
                "    except KeyboardInterrupt:\n",
                "        print(\"\\nðŸ›‘ Stopped\")\n",
                "    finally:\n",
                "        with open(OUTPUT_FILE, 'w') as f: json.dump(done, f, indent=2)\n",
                "        print(f\"\\n{'='*80}\")\n",
                "        print(f\"FINAL: {len(done)} generated | Success: {success} | Errors: {errors}\")\n",
                "        print(f\"Saved: {OUTPUT_FILE}\")\n",
                "        print(f\"{'='*80}\\n\")\n",
                "\n",
                "run()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"},
  "language_info\": {\"name\": \"python\", \"version\": \"3.10.0\"}
        },
 "nbformat\": 4,
 "nbformat_minor\": 4
    }