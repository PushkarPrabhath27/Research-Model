{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Score Synthetic Candidates\n",
                "\n",
                "**Goal:** Validate the Gemini-generated responses using our locally trained Gricean reward models.\n",
                "\n",
                "**Instructions:**\n",
                "1. **Upload Data:** Add `synthetic_candidates.json` (downloaded from generation step) to this notebook.\n",
                "2. **Add Models:** Add your `grice-reward-models` dataset.\n",
                "3. **Run All:** This will calculate margins for the new synthetic pairs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers torch safetensors tqdm\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "import json\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Candidates\n",
                "# Look for synthetic_candidates.json\n",
                "CANDIDATES_PATH = None\n",
                "possible_paths = [\n",
                "    \"/kaggle/input/synthetic-candidates/synthetic_candidates.json\",\n",
                "    \"synthetic_candidates.json\"\n",
                "]\n",
                "for p in possible_paths:\n",
                "    if os.path.exists(p):\n",
                "        CANDIDATES_PATH = p\n",
                "        break\n",
                "\n",
                "if not CANDIDATES_PATH:\n",
                "    print(\"‚ùå synthetic_candidates.json not found!\")\n",
                "    # Stop or use dummy for testing\n",
                "\n",
                "with open(CANDIDATES_PATH, 'r') as f:\n",
                "    candidates = json.load(f)\n",
                "\n",
                "print(f\"Loaded {len(candidates)} candidates for scoring.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Reward Models\n",
                "MODELS_DIR = \"/kaggle/input/grice-reward-models/\"\n",
                "MAXIMS = ['quantity', 'quality', 'relation', 'manner']\n",
                "models = {}\n",
                "tokenizers = {}\n",
                "\n",
                "for maxim in MAXIMS:\n",
                "    path = os.path.join(MODELS_DIR, f\"reward_model_{maxim}\")\n",
                "    print(f\"Loading {maxim} model from {path}...\")\n",
                "    try:\n",
                "        models[maxim] = AutoModelForSequenceClassification.from_pretrained(path).to(device).eval()\n",
                "        tokenizers[maxim] = AutoTokenizer.from_pretrained(path)\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading {maxim}: {e}\")\n",
                "\n",
                "def get_score(maxim, text):\n",
                "    inputs = tokenizers[maxim](text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
                "    with torch.no_grad():\n",
                "        outputs = models[maxim](**inputs)\n",
                "        # Assuming scalar output from reward model\n",
                "        score = outputs.logits[0].item()\n",
                "    return score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Score Generation Loop\n",
                "scored_results = []\n",
                "\n",
                "for item in tqdm(candidates):\n",
                "    prompt = item['prompt']\n",
                "    chosen = item['synthetic_chosen'] # The new cooperative response\n",
                "    rejected = item['original_chosen_failed'] # The old bad response\n",
                "    \n",
                "    # Construction for scoring (Prompt + Response)\n",
                "    # Note: Adjust format based on how reward models were trained (usually Input + Response)\n",
                "    text_chosen = f\"{prompt}\\n{chosen}\"\n",
                "    text_rejected = f\"{prompt}\\n{rejected}\"\n",
                "    \n",
                "    margins = {}\n",
                "    valid = True\n",
                "    \n",
                "    for maxim in MAXIMS:\n",
                "        s_chosen = get_score(maxim, text_chosen)\n",
                "        s_rejected = get_score(maxim, text_rejected)\n",
                "        margin = s_chosen - s_rejected\n",
                "        margins[maxim] = margin\n",
                "        \n",
                "        if margin <= 0:\n",
                "            valid = False\n",
                "            \n",
                "    item['synthetic_margins'] = margins\n",
                "    item['synthetic_valid'] = valid\n",
                "    scored_results.append(item)\n",
                "\n",
                "print(f\"Scoring complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Filter and Save\n",
                "final_clean = [r for r in scored_results if r['synthetic_valid']]\n",
                "print(f\"Pass Rate: {len(final_clean)}/{len(scored_results)} ({len(final_clean)/len(scored_results):.1%})\")\n",
                "\n",
                "with open(\"synthetic_clean_pairs.json\", 'w') as f:\n",
                "    json.dump(final_clean, f, indent=2)\n",
                "\n",
                "print(\"Saved synthetic_clean_pairs.json\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}