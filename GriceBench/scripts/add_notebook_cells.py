"""
Script to add remaining cells to the robust Phase 6 notebook
"""
import json

# Load existing notebook
with open(r'c:\Users\pushk\OneDrive\Documents\Research Model\GriceBench\KAGGLE_PHASE6_ROBUST.ipynb', 'r', encoding='utf-8') as f:
    nb = json.load(f)

# Cell 6: Tokenizer
cell6 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 6: Load Tokenizer\n",
        "# ============================================================================\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"LOADING TOKENIZER\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CONFIG.model_name)\n",
        "logger.info(f\"Tokenizer: {CONFIG.model_name}\")\n",
        "logger.info(f\"Vocab size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Test tokenization\n",
        "sample_text = train_data[0].text[:200]\n",
        "tokens = tokenizer(sample_text, truncation=True, max_length=CONFIG.max_length)\n",
        "logger.info(f\"Sample tokenization: {len(tokens['input_ids'])} tokens\")\n",
        "\n",
        "tracker.mark('Tokenizer Load', 'PASS')\n",
        "print(\"\\nCELL 6 COMPLETE: Tokenizer ready\")"
    ]
}

# Cell 7: Datasets
cell7 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 7: Create PyTorch Datasets\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"CREATING PYTORCH DATASETS\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "class GriceDataset(Dataset):\n",
        "    def __init__(self, examples: List[Example], tokenizer, max_length: int):\n",
        "        self.examples = examples\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        ex = self.examples[idx]\n",
        "        \n",
        "        encoding = self.tokenizer(\n",
        "            ex.text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(ex.labels, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = GriceDataset(train_data, tokenizer, CONFIG.max_length)\n",
        "val_dataset = GriceDataset(val_data, tokenizer, CONFIG.max_length)\n",
        "test_dataset = GriceDataset(test_data, tokenizer, CONFIG.max_length)\n",
        "\n",
        "logger.info(f\"Datasets created:\")\n",
        "logger.info(f\"  Train: {len(train_dataset)}\")\n",
        "logger.info(f\"  Val: {len(val_dataset)}\")\n",
        "logger.info(f\"  Test: {len(test_dataset)}\")\n",
        "\n",
        "# Verify a batch\n",
        "sample = train_dataset[0]\n",
        "logger.info(f\"\\nSample batch shape:\")\n",
        "logger.info(f\"  input_ids: {sample['input_ids'].shape}\")\n",
        "logger.info(f\"  attention_mask: {sample['attention_mask'].shape}\")\n",
        "logger.info(f\"  labels: {sample['labels'].tolist()}\")\n",
        "\n",
        "tracker.mark('Datasets Created', 'PASS')\n",
        "print(\"\\nCELL 7 COMPLETE: Datasets ready\")"
    ]
}

# Cell 8: DataLoaders
cell8 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 8: Create DataLoaders\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"CREATING DATALOADERS\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG.batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG.batch_size * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG.batch_size * 2,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "logger.info(f\"DataLoaders created:\")\n",
        "logger.info(f\"  Train batches: {len(train_loader)}\")\n",
        "logger.info(f\"  Val batches: {len(val_loader)}\")\n",
        "logger.info(f\"  Test batches: {len(test_loader)}\")\n",
        "logger.info(f\"  Effective batch size: {CONFIG.effective_batch}\")\n",
        "\n",
        "tracker.mark('DataLoaders Created', 'PASS')\n",
        "print(\"\\nCELL 8 COMPLETE: DataLoaders ready\")"
    ]
}

# Cell 9: Model Definition
cell9 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 9: Model Definition with Dynamic pos_weight\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"CREATING MODEL\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "class MultiLabelDetector(nn.Module):\n",
        "    \"\"\"Multi-label violation detector with stored pos_weight\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str, num_labels: int = 4, pos_weight: torch.Tensor = None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size // 2, num_labels)\n",
        "        )\n",
        "        \n",
        "        # CRITICAL: Store pos_weight as buffer (persists with model)\n",
        "        if pos_weight is None:\n",
        "            pos_weight = torch.ones(num_labels)\n",
        "        self.register_buffer('pos_weight', pos_weight)\n",
        "        \n",
        "        self.num_labels = num_labels\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "        logits = self.classifier(pooled)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = F.binary_cross_entropy_with_logits(\n",
        "                logits, labels, pos_weight=self.pos_weight\n",
        "            )\n",
        "        \n",
        "        return {'loss': loss, 'logits': logits}\n",
        "\n",
        "# Calculate pos_weight from training data\n",
        "logger.info(\"\\nCalculating pos_weight from training data:\")\n",
        "train_labels_np = np.array([ex.labels for ex in train_data])\n",
        "pos_weights = []\n",
        "\n",
        "for i, name in enumerate(maxim_names):\n",
        "    pos = train_labels_np[:, i].sum()\n",
        "    neg = len(train_labels_np) - pos\n",
        "    weight = neg / (pos + 1e-6)  # Avoid division by zero\n",
        "    pos_weights.append(weight)\n",
        "    logger.info(f\"  {name}: pos={int(pos)}, neg={int(neg)}, weight={weight:.2f}\")\n",
        "\n",
        "pos_weight_tensor = torch.tensor(pos_weights, dtype=torch.float32)\n",
        "\n",
        "# Create model\n",
        "model = MultiLabelDetector(\n",
        "    CONFIG.model_name,\n",
        "    num_labels=CONFIG.num_labels,\n",
        "    pos_weight=pos_weight_tensor\n",
        ").to(device)\n",
        "\n",
        "# Verify pos_weight is stored\n",
        "logger.info(f\"\\nModel pos_weight: {model.pos_weight.tolist()}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "logger.info(f\"\\nModel parameters:\")\n",
        "logger.info(f\"  Total: {total_params:,}\")\n",
        "logger.info(f\"  Trainable: {trainable_params:,}\")\n",
        "\n",
        "tracker.mark('Model Created', 'PASS', {\n",
        "    'params': f\"{total_params:,}\",\n",
        "    'pos_weight': [f\"{w:.2f}\" for w in pos_weights]\n",
        "})\n",
        "print(\"\\nCELL 9 COMPLETE: Model ready with pos_weight\")"
    ]
}

# Cell 10: Training Setup
cell10 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 10: Training Setup (Optimizer, Scheduler, Scaler)\n",
        "# ============================================================================\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"TRAINING SETUP\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG.learning_rate,\n",
        "    weight_decay=CONFIG.weight_decay\n",
        ")\n",
        "\n",
        "# Scheduler\n",
        "num_training_steps = len(train_loader) * CONFIG.num_epochs // CONFIG.gradient_accumulation\n",
        "num_warmup_steps = int(num_training_steps * CONFIG.warmup_ratio)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = torch.amp.GradScaler('cuda', enabled=CONFIG.fp16)\n",
        "\n",
        "logger.info(f\"Optimizer: AdamW (lr={CONFIG.learning_rate})\")\n",
        "logger.info(f\"Training steps: {num_training_steps}\")\n",
        "logger.info(f\"Warmup steps: {num_warmup_steps}\")\n",
        "logger.info(f\"Mixed precision: {CONFIG.fp16}\")\n",
        "\n",
        "tracker.mark('Training Setup', 'PASS')\n",
        "print(\"\\nCELL 10 COMPLETE: Training setup ready\")"
    ]
}

# Cell 11: Evaluation Function
cell11 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 11: Evaluation Function with Detailed Metrics\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate(model, dataloader, thresholds=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate model with detailed metrics per maxim.\n",
        "    Returns: macro_f1, per_class_scores, all_probs, all_labels\n",
        "    \"\"\"\n",
        "    if thresholds is None:\n",
        "        thresholds = [0.5, 0.5, 0.5, 0.5]\n",
        "    \n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            \n",
        "            outputs = model(input_ids, attention_mask, labels)\n",
        "            total_loss += outputs['loss'].item()\n",
        "            \n",
        "            probs = torch.sigmoid(outputs['logits']).cpu().numpy()\n",
        "            all_probs.extend(probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    \n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "    \n",
        "    # Apply thresholds\n",
        "    all_preds = (all_probs >= np.array(thresholds)).astype(int)\n",
        "    \n",
        "    # Calculate per-class metrics\n",
        "    results = {}\n",
        "    f1_scores = []\n",
        "    \n",
        "    for i, name in enumerate(maxim_names):\n",
        "        f1 = f1_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n",
        "        p = precision_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n",
        "        r = recall_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n",
        "        \n",
        "        f1_scores.append(f1)\n",
        "        results[name] = {'f1': f1, 'precision': p, 'recall': r}\n",
        "        \n",
        "        if verbose:\n",
        "            logger.info(f\"  {name}: F1={f1:.3f} (P={p:.3f}, R={r:.3f})\")\n",
        "    \n",
        "    macro_f1 = np.mean(f1_scores)\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    \n",
        "    if verbose:\n",
        "        logger.info(f\"  Macro F1: {macro_f1:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'macro_f1': macro_f1,\n",
        "        'loss': avg_loss,\n",
        "        'per_class': results,\n",
        "        'all_probs': all_probs,\n",
        "        'all_labels': all_labels\n",
        "    }\n",
        "\n",
        "logger.info(\"Evaluation function defined\")\n",
        "print(\"\\nCELL 11 COMPLETE: Evaluation function ready\")"
    ]
}

# Cell 12: Training Loop
cell12 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 12: Training Loop with Early Stopping\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"STARTING TRAINING\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "# Training state\n",
        "best_f1 = 0.0\n",
        "best_epoch = 0\n",
        "patience_counter = 0\n",
        "training_history = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(CONFIG.num_epochs):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    # Training\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    for step, batch in enumerate(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        \n",
        "        # Forward pass with mixed precision\n",
        "        with torch.amp.autocast('cuda', enabled=CONFIG.fp16):\n",
        "            outputs = model(input_ids, attention_mask, labels)\n",
        "            loss = outputs['loss'] / CONFIG.gradient_accumulation\n",
        "        \n",
        "        # Backward pass\n",
        "        scaler.scale(loss).backward()\n",
        "        total_train_loss += loss.item() * CONFIG.gradient_accumulation\n",
        "        \n",
        "        # Optimizer step after accumulation\n",
        "        if (step + 1) % CONFIG.gradient_accumulation == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG.max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "    # Evaluation\n",
        "    logger.info(f\"\\n{'='*60}\")\n",
        "    logger.info(f\"EPOCH {epoch + 1}/{CONFIG.num_epochs}\")\n",
        "    logger.info(f\"{'='*60}\")\n",
        "    logger.info(f\"Train Loss: {avg_train_loss:.4f} | Time: {epoch_time:.1f}s\")\n",
        "    logger.info(f\"\\nValidation Results:\")\n",
        "    \n",
        "    eval_results = evaluate(model, val_loader)\n",
        "    \n",
        "    # Track history\n",
        "    training_history.append({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': avg_train_loss,\n",
        "        'val_loss': eval_results['loss'],\n",
        "        'val_macro_f1': eval_results['macro_f1'],\n",
        "        'per_class': eval_results['per_class']\n",
        "    })\n",
        "    \n",
        "    # Check for improvement\n",
        "    if eval_results['macro_f1'] > best_f1 + CONFIG.min_delta:\n",
        "        best_f1 = eval_results['macro_f1']\n",
        "        best_epoch = epoch + 1\n",
        "        patience_counter = 0\n",
        "        \n",
        "        # Save best model\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_f1': best_f1,\n",
        "            'pos_weight': model.pos_weight\n",
        "        }, f'{CONFIG.output_dir}/best_model.pt')\n",
        "        \n",
        "        logger.info(f\"\\n  ✅ New best model saved! (F1={best_f1:.4f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        logger.info(f\"\\n  No improvement ({patience_counter}/{CONFIG.patience})\")\n",
        "        \n",
        "        if patience_counter >= CONFIG.patience:\n",
        "            logger.info(f\"\\n⚠️ Early stopping triggered at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "logger.info(f\"\\n{'='*60}\")\n",
        "logger.info(f\"TRAINING COMPLETE\")\n",
        "logger.info(f\"{'='*60}\")\n",
        "logger.info(f\"Best Epoch: {best_epoch}\")\n",
        "logger.info(f\"Best Val F1: {best_f1:.4f}\")\n",
        "\n",
        "tracker.mark('Training Complete', 'PASS', {\n",
        "    'best_epoch': best_epoch,\n",
        "    'best_f1': f\"{best_f1:.4f}\"\n",
        "})\n",
        "\n",
        "print(f\"\\nCELL 12 COMPLETE: Training finished. Best F1={best_f1:.4f}\")"
    ]
}

# Cell 13: Threshold Optimization
cell13 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 13: Threshold Optimization\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"THRESHOLD OPTIMIZATION\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load(f'{CONFIG.output_dir}/best_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "logger.info(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
        "\n",
        "# Get predictions on validation set\n",
        "eval_results = evaluate(model, val_loader, verbose=False)\n",
        "all_probs = eval_results['all_probs']\n",
        "all_labels = eval_results['all_labels']\n",
        "\n",
        "# Find optimal thresholds\n",
        "optimal_thresholds = []\n",
        "\n",
        "logger.info(\"\\nFinding optimal thresholds per maxim:\")\n",
        "for i, name in enumerate(maxim_names):\n",
        "    best_f1 = 0\n",
        "    best_thresh = 0.5\n",
        "    \n",
        "    for thresh in np.arange(0.1, 0.9, 0.05):\n",
        "        preds = (all_probs[:, i] >= thresh).astype(int)\n",
        "        f1 = f1_score(all_labels[:, i], preds, zero_division=0)\n",
        "        \n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_thresh = thresh\n",
        "    \n",
        "    optimal_thresholds.append(best_thresh)\n",
        "    \n",
        "    # Compare with default\n",
        "    default_preds = (all_probs[:, i] >= 0.5).astype(int)\n",
        "    default_f1 = f1_score(all_labels[:, i], default_preds, zero_division=0)\n",
        "    \n",
        "    improvement = best_f1 - default_f1\n",
        "    logger.info(f\"  {name}: thresh={best_thresh:.2f} (F1: {default_f1:.3f} -> {best_f1:.3f}, +{improvement:.3f})\")\n",
        "\n",
        "# Final evaluation with optimal thresholds\n",
        "logger.info(\"\\n\" + \"=\"*60)\n",
        "logger.info(\"FINAL EVALUATION (Optimal Thresholds)\")\n",
        "logger.info(\"=\"*60)\n",
        "\n",
        "final_results = evaluate(model, val_loader, thresholds=optimal_thresholds)\n",
        "\n",
        "logger.info(f\"\\nMacro F1 with optimal thresholds: {final_results['macro_f1']:.4f}\")\n",
        "\n",
        "# Save thresholds\n",
        "threshold_config = {\n",
        "    'thresholds': {name: thresh for name, thresh in zip(maxim_names, optimal_thresholds)},\n",
        "    'macro_f1': final_results['macro_f1']\n",
        "}\n",
        "\n",
        "with open(f'{CONFIG.output_dir}/optimal_thresholds.json', 'w') as f:\n",
        "    json.dump(threshold_config, f, indent=2)\n",
        "\n",
        "tracker.mark('Threshold Optimization', 'PASS', {\n",
        "    'final_f1': f\"{final_results['macro_f1']:.4f}\"\n",
        "})\n",
        "\n",
        "print(f\"\\nCELL 13 COMPLETE: Optimal F1={final_results['macro_f1']:.4f}\")"
    ]
}

# Cell 14: Test Set Evaluation
cell14 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 14: Final Test Set Evaluation\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(\"=\" * 60)\n",
        "logger.info(\"TEST SET EVALUATION\")\n",
        "logger.info(\"=\" * 60)\n",
        "\n",
        "test_results = evaluate(model, test_loader, thresholds=optimal_thresholds)\n",
        "\n",
        "logger.info(f\"\\nTest Set Macro F1: {test_results['macro_f1']:.4f}\")\n",
        "\n",
        "# Save final results\n",
        "final_report = {\n",
        "    'model': CONFIG.model_name,\n",
        "    'best_epoch': best_epoch,\n",
        "    'thresholds': {name: thresh for name, thresh in zip(maxim_names, optimal_thresholds)},\n",
        "    'validation': {\n",
        "        'macro_f1': final_results['macro_f1'],\n",
        "        'per_class': final_results['per_class']\n",
        "    },\n",
        "    'test': {\n",
        "        'macro_f1': test_results['macro_f1'],\n",
        "        'per_class': test_results['per_class']\n",
        "    },\n",
        "    'training_history': training_history\n",
        "}\n",
        "\n",
        "with open(f'{CONFIG.output_dir}/detector_v2_results.json', 'w') as f:\n",
        "    json.dump(final_report, f, indent=2, default=str)\n",
        "\n",
        "tracker.mark('Test Evaluation', 'PASS', {\n",
        "    'test_f1': f\"{test_results['macro_f1']:.4f}\"\n",
        "})\n",
        "\n",
        "print(f\"\\nCELL 14 COMPLETE: Test F1={test_results['macro_f1']:.4f}\")"
    ]
}

# Cell 15: Final Summary
cell15 = {
    "cell_type": "code",
    "execution_count": None,
    "metadata": {},
    "outputs": [],
    "source": [
        "# ============================================================================\n",
        "# CELL 15: Final Summary\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PHASE 6 DETECTOR V2 TRAINING COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Checkpoint summary\n",
        "tracker.summary()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nValidation Macro F1: {final_results['macro_f1']:.4f}\")\n",
        "print(f\"Test Macro F1:       {test_results['macro_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\nPer-Class Test Results:\")\n",
        "for name, metrics in test_results['per_class'].items():\n",
        "    print(f\"  {name}: F1={metrics['f1']:.3f} (P={metrics['precision']:.3f}, R={metrics['recall']:.3f})\")\n",
        "\n",
        "print(f\"\\nOptimal Thresholds:\")\n",
        "for name, thresh in zip(maxim_names, optimal_thresholds):\n",
        "    print(f\"  {name}: {thresh:.2f}\")\n",
        "\n",
        "print(f\"\\nOutput Files:\")\n",
        "print(f\"  {CONFIG.output_dir}/best_model.pt\")\n",
        "print(f\"  {CONFIG.output_dir}/detector_v2_results.json\")\n",
        "print(f\"  {CONFIG.output_dir}/optimal_thresholds.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ ALL COMPLETE - Download results from /kaggle/working/\")\n",
        "print(\"=\"*70)"
    ]
}

# Add all cells
nb['cells'].extend([cell6, cell7, cell8, cell9, cell10, cell11, cell12, cell13, cell14, cell15])

# Save
with open(r'c:\Users\pushk\OneDrive\Documents\Research Model\GriceBench\KAGGLE_PHASE6_ROBUST.ipynb', 'w') as f:
    json.dump(nb, f, indent=4)

print(f"Added cells 6-15. Total cells: {len(nb['cells'])}")
