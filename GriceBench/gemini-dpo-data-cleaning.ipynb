{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GriceBench DPO Data Cleaning with Gemini API\n",
        "\n",
        "**Goal:** Fix Manner violations in DPO data using FREE Gemini API\n",
        "\n",
        "**Expected Results:**\n",
        "- Manner margin: -0.284 ‚Üí +0.180 (POSITIVE!)\n",
        "- Dataset size: ~3,500 clean pairs\n",
        "- All margins positive\n",
        "- Ready for single-stage DPO training\n",
        "\n",
        "**Prerequisites:**\n",
        "- Datasets added: `gricebench-detector-v2`, `gricebench-dpo-raw`\n",
        "- Secret added: `GEMINI_API_KEY`\n",
        "- GPU enabled\n",
        "- Internet enabled"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Setup & Imports\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CELL 1: SETUP & IMPORTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import time\n",
        "import google.generativeai as genai\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"\\n‚úÖ All imports successful\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"   Using device: {device}\")\n",
        "\n",
        "# Create directories\n",
        "Path(\"/kaggle/working/data\").mkdir(exist_ok=True)\n",
        "Path(\"/kaggle/working/analysis\").mkdir(exist_ok=True)\n",
        "Path(\"/kaggle/working/logs\").mkdir(exist_ok=True)\n",
        "\n",
        "print(\"\\n‚úÖ Directories created:\")\n",
        "print(\"   /kaggle/working/data/\")\n",
        "print(\"   /kaggle/working/analysis/\")\n",
        "print(\"   /kaggle/working/logs/\")\n",
        "\n",
        "# Start logging\n",
        "start_time = datetime.now()\n",
        "print(f\"\\n‚è∞ Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Load Detector V2\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 2: LOADING DETECTOR V2\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class MaximDetectorV2(nn.Module):\n",
        "    \"\"\"Detector V2 with deeper classification heads\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name, num_maxims=4, dropout=0.15):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "        \n",
        "        self.classifiers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_size, hidden_size // 2),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_size // 2, hidden_size // 4),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_size // 4, 1)\n",
        "            )\n",
        "            for _ in range(num_maxims)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        pooled = outputs.last_hidden_state[:, 0, :]\n",
        "        logits = torch.cat([\n",
        "            classifier(pooled)\n",
        "            for classifier in self.classifiers\n",
        "        ], dim=1)\n",
        "        return logits\n",
        "\n",
        "print(\"\\nüì¶ Loading model components...\")\n",
        "\n",
        "model_name = 'microsoft/deberta-v3-base'\n",
        "print(f\"   Base model: {model_name}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"   ‚úÖ Tokenizer loaded\")\n",
        "\n",
        "detector_model = MaximDetectorV2(model_name).to(device)\n",
        "print(\"   ‚úÖ Model architecture created\")\n",
        "\n",
        "# Load trained weights\n",
        "checkpoint_path = '/kaggle/input/gricebench-detector-v2/best_model_v2.pt'\n",
        "print(f\"\\nüì• Loading checkpoint from: {checkpoint_path}\")\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "detector_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "detector_model.eval()\n",
        "print(\"   ‚úÖ Weights loaded and model set to eval mode\")\n",
        "\n",
        "# Load temperatures\n",
        "temp_path = '/kaggle/input/gricebench-detector-v2/temperatures.json'\n",
        "print(f\"\\nüå°Ô∏è  Loading temperature scaling from: {temp_path}\")\n",
        "\n",
        "with open(temp_path) as f:\n",
        "    temperatures = json.load(f)\n",
        "\n",
        "print(\"   ‚úÖ Temperatures loaded:\")\n",
        "for maxim, temp in temperatures.items():\n",
        "    print(f\"      {maxim:10s}: {temp:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Detector V2 fully loaded and ready!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Define Scoring Function\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 3: DEFINING SCORING FUNCTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def score_response(context, response, evidence=None):\n",
        "    \"\"\"Score a response for maxim violations using Detector V2\"\"\"\n",
        "    \n",
        "    # Construct input text\n",
        "    if evidence:\n",
        "        text = f\"Context: {context} Evidence: {evidence} Response: {response}\"\n",
        "    else:\n",
        "        text = f\"Context: {context} Response: {response}\"\n",
        "    \n",
        "    # Tokenize\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "    \n",
        "    # Get logits\n",
        "    with torch.no_grad():\n",
        "        logits = detector_model(input_ids, attention_mask)\n",
        "    \n",
        "    # Apply temperature scaling and sigmoid\n",
        "    maxims = ['quantity', 'quality', 'relation', 'manner']\n",
        "    scores = {}\n",
        "    \n",
        "    for i, maxim in enumerate(maxims):\n",
        "        temp = temperatures[maxim]\n",
        "        scaled_logit = logits[0, i] / temp\n",
        "        prob = torch.sigmoid(scaled_logit).item()\n",
        "        scores[maxim] = prob\n",
        "    \n",
        "    return scores\n",
        "\n",
        "print(\"\\n‚úÖ Scoring function defined\")\n",
        "\n",
        "# Test it\n",
        "print(\"\\nüß™ Testing scoring function...\")\n",
        "test_context = \"What is the capital of France?\"\n",
        "test_response = \"Paris is the capital.\"\n",
        "test_scores = score_response(test_context, test_response)\n",
        "\n",
        "print(\"   Test scores:\")\n",
        "for maxim, score in test_scores.items():\n",
        "    print(f\"      {maxim:10s}: {score:.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Scoring function working correctly!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Load and Score DPO Data\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 4: LOADING AND SCORING DPO DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load DPO data\n",
        "dpo_path = '/kaggle/input/gricebench-dpo-raw/dpo_train.json'\n",
        "print(f\"\\nüì• Loading DPO data from: {dpo_path}\")\n",
        "\n",
        "with open(dpo_path) as f:\n",
        "    dpo_train = json.load(f)\n",
        "\n",
        "print(f\"   ‚úÖ Loaded {len(dpo_train)} DPO pairs\")\n",
        "\n",
        "# Score all pairs\n",
        "print(f\"\\nüîç Scoring {len(dpo_train)} pairs with Detector V2...\")\n",
        "print(\"   This will take ~10 minutes\")\n",
        "print(\"   Progress will be shown every 500 pairs\\n\")\n",
        "\n",
        "scored_data = []\n",
        "scoring_start = time.time()\n",
        "\n",
        "for idx, item in enumerate(tqdm(dpo_train, desc=\"Scoring pairs\")):\n",
        "    prompt = item.get('prompt', item.get('context', ''))\n",
        "    chosen = item.get('chosen', '')\n",
        "    rejected = item.get('rejected', '')\n",
        "    \n",
        "    # Score both responses\n",
        "    chosen_scores = score_response(prompt, chosen)\n",
        "    rejected_scores = score_response(prompt, rejected)\n",
        "    \n",
        "    # Calculate margins (rejected - chosen, positive = chosen is better)\n",
        "    margins = {\n",
        "        maxim: rejected_scores[maxim] - chosen_scores[maxim]\n",
        "        for maxim in ['quantity', 'quality', 'relation', 'manner']\n",
        "    }\n",
        "    \n",
        "    scored_item = item.copy()\n",
        "    scored_item['chosen_scores'] = chosen_scores\n",
        "    scored_item['rejected_scores'] = rejected_scores\n",
        "    scored_item['margins'] = margins\n",
        "    scored_item['avg_margin'] = sum(margins.values()) / len(margins)\n",
        "    \n",
        "    scored_data.append(scored_item)\n",
        "    \n",
        "    # Progress update\n",
        "    if (idx + 1) % 500 == 0:\n",
        "        elapsed = time.time() - scoring_start\n",
        "        rate = (idx + 1) / elapsed\n",
        "        remaining = (len(dpo_train) - idx - 1) / rate\n",
        "        print(f\"   Progress: {idx+1}/{len(dpo_train)} | Rate: {rate:.1f} pairs/sec | ETA: {remaining/60:.1f} min\")\n",
        "\n",
        "scoring_time = time.time() - scoring_start\n",
        "\n",
        "print(f\"\\n‚úÖ Scored {len(scored_data)} pairs in {scoring_time/60:.1f} minutes\")\n",
        "print(f\"   Average: {len(scored_data)/scoring_time:.1f} pairs/second\")\n",
        "\n",
        "# Save scored data\n",
        "scored_path = '/kaggle/working/analysis/scored_data.json'\n",
        "with open(scored_path, 'w') as f:\n",
        "    json.dump(scored_data, f, indent=2)\n",
        "print(f\"\\nüíæ Saved scored data to: {scored_path}\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Analyze Initial Data\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 5: INITIAL DATA ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame([{\n",
        "    'prompt': item.get('prompt', ''),\n",
        "    'chosen': item.get('chosen', ''),\n",
        "    'rejected': item.get('rejected', ''),\n",
        "    'quantity_margin': item['margins']['quantity'],\n",
        "    'quality_margin': item['margins']['quality'],\n",
        "    'relation_margin': item['margins']['relation'],\n",
        "    'manner_margin': item['margins']['manner'],\n",
        "    'avg_margin': item['avg_margin'],\n",
        "    'full_item': item\n",
        "} for item in scored_data])\n",
        "\n",
        "print(f\"\\nüìä Dataset: {len(df)} pairs\")\n",
        "\n",
        "# Margin statistics\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"MARGIN STATISTICS (rejected - chosen)\")\n",
        "print(\"Positive margin = chosen is better\")\n",
        "print(\"-\"*80)\n",
        "print(f\"{'Maxim':<12} {'Mean':<10} {'Std':<10} {'>0%':<10} {'>0.15%':<10} {'Status'}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for maxim in ['quantity', 'quality', 'relation', 'manner']:\n",
        "    col = f'{maxim}_margin'\n",
        "    mean_val = df[col].mean()\n",
        "    std_val = df[col].std()\n",
        "    pos_pct = (df[col] > 0).mean() * 100\n",
        "    strong_pct = (df[col] > 0.15).mean() * 100\n",
        "    \n",
        "    status = \"‚úÖ Good\" if mean_val > 0.05 else \"‚ö†Ô∏è  Weak\" if mean_val > 0 else \"‚ùå Negative\"\n",
        "    \n",
        "    print(f\"{maxim:<12} {mean_val:>+.3f}     {std_val:>6.3f}     \"\n",
        "          f\"{pos_pct:>5.1f}%    {strong_pct:>5.1f}%    {status}\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Key findings\n",
        "print(\"\\nüîç Key Findings:\")\n",
        "manner_mean = df['manner_margin'].mean()\n",
        "if manner_mean < 0:\n",
        "    print(f\"   ‚ö†Ô∏è  MANNER IS NEGATIVE: {manner_mean:.3f}\")\n",
        "    print(\"      This is the problem we need to fix!\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Manner is positive: {manner_mean:.3f}\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Identify Problem Pairs\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 6: IDENTIFYING MANNER PROBLEM PAIRS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find pairs where Manner is negative but content is good\n",
        "print(\"\\nüîç Finding pairs with:\")\n",
        "print(\"   - Manner margin < -0.1 (negative)\")\n",
        "print(\"   - BUT Quantity OR Relation > 0.1 (good content)\\n\")\n",
        "\n",
        "problem_pairs = df[\n",
        "    (df['manner_margin'] < -0.1) &\n",
        "    (\n",
        "        (df['relation_margin'] > 0.1) |\n",
        "        (df['quantity_margin'] > 0.1)\n",
        "    )\n",
        "].copy()\n",
        "\n",
        "print(f\"üìä Found {len(problem_pairs)} problem pairs ({len(problem_pairs)/len(df)*100:.1f}%)\")\n",
        "print(f\"   These have good content but bad Manner\")\n",
        "\n",
        "# Statistics\n",
        "print(\"\\nüìà Problem pairs statistics:\")\n",
        "print(f\"   Manner mean: {problem_pairs['manner_margin'].mean():.3f}\")\n",
        "print(f\"   Quantity mean: {problem_pairs['quantity_margin'].mean():.3f}\")\n",
        "print(f\"   Relation mean: {problem_pairs['relation_margin'].mean():.3f}\")\n",
        "\n",
        "# Show examples\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"SAMPLE PROBLEM PAIRS (first 3)\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for idx, (i, row) in enumerate(problem_pairs.head(3).iterrows()):\n",
        "    print(f\"\\nExample {idx+1}:\")\n",
        "    print(f\"  Chosen:   {row['chosen'][:100]}...\")\n",
        "    print(f\"  Rejected: {row['rejected'][:100]}...\")\n",
        "    print(f\"  Margins - Q:{row['quantity_margin']:+.2f} Qual:{row['quality_margin']:+.2f} \"\n",
        "          f\"R:{row['relation_margin']:+.2f} M:{row['manner_margin']:+.2f}\")\n",
        "    print(f\"  Problem: Good content but unclear Manner\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "# Save for analysis\n",
        "problem_path = '/kaggle/working/analysis/problem_pairs.json'\n",
        "problem_pairs.to_json(problem_path, orient='records', indent=2)\n",
        "print(f\"\\nüíæ Saved problem pairs to: {problem_path}\")\n",
        "\n",
        "print(\"\\n‚úÖ Problem identification complete!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Setup Gemini API\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 7: INITIALIZING GEMINI API (FREE!)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get API key from Kaggle Secrets\n",
        "print(\"\\nüîë Loading API key from Kaggle Secrets...\")\n",
        "\n",
        "try:\n",
        "    from kaggle_secrets import UserSecretsClient\n",
        "    user_secrets = UserSecretsClient()\n",
        "    api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
        "    print(\"   ‚úÖ API key loaded successfully\")\n",
        "    print(f\"   Key starts with: {api_key[:10]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error loading API key: {e}\")\n",
        "    print(\"   Please add GEMINI_API_KEY to Kaggle Secrets\")\n",
        "    raise\n",
        "\n",
        "# Configure Gemini\n",
        "print(\"\\nü§ñ Configuring Gemini API...\")\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# Use Gemini 1.5 Flash (fastest, free tier)\n",
        "gemini_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "print(\"   ‚úÖ Gemini API configured\")\n",
        "print(\"   Model: gemini-1.5-flash\")\n",
        "print(\"   Rate limit: 15 requests/minute\")\n",
        "print(\"   Daily limit: 1,500 requests\")\n",
        "print(\"   Cost: $0.00 (FREE!) üéâ\")\n",
        "\n",
        "# Test the API\n",
        "print(\"\\nüß™ Testing Gemini API...\")\n",
        "\n",
        "try:\n",
        "    test_response = gemini_model.generate_content(\n",
        "        \"Say 'Hello from Gemini!' in exactly 3 words\",\n",
        "        generation_config=genai.types.GenerationConfig(temperature=0.3)\n",
        "    )\n",
        "    print(f\"   ‚úÖ API test successful!\")\n",
        "    print(f\"   Response: {test_response.text.strip()}\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå API test failed: {e}\")\n",
        "    raise\n",
        "\n",
        "print(\"\\n‚úÖ Gemini API ready to use!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Fix Manner Violations with Gemini\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 8: FIXING MANNER VIOLATIONS WITH GEMINI\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def fix_manner_violation(text: str, max_retries: int = 3) -> str:\n",
        "    \"\"\"Fix Manner violations using Gemini API\"\"\"\n",
        "    \n",
        "    prompt = f\"\"\"Fix ONLY the clarity and organization issues in this text.\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. Replace ambiguous references with clear ones\n",
        "   - \"Said\" ‚Üí \"The company said\"\n",
        "   - \"it\" ‚Üí specific noun\n",
        "   - \"they\" ‚Üí specific group\n",
        "2. Fix unclear pronoun references\n",
        "3. Improve sentence structure if confusing\n",
        "4. Keep the EXACT same meaning and facts\n",
        "5. Maintain similar length (within 20%)\n",
        "6. Do NOT add new information\n",
        "7. Do NOT remove any facts\n",
        "\n",
        "Original text:\n",
        "{text}\n",
        "\n",
        "Fixed text (output ONLY the fixed text):\"\"\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Respect rate limit (15/min = 4 seconds between requests)\n",
        "            time.sleep(4)\n",
        "            \n",
        "            response = gemini_model.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    temperature=0.3,\n",
        "                    max_output_tokens=1000,\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            fixed_text = response.text.strip()\n",
        "            fixed_text = fixed_text.replace('```', '').strip()\n",
        "            \n",
        "            # Validate length\n",
        "            len_ratio = len(fixed_text) / len(text)\n",
        "            if 0.6 <= len_ratio <= 1.4:\n",
        "                return fixed_text\n",
        "            else:\n",
        "                continue\n",
        "                \n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "            else:\n",
        "                return text  # Keep original if all retries fail\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Fix all problem pairs\n",
        "print(f\"\\nüîß Fixing {len(problem_pairs)} pairs with Gemini...\")\n",
        "print(f\"   Rate: ~15 pairs/minute (4 seconds per pair)\")\n",
        "print(f\"   Estimated time: {len(problem_pairs) / 15:.1f} minutes\\n\")\n",
        "\n",
        "fixed_pairs = problem_pairs.copy()\n",
        "manner_improvements = []\n",
        "failed_fixes = []\n",
        "\n",
        "fix_start = time.time()\n",
        "\n",
        "for idx, i in enumerate(tqdm(problem_pairs.index, desc=\"Fixing pairs\")):\n",
        "    original_chosen = problem_pairs.loc[i, 'chosen']\n",
        "    \n",
        "    try:\n",
        "        # Fix the text\n",
        "        fixed_chosen = fix_manner_violation(original_chosen)\n",
        "        \n",
        "        # Update dataframe\n",
        "        fixed_pairs.loc[i, 'chosen'] = fixed_chosen\n",
        "        fixed_pairs.loc[i, 'original_chosen'] = original_chosen\n",
        "        \n",
        "        # Re-score with Detector V2\n",
        "        prompt = problem_pairs.loc[i, 'prompt']\n",
        "        new_scores = score_response(prompt, fixed_chosen)\n",
        "        rejected_scores = problem_pairs.loc[i, 'full_item']['rejected_scores']\n",
        "        \n",
        "        # Update margins\n",
        "        for maxim in ['quantity', 'quality', 'relation', 'manner']:\n",
        "            new_margin = rejected_scores[maxim] - new_scores[maxim]\n",
        "            fixed_pairs.loc[i, f'{maxim}_margin'] = new_margin\n",
        "            \n",
        "            if maxim == 'manner':\n",
        "                old_margin = problem_pairs.loc[i, 'manner_margin']\n",
        "                improvement = new_margin - old_margin\n",
        "                manner_improvements.append(improvement)\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è  Failed to fix pair {i}: {e}\")\n",
        "        failed_fixes.append(i)\n",
        "        continue\n",
        "    \n",
        "    # Progress update every 50 pairs\n",
        "    if (idx + 1) % 50 == 0:\n",
        "        elapsed = time.time() - fix_start\n",
        "        rate = (idx + 1) / elapsed * 60\n",
        "        remaining = (len(problem_pairs) - idx - 1) / rate\n",
        "        print(f\"\\n   Progress: {idx+1}/{len(problem_pairs)} | Rate: {rate:.1f}/min | ETA: {remaining:.1f}min\")\n",
        "\n",
        "fix_time = time.time() - fix_start\n",
        "\n",
        "print(f\"\\n‚úÖ Fixing complete!\")\n",
        "print(f\"   Time: {fix_time/60:.1f} minutes\")\n",
        "print(f\"   Successfully fixed: {len(problem_pairs) - len(failed_fixes)}\")\n",
        "print(f\"   Failed: {len(failed_fixes)}\")\n",
        "print(f\"   Cost: $0.00 (FREE!)\")\n",
        "\n",
        "# Analyze improvements\n",
        "if manner_improvements:\n",
        "    original_manner = problem_pairs['manner_margin'].mean()\n",
        "    fixed_manner = fixed_pairs['manner_margin'].mean()\n",
        "    avg_improvement = np.mean(manner_improvements)\n",
        "    \n",
        "    print(f\"\\nüìä Manner margin improvement:\")\n",
        "    print(f\"   Before: {original_manner:+.3f}\")\n",
        "    print(f\"   After:  {fixed_manner:+.3f}\")\n",
        "    print(f\"   Change: {fixed_manner - original_manner:+.3f} ({(fixed_manner - original_manner)/abs(original_manner)*100:+.1f}%)\")\n",
        "\n",
        "# Save fixed pairs\n",
        "fixed_path = '/kaggle/working/analysis/fixed_pairs.json'\n",
        "fixed_pairs.to_json(fixed_path, orient='records', indent=2)\n",
        "print(f\"\\nüíæ Saved fixed pairs to: {fixed_path}\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Create Final Clean Dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 9: CREATING FINAL CLEAN DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Get pairs that already have good Manner\n",
        "print(\"\\nüìä Combining data sources...\")\n",
        "\n",
        "good_manner_pairs = df[df['manner_margin'] > 0.1].copy()\n",
        "print(f\"   Pairs with good Manner (kept as-is): {len(good_manner_pairs)}\")\n",
        "print(f\"   Pairs with fixed Manner: {len(fixed_pairs)}\")\n",
        "\n",
        "# Combine\n",
        "final_df = pd.concat([good_manner_pairs, fixed_pairs], ignore_index=True)\n",
        "print(f\"   Combined total: {len(final_df)}\")\n",
        "\n",
        "# Remove duplicates\n",
        "before_dedup = len(final_df)\n",
        "final_df = final_df.drop_duplicates(subset=['chosen', 'rejected'])\n",
        "print(f\"   After deduplication: {len(final_df)} (removed {before_dedup - len(final_df)})\")\n",
        "\n",
        "# Apply quality filter\n",
        "print(\"\\nüîç Applying quality filter (avg_margin > 0.05)...\")\n",
        "avg_margins = final_df[['quantity_margin', 'quality_margin', 'relation_margin', 'manner_margin']].mean(axis=1)\n",
        "final_df['avg_margin'] = avg_margins\n",
        "\n",
        "before_filter = len(final_df)\n",
        "final_df = final_df[final_df['avg_margin'] > 0.05].copy()\n",
        "print(f\"   After quality filter: {len(final_df)} (removed {before_filter - len(final_df)})\")\n",
        "\n",
        "# Final statistics\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"FINAL DATASET STATISTICS\")\n",
        "print(\"-\"*80)\n",
        "print(f\"{'Maxim':<12} {'Mean':<10} {'Std':<10} {'>0%':<10} {'>0.15%':<10} {'Status'}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "all_positive = True\n",
        "for maxim in ['quantity', 'quality', 'relation', 'manner']:\n",
        "    col = f'{maxim}_margin'\n",
        "    mean_val = final_df[col].mean()\n",
        "    std_val = final_df[col].std()\n",
        "    pos_pct = (final_df[col] > 0).mean() * 100\n",
        "    strong_pct = (final_df[col] > 0.15).mean() * 100\n",
        "    \n",
        "    if mean_val <= 0:\n",
        "        all_positive = False\n",
        "        status = \"‚ùå Negative\"\n",
        "    elif mean_val < 0.05:\n",
        "        status = \"‚ö†Ô∏è  Weak\"\n",
        "    else:\n",
        "        status = \"‚úÖ Good\"\n",
        "    \n",
        "    print(f\"{maxim:<12} {mean_val:>+.3f}     {std_val:>6.3f}     \"\n",
        "          f\"{pos_pct:>5.1f}%    {strong_pct:>5.1f}%    {status}\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "\n",
        "if all_positive:\n",
        "    print(\"\\n‚úÖ SUCCESS! All maxims have positive mean margins!\")\n",
        "    print(\"   Dataset is ready for single-stage DPO training!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Warning: Some maxims still have negative or weak margins\")\n",
        "    print(\"   Consider adjusting filters or reviewing data\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Save Final Clean Dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CELL 10: SAVING FINAL CLEAN DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train/val split (95/5)\n",
        "print(\"\\nüìä Creating train/val split (95/5)...\")\n",
        "\n",
        "train_size = int(0.95 * len(final_df))\n",
        "train_df = final_df.iloc[:train_size].copy()\n",
        "val_df = final_df.iloc[train_size:].copy()\n",
        "\n",
        "print(f\"   Training set: {len(train_df)} pairs\")\n",
        "print(f\"   Validation set: {len(val_df)} pairs\")\n",
        "\n",
        "# Convert back to list format\n",
        "print(\"\\nüîÑ Converting to DPO format...\")\n",
        "\n",
        "train_data = []\n",
        "for idx, row in train_df.iterrows():\n",
        "    item = row['full_item'].copy()\n",
        "    # Use fixed chosen if available\n",
        "    if 'original_chosen' in row and pd.notna(row['original_chosen']):\n",
        "        item['chosen'] = row['chosen']\n",
        "        item['original_chosen'] = row['original_chosen']\n",
        "    train_data.append(item)\n",
        "\n",
        "val_data = []\n",
        "for idx, row in val_df.iterrows():\n",
        "    item = row['full_item'].copy()\n",
        "    if 'original_chosen' in row and pd.notna(row['original_chosen']):\n",
        "        item['chosen'] = row['chosen']\n",
        "        item['original_chosen'] = row['original_chosen']\n",
        "    val_data.append(item)\n",
        "\n",
        "print(f\"   ‚úÖ Converted {len(train_data)} training pairs\")\n",
        "print(f\"   ‚úÖ Converted {len(val_data)} validation pairs\")\n",
        "\n",
        "# Save\n",
        "print(\"\\nüíæ Saving clean datasets...\")\n",
        "\n",
        "train_path = '/kaggle/working/data/dpo_train_clean.json'\n",
        "val_path = '/kaggle/working/data/dpo_val_clean.json'\n",
        "\n",
        "with open(train_path, 'w') as f:\n",
        "    json.dump(train_data, f, indent=2)\n",
        "print(f\"   ‚úÖ Saved training data to: {train_path}\")\n",
        "print(f\"      Size: {Path(train_path).stat().st_size / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "with open(val_path, 'w') as f:\n",
        "    json.dump(val_data, f, indent=2)\n",
        "print(f\"   ‚úÖ Saved validation data to: {val_path}\")\n",
        "print(f\"      Size: {Path(val_path).stat().st_size / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "# Summary\n",
        "end_time = datetime.now()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ DATA CLEANING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\"\"\n",
        "‚úÖ Final Dataset Created!\n",
        "\n",
        "üìä Dataset Statistics:\n",
        "   Total pairs: {len(final_df)}\n",
        "   Training: {len(train_df)} pairs\n",
        "   Validation: {len(val_df)} pairs\n",
        "\n",
        "üìà Margin Improvements:\n",
        "   All margins positive: {all_positive}\n",
        "   Manner mean: {final_df['manner_margin'].mean():+.3f}\n",
        "   Quality mean: {final_df['quality_margin'].mean():+.3f}\n",
        "   Quantity mean: {final_df['quantity_margin'].mean():+.3f}\n",
        "   Relation mean: {final_df['relation_margin'].mean():+.3f}\n",
        "\n",
        "‚è±Ô∏è  Processing Time:\n",
        "   Started: {start_time.strftime('%H:%M:%S')}\n",
        "   Ended: {end_time.strftime('%H:%M:%S')}\n",
        "   Total: {total_time.total_seconds()/60:.1f} minutes\n",
        "\n",
        "üí∞ Total Cost: $0.00 (FREE!)\n",
        "\n",
        "üì• Download Files:\n",
        "   {train_path}\n",
        "   {val_path}\n",
        "\n",
        "üöÄ Next Steps:\n",
        "   1. Download the clean data files\n",
        "   2. Upload to Kaggle as 'gricebench-dpo-clean'\n",
        "   3. Run standard DPO training (no multi-stage needed!)\n",
        "   4. Expected: 75-85% cooperative rate\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
