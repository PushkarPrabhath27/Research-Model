{
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GriceBench Phase 1: Data Preparation\n",
                "\n",
                "This notebook generates the required data files for Phase 2 evaluation:\n",
                "1. Relation Evaluation Set (200 examples)\n",
                "2. Annotation Sample (1000 examples)\n",
                "\n",
                "## Required Datasets to Add:\n",
                "\n",
                "Before running this notebook, add these datasets:\n",
                "\n",
                "1. **Your GriceBench Data** - Upload as private dataset containing:\n",
                "   - `repair_data/repair_test.json`\n",
                "   - `gold_annotation_set.json`\n",
                "   - `val_examples.json`\n",
                "   - `topical_corpus.json`\n",
                "\n",
                "Mount path: `/kaggle/input/gricebench-scientific-fix/`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Configuration\n",
                "import os\n",
                "import json\n",
                "import random\n",
                "from pathlib import Path\n",
                "from collections import defaultdict\n",
                "import re\n",
                "\n",
                "# Paths - adjust based on your dataset mount\n",
                "DATA_INPUT = Path(\"/kaggle/input/gricebench-scientific-fix\")\n",
                "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
                "\n",
                "# Verify dataset is mounted\n",
                "if DATA_INPUT.exists():\n",
                "    print(f\"‚úÖ Dataset mounted at {DATA_INPUT}\")\n",
                "    print(\"Contents:\")\n",
                "    for item in DATA_INPUT.iterdir():\n",
                "        print(f\"  - {item.name}\")\n",
                "else:\n",
                "    print(\"‚ùå Dataset not found! Please add gricebench-scientific-fix dataset.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Create Relation Evaluation Set\n",
                "\n",
                "Samples 200 examples with Relation violations for MRR evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Create Relation Eval Set\n",
                "\n",
                "def create_relation_eval_set(test_data_path, output_path, num_examples=200, seed=42):\n",
                "    \"\"\"\n",
                "    Sample 200 examples with Relation violations for MRR evaluation.\n",
                "    Per morechanges.md lines 746-769.\n",
                "    \"\"\"\n",
                "    random.seed(seed)\n",
                "    \n",
                "    print(\"=\" * 70)\n",
                "    print(\"CREATE RELATION EVALUATION SET\")\n",
                "    print(\"=\" * 70)\n",
                "    \n",
                "    # Load test data\n",
                "    print(f\"\\nLoading test data from {test_data_path}...\")\n",
                "    with open(test_data_path, 'r', encoding='utf-8') as f:\n",
                "        test_data = json.load(f)\n",
                "    \n",
                "    print(f\"  Total examples: {len(test_data)}\")\n",
                "    \n",
                "    # Filter for Relation violations\n",
                "    relation_examples = []\n",
                "    for i, item in enumerate(test_data):\n",
                "        input_text = item.get(\"input_text\", \"\")\n",
                "        if \"[VIOLATION=RELATION]\" in input_text:\n",
                "            example = {\n",
                "                \"id\": f\"relation_eval_{i}\",\n",
                "                \"input_text\": input_text,\n",
                "                \"target_text\": item.get(\"target_text\", \"\"),\n",
                "                \"source_index\": i\n",
                "            }\n",
                "            \n",
                "            # Extract context and response\n",
                "            context_match = re.search(r'\\[CONTEXT\\](.*?)\\[', input_text, re.DOTALL)\n",
                "            response_match = re.search(r'\\[RESPONSE\\](.*?)$', input_text, re.DOTALL)\n",
                "            \n",
                "            if context_match:\n",
                "                example[\"context\"] = context_match.group(1).strip()\n",
                "            if response_match:\n",
                "                example[\"response\"] = response_match.group(1).strip()\n",
                "            \n",
                "            relation_examples.append(example)\n",
                "    \n",
                "    print(f\"  Relation violations found: {len(relation_examples)}\")\n",
                "    \n",
                "    if len(relation_examples) < num_examples:\n",
                "        print(f\"  WARNING: Only {len(relation_examples)} examples available\")\n",
                "        num_examples = len(relation_examples)\n",
                "    \n",
                "    # Sample\n",
                "    sampled = random.sample(relation_examples, num_examples)\n",
                "    \n",
                "    # Save\n",
                "    output_path = Path(output_path)\n",
                "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    with open(output_path, 'w', encoding='utf-8') as f:\n",
                "        json.dump(sampled, f, indent=2, ensure_ascii=False)\n",
                "    \n",
                "    print(f\"\\n‚úÖ Saved {len(sampled)} examples to {output_path}\")\n",
                "    return sampled\n",
                "\n",
                "# Run\n",
                "repair_test_path = DATA_INPUT / \"repair_data\" / \"repair_test.json\"\n",
                "if not repair_test_path.exists():\n",
                "    repair_test_path = DATA_INPUT / \"repair_test.json\"\n",
                "\n",
                "if repair_test_path.exists():\n",
                "    relation_eval_set = create_relation_eval_set(\n",
                "        test_data_path=str(repair_test_path),\n",
                "        output_path=str(OUTPUT_DIR / \"relation_eval_set.json\"),\n",
                "        num_examples=200\n",
                "    )\n",
                "else:\n",
                "    print(f\"‚ùå repair_test.json not found at expected paths\")\n",
                "    print(f\"   Checked: {repair_test_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Create Annotation Sample (1000 examples)\n",
                "\n",
                "Creates stratified sample for human annotation per morechanges.md."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Create Annotation Sample\n",
                "\n",
                "def create_annotation_sample(val_data_path, gold_data_path, output_path, num_samples=1000, seed=42):\n",
                "    \"\"\"\n",
                "    Create stratified sample for annotation.\n",
                "    Per morechanges.md lines 658-679.\n",
                "    \"\"\"\n",
                "    random.seed(seed)\n",
                "    \n",
                "    print(\"=\" * 70)\n",
                "    print(\"CREATE ANNOTATION SAMPLE (1,000 examples)\")\n",
                "    print(\"=\" * 70)\n",
                "    \n",
                "    all_examples = []\n",
                "    \n",
                "    # Load validation data\n",
                "    if Path(val_data_path).exists():\n",
                "        with open(val_data_path, 'r', encoding='utf-8') as f:\n",
                "            val_data = json.load(f)\n",
                "        print(f\"  Validation: {len(val_data)} examples\")\n",
                "        for i, item in enumerate(val_data):\n",
                "            item['source_file'] = 'validation'\n",
                "            item['source_idx'] = i\n",
                "        all_examples.extend(val_data)\n",
                "    \n",
                "    # Load gold data\n",
                "    if Path(gold_data_path).exists():\n",
                "        with open(gold_data_path, 'r', encoding='utf-8') as f:\n",
                "            gold_data = json.load(f)\n",
                "        print(f\"  Gold: {len(gold_data)} examples\")\n",
                "        for i, item in enumerate(gold_data):\n",
                "            item['source_file'] = 'gold'\n",
                "            item['source_idx'] = i\n",
                "        all_examples.extend(gold_data)\n",
                "    \n",
                "    print(f\"\\nTotal pool: {len(all_examples)} examples\")\n",
                "    \n",
                "    # Categorize\n",
                "    maxims = ['quantity', 'quality', 'relation', 'manner']\n",
                "    detector_positives = defaultdict(list)\n",
                "    detector_negatives = []\n",
                "    \n",
                "    for item in all_examples:\n",
                "        labels = item.get('labels', item.get('detector_predictions', {}))\n",
                "        has_violation = any(labels.get(m, 0) for m in maxims)\n",
                "        \n",
                "        if not has_violation:\n",
                "            detector_negatives.append(item)\n",
                "        else:\n",
                "            for maxim in maxims:\n",
                "                if labels.get(maxim, 0):\n",
                "                    detector_positives[maxim].append(item)\n",
                "    \n",
                "    print(f\"\\nCategorization:\")\n",
                "    print(f\"  Clean examples: {len(detector_negatives)}\")\n",
                "    for maxim in maxims:\n",
                "        print(f\"  {maxim} positives: {len(detector_positives[maxim])}\")\n",
                "    \n",
                "    # Sample\n",
                "    final_sample = []\n",
                "    seen_ids = set()\n",
                "    \n",
                "    def add_samples(pool, count, category):\n",
                "        nonlocal final_sample, seen_ids\n",
                "        shuffled = pool.copy()\n",
                "        random.shuffle(shuffled)\n",
                "        added = 0\n",
                "        for item in shuffled:\n",
                "            item_id = item.get('id', f\"{item.get('source_file', 'unknown')}_{item.get('source_idx', 0)}\")\n",
                "            if item_id not in seen_ids:\n",
                "                item['annotation_category'] = category\n",
                "                item['sample_id'] = f\"sample_{len(final_sample)}\"\n",
                "                final_sample.append(item)\n",
                "                seen_ids.add(item_id)\n",
                "                added += 1\n",
                "                if added >= count:\n",
                "                    break\n",
                "        return added\n",
                "    \n",
                "    # 200 per maxim\n",
                "    for maxim in maxims:\n",
                "        added = add_samples(detector_positives[maxim], 200, f\"{maxim}_positive\")\n",
                "        print(f\"  Added {added} {maxim} positives\")\n",
                "    \n",
                "    # 200 clean\n",
                "    added = add_samples(detector_negatives, 200, \"clean\")\n",
                "    print(f\"  Added {added} clean examples\")\n",
                "    \n",
                "    # 100 random\n",
                "    remaining = [item for item in all_examples \n",
                "                 if item.get('id', f\"{item.get('source_file', '')}_{item.get('source_idx', 0)}\") not in seen_ids]\n",
                "    added = add_samples(remaining, 100, \"random\")\n",
                "    print(f\"  Added {added} random examples\")\n",
                "    \n",
                "    print(f\"\\nTotal sampled: {len(final_sample)}\")\n",
                "    \n",
                "    # Shuffle and assign IDs\n",
                "    random.shuffle(final_sample)\n",
                "    for i, item in enumerate(final_sample):\n",
                "        item['id'] = f\"annotation_{i:04d}\"\n",
                "    \n",
                "    # Save\n",
                "    output_path = Path(output_path)\n",
                "    with open(output_path, 'w', encoding='utf-8') as f:\n",
                "        json.dump(final_sample, f, indent=2, ensure_ascii=False)\n",
                "    \n",
                "    print(f\"\\n‚úÖ Saved {len(final_sample)} examples to {output_path}\")\n",
                "    return final_sample\n",
                "\n",
                "# Run\n",
                "val_path = DATA_INPUT / \"val_examples.json\"\n",
                "gold_path = DATA_INPUT / \"gold_annotation_set.json\"\n",
                "\n",
                "annotation_sample = create_annotation_sample(\n",
                "    val_data_path=str(val_path),\n",
                "    gold_data_path=str(gold_path),\n",
                "    output_path=str(OUTPUT_DIR / \"annotation_sample_1000.json\"),\n",
                "    num_samples=1000\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Verify Outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Verify and download outputs\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"OUTPUT VERIFICATION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "outputs = [\n",
                "    OUTPUT_DIR / \"relation_eval_set.json\",\n",
                "    OUTPUT_DIR / \"annotation_sample_1000.json\"\n",
                "]\n",
                "\n",
                "for output_file in outputs:\n",
                "    if output_file.exists():\n",
                "        size_kb = output_file.stat().st_size / 1024\n",
                "        with open(output_file, 'r') as f:\n",
                "            data = json.load(f)\n",
                "        print(f\"\\n‚úÖ {output_file.name}\")\n",
                "        print(f\"   Size: {size_kb:.1f} KB\")\n",
                "        print(f\"   Examples: {len(data)}\")\n",
                "        if data:\n",
                "            print(f\"   Sample keys: {list(data[0].keys())[:5]}\")\n",
                "    else:\n",
                "        print(f\"\\n‚ùå {output_file.name} - NOT CREATED\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"PHASE 1 COMPLETE\")\n",
                "print(\"=\" * 70)\n",
                "print(\"\\nüì• Download and add to your dataset for Phase 2:\")\n",
                "print(\"   1. relation_eval_set.json\")\n",
                "print(\"   2. annotation_sample_1000.json\")"
            ]
        }
    ]
}
