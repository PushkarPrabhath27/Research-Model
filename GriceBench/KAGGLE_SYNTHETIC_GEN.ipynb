{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Synthetic Data Generation with Gemini\n",
                "\n",
                "**Instructions:**\n",
                "1. **Upload Data:** Create a new dataset with `scored_data.json` and add it to this notebook.\n",
                "2. **API Key:** Get your Gemini API key from Google AI Studio.\n",
                "3. **Run All:** Run the cells below. The script will save progress automatically.\n",
                "4. **Download:** When finished (or stopped), download `synthetic_candidates.json`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Gemini SDK\n",
                "!pip install -q -U google-generativeai tqdm\n",
                "\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "import tqdm\n",
                "import google.generativeai as genai\n",
                "from google.colab import userdata # Try colab/kaggle secrets if available, else manual input\n",
                "\n",
                "print(\"Libraries installed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================\n",
                "# CONFIGURATION\n",
                "# ============================\n",
                "\n",
                "# 1. API KEY SETUP\n",
                "try:\n",
                "    from kaggle_secrets import UserSecretsClient\n",
                "    user_secrets = UserSecretsClient()\n",
                "    API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n",
                "except:\n",
                "    API_KEY = input(\"Enter your Gemini API Key: \").strip()\n",
                "\n",
                "genai.configure(api_key=API_KEY)\n",
                "\n",
                "# 2. DATA LOAD\n",
                "# Look for the file in common locations\n",
                "POSSIBLE_PATHS = [\n",
                "    \"/kaggle/input/scored-data/scored_data.json\",\n",
                "    \"/kaggle/input/gricebench-scored/scored_data.json\",\n",
                "    \"scored_data.json\"\n",
                "]\n",
                "INPUT_FILE = next((p for p in POSSIBLE_PATHS if os.path.exists(p)), None)\n",
                "\n",
                "if not INPUT_FILE:\n",
                "    print(\"❌ Error: scored_data.json not found!\")\n",
                "    print(\"Please upload it as a dataset and add it to the notebook.\")\n",
                "    # Stop execution if possible, or raise error\n",
                "else:\n",
                "    print(f\"✅ Found input file: {INPUT_FILE}\")\n",
                "\n",
                "# 3. OUTPUT SETUP\n",
                "OUTPUT_FILE = \"/kaggle/working/synthetic_candidates.json\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================\n",
                "# GENERATION LOGIC\n",
                "# ============================\n",
                "\n",
                "# Strict Gricean System Prompt\n",
                "SYSTEM_INSTRUCTION = \"\"\"You are a Gricean Cooperative Assistant.\n",
                "Your task is to generate responses that strictly adhere to all four Gricean Maxims:\n",
                "1. Quantity: Be as informative as required, but no more.\n",
                "2. Quality: Do not say what you believe to be false or lack evidence for.\n",
                "3. Relation: Be strictly relevant to the user's prompt.\n",
                "4. Manner: Be perspicuous—avoid obscurity, ambiguity, and unnecessary verbosity. Be orderly and polite.\n",
                "\n",
                "Context: You are providing a 'chosen' response for a DPO dataset.\n",
                "Your output must be significantly better than a typical chatbot response in terms of cooperation and clarity.\n",
                "Do not be chatty. Do not offer unsolicited advice. Answer the prompt directly and cooperatively.\"\"\"\n",
                "\n",
                "model = genai.GenerativeModel(\n",
                "    model_name=\"gemini-1.5-flash\",\n",
                "    generation_config={\n",
                "        \"temperature\": 0.7,\n",
                "        \"top_p\": 0.95,\n",
                "        \"top_k\": 40,\n",
                "        \"max_output_tokens\": 1024,\n",
                "        \"response_mime_type\": \"text/plain\",\n",
                "    },\n",
                "    system_instruction=SYSTEM_INSTRUCTION\n",
                ")\n",
                "\n",
                "def get_failed_prompts(data_path):\n",
                "    with open(data_path, 'r') as f: data = json.load(f)\n",
                "    candidates = []\n",
                "    for entry in data:\n",
                "        m = entry.get('margins', {})\n",
                "        # Select if ANY margin is non-positive\n",
                "        if not (m.get('quantity',0)>0 and m.get('quality',0)>0 and m.get('relation',0)>0 and m.get('manner',0)>0):\n",
                "            candidates.append(entry)\n",
                "    return candidates\n",
                "\n",
                "def run_generation():\n",
                "    if not INPUT_FILE: return\n",
                "    \n",
                "    # Load targets\n",
                "    all_candidates = get_failed_prompts(INPUT_FILE)\n",
                "    print(f\"Target prompts to generate: {len(all_candidates)}\")\n",
                "    \n",
                "    # Resume capability\n",
                "    completed = []\n",
                "    if os.path.exists(OUTPUT_FILE):\n",
                "        try:\n",
                "            with open(OUTPUT_FILE, 'r') as f: completed = json.load(f)\n",
                "            print(f\"Resuming: {len(completed)} already done.\")\n",
                "        except: pass\n",
                "    \n",
                "    completed_prompts = {c['prompt'] for c in completed}\n",
                "    todos = [c for c in all_candidates if c['prompt'] not in completed_prompts]\n",
                "    print(f\"Remaining: {len(todos)}\")\n",
                "    \n",
                "    # Loop\n",
                "    print(\"Starting generation... (Autosaving every 10 items)\")\n",
                "    print(\"Stop anytime by interrupting the cell. Data is saved.\")\n",
                "\n",
                "    try:\n",
                "        count = 0\n",
                "        for i, item in enumerate(tqdm.tqdm(todos)):\n",
                "            try:\n",
                "                # Generate\n",
                "                response = model.generate_content(item['prompt'])\n",
                "                text = response.text.strip()\n",
                "                \n",
                "                # Save result\n",
                "                res_entry = item.copy()\n",
                "                res_entry['synthetic_chosen'] = text\n",
                "                res_entry['original_chosen_failed'] = item['chosen']\n",
                "                res_entry['chosen'] = text # New chosen\n",
                "                \n",
                "                completed.append(res_entry)\n",
                "                count += 1\n",
                "                \n",
                "                # Autosave\n",
                "                if count % 10 == 0:\n",
                "                    with open(OUTPUT_FILE, 'w') as f: json.dump(completed, f, indent=2)\n",
                "                \n",
                "                # Rate limit sleep (adjust based on your tier)\n",
                "                time.sleep(2.0) \n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"Error on item {i}: {e}\")\n",
                "                time.sleep(10)\n",
                "                \n",
                "    except KeyboardInterrupt:\n",
                "        print(\"\\nStopped by user.\")\n",
                "    finally:\n",
                "        with open(OUTPUT_FILE, 'w') as f: json.dump(completed, f, indent=2)\n",
                "        print(f\"\\n✅ Saved {len(completed)} items to {OUTPUT_FILE}\")\n",
                "\n",
                "run_generation()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}