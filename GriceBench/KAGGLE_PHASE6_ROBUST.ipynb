{
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "kaggle": {
            "accelerator": "gpu",
            "dataSources": [
                {
                    "datasetId": "gricebench-scientific-fix",
                    "sourceType": "datasetVersion"
                }
            ]
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 6: Robust Detector V2 Training\n",
                "\n",
                "**Version**: 2.0 (Clean Slate Implementation)\n",
                "\n",
                "## Key Features\n",
                "- **Comprehensive Logging**: Every step is logged with clear checkpoints\n",
                "- **Data Validation**: Assertions prevent silent failures\n",
                "- **Stratified Split**: Ensures balanced validation set\n",
                "- **Dynamic pos_weight**: Calculated from training data\n",
                "- **Threshold Optimization**: Per-class threshold tuning\n",
                "- **Early Stopping**: Prevents overfitting\n",
                "\n",
                "## Execution Order\n",
                "Run ALL cells in order. Do NOT skip cells."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 1: Environment Setup & Logging\n",
                "# ============================================================================\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import json\n",
                "import time\n",
                "import random\n",
                "import logging\n",
                "from datetime import datetime\n",
                "from dataclasses import dataclass, field\n",
                "from typing import List, Dict, Tuple, Optional, Any\n",
                "from pathlib import Path\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "# Setup comprehensive logging\n",
                "class ColoredFormatter(logging.Formatter):\n",
                "    COLORS = {\n",
                "        'DEBUG': '\\033[36m',    # Cyan\n",
                "        'INFO': '\\033[32m',     # Green\n",
                "        'WARNING': '\\033[33m',  # Yellow\n",
                "        'ERROR': '\\033[31m',    # Red\n",
                "        'CRITICAL': '\\033[41m', # Red bg\n",
                "    }\n",
                "    RESET = '\\033[0m'\n",
                "    \n",
                "    def format(self, record):\n",
                "        color = self.COLORS.get(record.levelname, '')\n",
                "        record.levelname = f\"{color}{record.levelname}{self.RESET}\"\n",
                "        return super().format(record)\n",
                "\n",
                "# Create logger\n",
                "logger = logging.getLogger('Phase6')\n",
                "logger.setLevel(logging.DEBUG)\n",
                "\n",
                "# Console handler with colors\n",
                "console_handler = logging.StreamHandler()\n",
                "console_handler.setLevel(logging.INFO)\n",
                "console_format = ColoredFormatter('%(levelname)s | %(message)s')\n",
                "console_handler.setFormatter(console_format)\n",
                "logger.addHandler(console_handler)\n",
                "\n",
                "# File handler for detailed logs\n",
                "os.makedirs('/kaggle/working/logs', exist_ok=True)\n",
                "file_handler = logging.FileHandler(f'/kaggle/working/logs/training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n",
                "file_handler.setLevel(logging.DEBUG)\n",
                "file_format = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
                "file_handler.setFormatter(file_format)\n",
                "logger.addHandler(file_handler)\n",
                "\n",
                "# Checkpoint tracking\n",
                "class CheckpointTracker:\n",
                "    def __init__(self):\n",
                "        self.checkpoints = {}\n",
                "        self.start_time = time.time()\n",
                "    \n",
                "    def mark(self, name: str, status: str = 'PASS', details: dict = None):\n",
                "        elapsed = time.time() - self.start_time\n",
                "        self.checkpoints[name] = {\n",
                "            'status': status,\n",
                "            'time': elapsed,\n",
                "            'details': details or {}\n",
                "        }\n",
                "        icon = '\u2705' if status == 'PASS' else '\u274c' if status == 'FAIL' else '\u26a0\ufe0f'\n",
                "        logger.info(f\"{icon} CHECKPOINT [{name}]: {status}\")\n",
                "        if details:\n",
                "            for k, v in details.items():\n",
                "                logger.info(f\"   {k}: {v}\")\n",
                "    \n",
                "    def summary(self):\n",
                "        logger.info(\"=\" * 60)\n",
                "        logger.info(\"CHECKPOINT SUMMARY\")\n",
                "        logger.info(\"=\" * 60)\n",
                "        for name, data in self.checkpoints.items():\n",
                "            icon = '\u2705' if data['status'] == 'PASS' else '\u274c'\n",
                "            logger.info(f\"{icon} {name}: {data['status']} ({data['time']:.1f}s)\")\n",
                "\n",
                "tracker = CheckpointTracker()\n",
                "\n",
                "# Set seeds\n",
                "def set_all_seeds(seed=42):\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "\n",
                "set_all_seeds(42)\n",
                "\n",
                "# Device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "logger.info(f\"Device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "\n",
                "tracker.mark('Environment Setup', 'PASS', {'device': str(device)})\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"CELL 1 COMPLETE: Environment Ready\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 2: Configuration\n",
                "# ============================================================================\n",
                "\n",
                "@dataclass\n",
                "class Config:\n",
                "    # Paths\n",
                "    data_dir: str = '/kaggle/input/gricebench-scientific-fix'\n",
                "    output_dir: str = '/kaggle/working'\n",
                "    \n",
                "    # Model\n",
                "    model_name: str = 'microsoft/deberta-v3-small'\n",
                "    num_labels: int = 4\n",
                "    max_length: int = 256\n",
                "    \n",
                "    # Training\n",
                "    batch_size: int = 16\n",
                "    gradient_accumulation: int = 4\n",
                "    learning_rate: float = 2e-5\n",
                "    weight_decay: float = 0.01\n",
                "    num_epochs: int = 10\n",
                "    warmup_ratio: float = 0.1\n",
                "    max_grad_norm: float = 1.0\n",
                "    \n",
                "    # Early stopping\n",
                "    patience: int = 3\n",
                "    min_delta: float = 0.01\n",
                "    \n",
                "    # Data split\n",
                "    val_ratio: float = 0.15\n",
                "    test_ratio: float = 0.15\n",
                "    \n",
                "    # Mixed precision\n",
                "    fp16: bool = True\n",
                "    \n",
                "    def __post_init__(self):\n",
                "        self.effective_batch = self.batch_size * self.gradient_accumulation\n",
                "\n",
                "CONFIG = Config()\n",
                "\n",
                "logger.info(\"Configuration:\")\n",
                "for k, v in vars(CONFIG).items():\n",
                "    logger.info(f\"  {k}: {v}\")\n",
                "\n",
                "tracker.mark('Configuration', 'PASS')\n",
                "print(\"\\nCELL 2 COMPLETE: Configuration set\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 3: Data Structures & Utilities\n",
                "# ============================================================================\n",
                "\n",
                "@dataclass\n",
                "class Example:\n",
                "    \"\"\"Validated example structure\"\"\"\n",
                "    text: str\n",
                "    labels: List[int]\n",
                "    source: str\n",
                "    example_id: str = ''\n",
                "    \n",
                "    def __post_init__(self):\n",
                "        # Validation\n",
                "        if not isinstance(self.text, str):\n",
                "            raise ValueError(f\"Text must be string, got {type(self.text)}\")\n",
                "        if len(self.labels) != 4:\n",
                "            raise ValueError(f\"Labels must be length 4, got {len(self.labels)}\")\n",
                "        if not all(l in [0, 1] for l in self.labels):\n",
                "            raise ValueError(f\"Labels must be 0/1, got {self.labels}\")\n",
                "        if self.source not in ['phase4_violation', 'phase4_clean', 'synthetic']:\n",
                "            raise ValueError(f\"Invalid source: {self.source}\")\n",
                "\n",
                "def normalize_text(raw: Any) -> str:\n",
                "    \"\"\"Convert ANY format to clean string\"\"\"\n",
                "    if raw is None:\n",
                "        return ''\n",
                "    \n",
                "    if isinstance(raw, str):\n",
                "        return raw.strip()\n",
                "    \n",
                "    if isinstance(raw, list):\n",
                "        parts = []\n",
                "        for item in raw:\n",
                "            if isinstance(item, dict):\n",
                "                speaker = item.get('speaker', 'agent')\n",
                "                text = item.get('text', '')\n",
                "                parts.append(f\"[{speaker}]: {text}\")\n",
                "            elif isinstance(item, str):\n",
                "                parts.append(item)\n",
                "        return ' '.join(parts).strip()\n",
                "    \n",
                "    if isinstance(raw, dict):\n",
                "        if 'speaker' in raw and 'text' in raw:\n",
                "            return f\"[{raw['speaker']}]: {raw['text']}\"\n",
                "        # Try common keys\n",
                "        for key in ['text', 'response', 'content']:\n",
                "            if key in raw:\n",
                "                return normalize_text(raw[key])\n",
                "        return str(raw)\n",
                "    \n",
                "    return str(raw).strip()\n",
                "\n",
                "# Test normalize_text\n",
                "test_cases = [\n",
                "    \"Simple string\",\n",
                "    {'speaker': 'A', 'text': 'Hello'},\n",
                "    [{'speaker': 'A', 'text': 'Hi'}, {'speaker': 'B', 'text': 'Hello'}],\n",
                "    None\n",
                "]\n",
                "\n",
                "logger.info(\"Testing normalize_text:\")\n",
                "for tc in test_cases:\n",
                "    result = normalize_text(tc)\n",
                "    logger.debug(f\"  {type(tc).__name__} -> '{result[:50]}...'\" if len(str(result)) > 50 else f\"  {type(tc).__name__} -> '{result}'\")\n",
                "\n",
                "tracker.mark('Data Structures', 'PASS')\n",
                "print(\"\\nCELL 3 COMPLETE: Data structures defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 4: Load Phase 4 Data\n",
                "# ============================================================================\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"LOADING PHASE 4 DATA\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "# Check file exists\n",
                "phase4_path = f\"{CONFIG.data_dir}/natural_violations.json\"\n",
                "if not os.path.exists(phase4_path):\n",
                "    logger.error(f\"File not found: {phase4_path}\")\n",
                "    raise FileNotFoundError(phase4_path)\n",
                "\n",
                "logger.info(f\"Loading from: {phase4_path}\")\n",
                "file_size = os.path.getsize(phase4_path) / 1024\n",
                "logger.info(f\"File size: {file_size:.1f} KB\")\n",
                "\n",
                "with open(phase4_path, 'r') as f:\n",
                "    raw_data = json.load(f)\n",
                "\n",
                "logger.info(f\"Raw records loaded: {len(raw_data)}\")\n",
                "\n",
                "# Sample inspection\n",
                "if raw_data:\n",
                "    sample = raw_data[0]\n",
                "    logger.info(f\"Sample keys: {list(sample.keys())}\")\n",
                "    logger.debug(f\"Sample record: {json.dumps(sample, indent=2)[:500]}...\")\n",
                "\n",
                "# Process violations and clean examples\n",
                "violations = []\n",
                "clean_examples = []\n",
                "errors = []\n",
                "\n",
                "for idx, item in enumerate(raw_data):\n",
                "    try:\n",
                "        # Get context and combine with response\n",
                "        context = normalize_text(item.get('context', ''))\n",
                "        \n",
                "        # VIOLATION: violated_response with labels\n",
                "        violated_response = normalize_text(item.get('violated_response', ''))\n",
                "        if violated_response:\n",
                "            text = f\"{context} [SEP] {violated_response}\" if context else violated_response\n",
                "            \n",
                "            # Get labels\n",
                "            labels_dict = item.get('labels', {})\n",
                "            if isinstance(labels_dict, dict):\n",
                "                labels = [\n",
                "                    int(labels_dict.get('quantity', 0)),\n",
                "                    int(labels_dict.get('quality', 0)),\n",
                "                    int(labels_dict.get('relation', 0)),\n",
                "                    int(labels_dict.get('manner', 0))\n",
                "                ]\n",
                "            else:\n",
                "                # Infer from maxim field\n",
                "                maxim = str(item.get('maxim', '')).lower()\n",
                "                labels = [\n",
                "                    1 if 'quantity' in maxim else 0,\n",
                "                    1 if 'quality' in maxim else 0,\n",
                "                    1 if 'relation' in maxim else 0,\n",
                "                    1 if 'manner' in maxim else 0\n",
                "                ]\n",
                "            \n",
                "            if sum(labels) > 0 and len(text) > 50:\n",
                "                violations.append(Example(\n",
                "                    text=text,\n",
                "                    labels=labels,\n",
                "                    source='phase4_violation',\n",
                "                    example_id=str(item.get('id', idx))\n",
                "                ))\n",
                "        \n",
                "        # CLEAN: original_response with [0,0,0,0]\n",
                "        original_response = normalize_text(item.get('original_response', ''))\n",
                "        if original_response:\n",
                "            text = f\"{context} [SEP] {original_response}\" if context else original_response\n",
                "            if len(text) > 50:\n",
                "                clean_examples.append(Example(\n",
                "                    text=text,\n",
                "                    labels=[0, 0, 0, 0],\n",
                "                    source='phase4_clean',\n",
                "                    example_id=f\"{item.get('id', idx)}_clean\"\n",
                "                ))\n",
                "    \n",
                "    except Exception as e:\n",
                "        errors.append(f\"Item {idx}: {str(e)}\")\n",
                "\n",
                "logger.info(f\"\\nProcessing Results:\")\n",
                "logger.info(f\"  Violations: {len(violations)}\")\n",
                "logger.info(f\"  Clean: {len(clean_examples)}\")\n",
                "logger.info(f\"  Errors: {len(errors)}\")\n",
                "\n",
                "if errors[:3]:\n",
                "    logger.warning(\"Sample errors:\")\n",
                "    for e in errors[:3]:\n",
                "        logger.warning(f\"  {e}\")\n",
                "\n",
                "# Label distribution\n",
                "logger.info(\"\\nViolation Label Distribution:\")\n",
                "maxim_names = ['Quantity', 'Quality', 'Relation', 'Manner']\n",
                "for i, name in enumerate(maxim_names):\n",
                "    count = sum(1 for ex in violations if ex.labels[i] == 1)\n",
                "    logger.info(f\"  {name}: {count} ({100*count/len(violations):.1f}%)\")\n",
                "\n",
                "tracker.mark('Phase 4 Data Load', 'PASS' if len(violations) > 0 else 'FAIL', {\n",
                "    'violations': len(violations),\n",
                "    'clean': len(clean_examples)\n",
                "})\n",
                "\n",
                "print(f\"\\nCELL 4 COMPLETE: {len(violations)} violations, {len(clean_examples)} clean\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 5: Stratified Train/Val/Test Split\n",
                "# ============================================================================\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"CREATING STRATIFIED SPLITS\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "def stratified_split(examples: List[Example], val_ratio: float, test_ratio: float, seed: int = 42):\n",
                "    \"\"\"Split ensuring each source/label pattern appears in all splits\"\"\"\n",
                "    random.seed(seed)\n",
                "    \n",
                "    # Group by source and label pattern\n",
                "    groups = {}\n",
                "    for ex in examples:\n",
                "        key = (ex.source, tuple(ex.labels))\n",
                "        if key not in groups:\n",
                "            groups[key] = []\n",
                "        groups[key].append(ex)\n",
                "    \n",
                "    logger.info(f\"Found {len(groups)} unique source/label groups\")\n",
                "    \n",
                "    train, val, test = [], [], []\n",
                "    \n",
                "    for key, group in groups.items():\n",
                "        random.shuffle(group)\n",
                "        n = len(group)\n",
                "        \n",
                "        test_end = int(n * test_ratio)\n",
                "        val_end = test_end + int(n * val_ratio)\n",
                "        \n",
                "        test.extend(group[:test_end])\n",
                "        val.extend(group[test_end:val_end])\n",
                "        train.extend(group[val_end:])\n",
                "    \n",
                "    # Shuffle each split\n",
                "    random.shuffle(train)\n",
                "    random.shuffle(val)\n",
                "    random.shuffle(test)\n",
                "    \n",
                "    return train, val, test\n",
                "\n",
                "# Combine all examples\n",
                "all_examples = violations + clean_examples\n",
                "logger.info(f\"Total examples before split: {len(all_examples)}\")\n",
                "\n",
                "# Split\n",
                "train_data, val_data, test_data = stratified_split(\n",
                "    all_examples, \n",
                "    CONFIG.val_ratio, \n",
                "    CONFIG.test_ratio\n",
                ")\n",
                "\n",
                "logger.info(f\"\\nSplit Results:\")\n",
                "logger.info(f\"  Train: {len(train_data)}\")\n",
                "logger.info(f\"  Val: {len(val_data)}\")\n",
                "logger.info(f\"  Test: {len(test_data)}\")\n",
                "\n",
                "# Verify source distribution\n",
                "def count_sources(data):\n",
                "    from collections import Counter\n",
                "    return dict(Counter(ex.source for ex in data))\n",
                "\n",
                "logger.info(f\"\\nSource Distribution:\")\n",
                "logger.info(f\"  Train: {count_sources(train_data)}\")\n",
                "logger.info(f\"  Val: {count_sources(val_data)}\")\n",
                "logger.info(f\"  Test: {count_sources(test_data)}\")\n",
                "\n",
                "# CRITICAL: Verify val has positive examples for each maxim\n",
                "logger.info(\"\\nValidation Set Label Check:\")\n",
                "val_labels = np.array([ex.labels for ex in val_data])\n",
                "all_positive = True\n",
                "for i, name in enumerate(maxim_names):\n",
                "    count = val_labels[:, i].sum()\n",
                "    status = '\u2705' if count > 0 else '\u274c'\n",
                "    logger.info(f\"  {name}: {count} positives {status}\")\n",
                "    if count == 0:\n",
                "        all_positive = False\n",
                "\n",
                "if not all_positive:\n",
                "    logger.error(\"CRITICAL: Validation set missing positive examples!\")\n",
                "    raise ValueError(\"Validation set must have positive examples for all maxims\")\n",
                "\n",
                "tracker.mark('Stratified Split', 'PASS', {\n",
                "    'train': len(train_data),\n",
                "    'val': len(val_data),\n",
                "    'test': len(test_data)\n",
                "})\n",
                "\n",
                "print(f\"\\nCELL 5 COMPLETE: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 6: Load Tokenizer\n",
                "# ============================================================================\n",
                "\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"LOADING TOKENIZER\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(CONFIG.model_name)\n",
                "logger.info(f\"Tokenizer: {CONFIG.model_name}\")\n",
                "logger.info(f\"Vocab size: {tokenizer.vocab_size}\")\n",
                "\n",
                "# Test tokenization\n",
                "sample_text = train_data[0].text[:200]\n",
                "tokens = tokenizer(sample_text, truncation=True, max_length=CONFIG.max_length)\n",
                "logger.info(f\"Sample tokenization: {len(tokens['input_ids'])} tokens\")\n",
                "\n",
                "tracker.mark('Tokenizer Load', 'PASS')\n",
                "print(\"\\nCELL 6 COMPLETE: Tokenizer ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 7: Create PyTorch Datasets\n",
                "# ============================================================================\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"CREATING PYTORCH DATASETS\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "class GriceDataset(Dataset):\n",
                "    def __init__(self, examples: List[Example], tokenizer, max_length: int):\n",
                "        self.examples = examples\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.examples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        ex = self.examples[idx]\n",
                "        \n",
                "        encoding = self.tokenizer(\n",
                "            ex.text,\n",
                "            max_length=self.max_length,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            'input_ids': encoding['input_ids'].squeeze(0),\n",
                "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
                "            'labels': torch.tensor(ex.labels, dtype=torch.float32)\n",
                "        }\n",
                "\n",
                "# Create datasets\n",
                "train_dataset = GriceDataset(train_data, tokenizer, CONFIG.max_length)\n",
                "val_dataset = GriceDataset(val_data, tokenizer, CONFIG.max_length)\n",
                "test_dataset = GriceDataset(test_data, tokenizer, CONFIG.max_length)\n",
                "\n",
                "logger.info(f\"Datasets created:\")\n",
                "logger.info(f\"  Train: {len(train_dataset)}\")\n",
                "logger.info(f\"  Val: {len(val_dataset)}\")\n",
                "logger.info(f\"  Test: {len(test_dataset)}\")\n",
                "\n",
                "# Verify a batch\n",
                "sample = train_dataset[0]\n",
                "logger.info(f\"\\nSample batch shape:\")\n",
                "logger.info(f\"  input_ids: {sample['input_ids'].shape}\")\n",
                "logger.info(f\"  attention_mask: {sample['attention_mask'].shape}\")\n",
                "logger.info(f\"  labels: {sample['labels'].tolist()}\")\n",
                "\n",
                "tracker.mark('Datasets Created', 'PASS')\n",
                "print(\"\\nCELL 7 COMPLETE: Datasets ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 8: Create DataLoaders\n",
                "# ============================================================================\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"CREATING DATALOADERS\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "train_loader = DataLoader(\n",
                "    train_dataset,\n",
                "    batch_size=CONFIG.batch_size,\n",
                "    shuffle=True,\n",
                "    num_workers=2,\n",
                "    pin_memory=True\n",
                ")\n",
                "\n",
                "val_loader = DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=CONFIG.batch_size * 2,\n",
                "    shuffle=False,\n",
                "    num_workers=2,\n",
                "    pin_memory=True\n",
                ")\n",
                "\n",
                "test_loader = DataLoader(\n",
                "    test_dataset,\n",
                "    batch_size=CONFIG.batch_size * 2,\n",
                "    shuffle=False,\n",
                "    num_workers=2,\n",
                "    pin_memory=True\n",
                ")\n",
                "\n",
                "logger.info(f\"DataLoaders created:\")\n",
                "logger.info(f\"  Train batches: {len(train_loader)}\")\n",
                "logger.info(f\"  Val batches: {len(val_loader)}\")\n",
                "logger.info(f\"  Test batches: {len(test_loader)}\")\n",
                "logger.info(f\"  Effective batch size: {CONFIG.effective_batch}\")\n",
                "\n",
                "tracker.mark('DataLoaders Created', 'PASS')\n",
                "print(\"\\nCELL 8 COMPLETE: DataLoaders ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 9: Model Definition with Dynamic pos_weight\n",
                "# ============================================================================\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"CREATING MODEL\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "class MultiLabelDetector(nn.Module):\n",
                "    \"\"\"Multi-label violation detector with stored pos_weight\"\"\"\n",
                "    \n",
                "    def __init__(self, model_name: str, num_labels: int = 4, pos_weight: torch.Tensor = None):\n",
                "        super().__init__()\n",
                "        \n",
                "        self.encoder = AutoModel.from_pretrained(model_name)\n",
                "        hidden_size = self.encoder.config.hidden_size\n",
                "        \n",
                "        self.classifier = nn.Sequential(\n",
                "            nn.Dropout(0.1),\n",
                "            nn.Linear(hidden_size, hidden_size // 2),\n",
                "            nn.GELU(),\n",
                "            nn.Dropout(0.1),\n",
                "            nn.Linear(hidden_size // 2, num_labels)\n",
                "        )\n",
                "        \n",
                "        # CRITICAL: Store pos_weight as buffer (persists with model)\n",
                "        if pos_weight is None:\n",
                "            pos_weight = torch.ones(num_labels)\n",
                "        self.register_buffer('pos_weight', pos_weight)\n",
                "        \n",
                "        self.num_labels = num_labels\n",
                "    \n",
                "    def forward(self, input_ids, attention_mask, labels=None):\n",
                "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        pooled = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
                "        logits = self.classifier(pooled)\n",
                "        \n",
                "        loss = None\n",
                "        if labels is not None:\n",
                "            loss = F.binary_cross_entropy_with_logits(\n",
                "                logits, labels, pos_weight=self.pos_weight\n",
                "            )\n",
                "        \n",
                "        return {'loss': loss, 'logits': logits}\n",
                "\n",
                "# Calculate pos_weight from training data\n",
                "logger.info(\"\\nCalculating pos_weight from training data:\")\n",
                "train_labels_np = np.array([ex.labels for ex in train_data])\n",
                "pos_weights = []\n",
                "\n",
                "for i, name in enumerate(maxim_names):\n",
                "    pos = train_labels_np[:, i].sum()\n",
                "    neg = len(train_labels_np) - pos\n",
                "    weight = neg / (pos + 1e-6)  # Avoid division by zero\n",
                "    pos_weights.append(weight)\n",
                "    logger.info(f\"  {name}: pos={int(pos)}, neg={int(neg)}, weight={weight:.2f}\")\n",
                "\n",
                "pos_weight_tensor = torch.tensor(pos_weights, dtype=torch.float32)\n",
                "\n",
                "# Create model\n",
                "model = MultiLabelDetector(\n",
                "    CONFIG.model_name,\n",
                "    num_labels=CONFIG.num_labels,\n",
                "    pos_weight=pos_weight_tensor\n",
                ").to(device)\n",
                "\n",
                "# Verify pos_weight is stored\n",
                "logger.info(f\"\\nModel pos_weight: {model.pos_weight.tolist()}\")\n",
                "\n",
                "# Count parameters\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "logger.info(f\"\\nModel parameters:\")\n",
                "logger.info(f\"  Total: {total_params:,}\")\n",
                "logger.info(f\"  Trainable: {trainable_params:,}\")\n",
                "\n",
                "tracker.mark('Model Created', 'PASS', {\n",
                "    'params': f\"{total_params:,}\",\n",
                "    'pos_weight': [f\"{w:.2f}\" for w in pos_weights]\n",
                "})\n",
                "print(\"\\nCELL 9 COMPLETE: Model ready with pos_weight\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 10: Training Setup (Optimizer, Scheduler, Scaler)\n",
                "# ============================================================================\n",
                "\n",
                "from transformers import get_linear_schedule_with_warmup\n",
                "from sklearn.metrics import f1_score, precision_score, recall_score\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"TRAINING SETUP\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "# Optimizer\n",
                "optimizer = torch.optim.AdamW(\n",
                "    model.parameters(),\n",
                "    lr=CONFIG.learning_rate,\n",
                "    weight_decay=CONFIG.weight_decay\n",
                ")\n",
                "\n",
                "# Scheduler\n",
                "num_training_steps = len(train_loader) * CONFIG.num_epochs // CONFIG.gradient_accumulation\n",
                "num_warmup_steps = int(num_training_steps * CONFIG.warmup_ratio)\n",
                "\n",
                "scheduler = get_linear_schedule_with_warmup(\n",
                "    optimizer,\n",
                "    num_warmup_steps=num_warmup_steps,\n",
                "    num_training_steps=num_training_steps\n",
                ")\n",
                "\n",
                "# Mixed precision scaler\n",
                "scaler = torch.amp.GradScaler('cuda', enabled=CONFIG.fp16)\n",
                "\n",
                "logger.info(f\"Optimizer: AdamW (lr={CONFIG.learning_rate})\")\n",
                "logger.info(f\"Training steps: {num_training_steps}\")\n",
                "logger.info(f\"Warmup steps: {num_warmup_steps}\")\n",
                "logger.info(f\"Mixed precision: {CONFIG.fp16}\")\n",
                "\n",
                "tracker.mark('Training Setup', 'PASS')\n",
                "print(\"\\nCELL 10 COMPLETE: Training setup ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 11: Evaluation Function with Detailed Metrics\n",
                "# ============================================================================\n",
                "\n",
                "def evaluate(model, dataloader, thresholds=None, verbose=True):\n",
                "    \"\"\"\n",
                "    Evaluate model with detailed metrics per maxim.\n",
                "    Returns: macro_f1, per_class_scores, all_probs, all_labels\n",
                "    \"\"\"\n",
                "    if thresholds is None:\n",
                "        thresholds = [0.5, 0.5, 0.5, 0.5]\n",
                "    \n",
                "    model.eval()\n",
                "    all_probs = []\n",
                "    all_labels = []\n",
                "    total_loss = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in dataloader:\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            attention_mask = batch['attention_mask'].to(device)\n",
                "            labels = batch['labels'].to(device)\n",
                "            \n",
                "            outputs = model(input_ids, attention_mask, labels)\n",
                "            total_loss += outputs['loss'].item()\n",
                "            \n",
                "            probs = torch.sigmoid(outputs['logits']).cpu().numpy()\n",
                "            all_probs.extend(probs)\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "    \n",
                "    all_probs = np.array(all_probs)\n",
                "    all_labels = np.array(all_labels)\n",
                "    \n",
                "    # Apply thresholds\n",
                "    all_preds = (all_probs >= np.array(thresholds)).astype(int)\n",
                "    \n",
                "    # Calculate per-class metrics\n",
                "    results = {}\n",
                "    f1_scores = []\n",
                "    \n",
                "    for i, name in enumerate(maxim_names):\n",
                "        f1 = f1_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n",
                "        p = precision_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n",
                "        r = recall_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n",
                "        \n",
                "        f1_scores.append(f1)\n",
                "        results[name] = {'f1': f1, 'precision': p, 'recall': r}\n",
                "        \n",
                "        if verbose:\n",
                "            logger.info(f\"  {name}: F1={f1:.3f} (P={p:.3f}, R={r:.3f})\")\n",
                "    \n",
                "    macro_f1 = np.mean(f1_scores)\n",
                "    avg_loss = total_loss / len(dataloader)\n",
                "    \n",
                "    if verbose:\n",
                "        logger.info(f\"  Macro F1: {macro_f1:.4f}\")\n",
                "    \n",
                "    return {\n",
                "        'macro_f1': macro_f1,\n",
                "        'loss': avg_loss,\n",
                "        'per_class': results,\n",
                "        'all_probs': all_probs,\n",
                "        'all_labels': all_labels\n",
                "    }\n",
                "\n",
                "logger.info(\"Evaluation function defined\")\n",
                "print(\"\\nCELL 11 COMPLETE: Evaluation function ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 12: Training Loop with Early Stopping\n",
                "# ============================================================================\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"STARTING TRAINING\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "# Training state\n",
                "best_f1 = 0.0\n",
                "best_epoch = 0\n",
                "patience_counter = 0\n",
                "training_history = []\n",
                "\n",
                "# Training loop\n",
                "for epoch in range(CONFIG.num_epochs):\n",
                "    epoch_start = time.time()\n",
                "    \n",
                "    # Training\n",
                "    model.train()\n",
                "    total_train_loss = 0\n",
                "    optimizer.zero_grad()\n",
                "    \n",
                "    for step, batch in enumerate(train_loader):\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['labels'].to(device)\n",
                "        \n",
                "        # Forward pass with mixed precision\n",
                "        with torch.amp.autocast('cuda', enabled=CONFIG.fp16):\n",
                "            outputs = model(input_ids, attention_mask, labels)\n",
                "            loss = outputs['loss'] / CONFIG.gradient_accumulation\n",
                "        \n",
                "        # Backward pass\n",
                "        scaler.scale(loss).backward()\n",
                "        total_train_loss += loss.item() * CONFIG.gradient_accumulation\n",
                "        \n",
                "        # Optimizer step after accumulation\n",
                "        if (step + 1) % CONFIG.gradient_accumulation == 0:\n",
                "            scaler.unscale_(optimizer)\n",
                "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG.max_grad_norm)\n",
                "            scaler.step(optimizer)\n",
                "            scaler.update()\n",
                "            scheduler.step()\n",
                "            optimizer.zero_grad()\n",
                "    \n",
                "    avg_train_loss = total_train_loss / len(train_loader)\n",
                "    epoch_time = time.time() - epoch_start\n",
                "    \n",
                "    # Evaluation\n",
                "    logger.info(f\"\\n{'='*60}\")\n",
                "    logger.info(f\"EPOCH {epoch + 1}/{CONFIG.num_epochs}\")\n",
                "    logger.info(f\"{'='*60}\")\n",
                "    logger.info(f\"Train Loss: {avg_train_loss:.4f} | Time: {epoch_time:.1f}s\")\n",
                "    logger.info(f\"\\nValidation Results:\")\n",
                "    \n",
                "    eval_results = evaluate(model, val_loader)\n",
                "    \n",
                "    # Track history\n",
                "    training_history.append({\n",
                "        'epoch': epoch + 1,\n",
                "        'train_loss': avg_train_loss,\n",
                "        'val_loss': eval_results['loss'],\n",
                "        'val_macro_f1': eval_results['macro_f1'],\n",
                "        'per_class': eval_results['per_class']\n",
                "    })\n",
                "    \n",
                "    # Check for improvement\n",
                "    if eval_results['macro_f1'] > best_f1 + CONFIG.min_delta:\n",
                "        best_f1 = eval_results['macro_f1']\n",
                "        best_epoch = epoch + 1\n",
                "        patience_counter = 0\n",
                "        \n",
                "        # Save best model\n",
                "        torch.save({\n",
                "            'epoch': epoch + 1,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'best_f1': best_f1,\n",
                "            'pos_weight': model.pos_weight\n",
                "        }, f'{CONFIG.output_dir}/best_model.pt')\n",
                "        \n",
                "        logger.info(f\"\\n  \u2705 New best model saved! (F1={best_f1:.4f})\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "        logger.info(f\"\\n  No improvement ({patience_counter}/{CONFIG.patience})\")\n",
                "        \n",
                "        if patience_counter >= CONFIG.patience:\n",
                "            logger.info(f\"\\n\u26a0\ufe0f Early stopping triggered at epoch {epoch + 1}\")\n",
                "            break\n",
                "\n",
                "logger.info(f\"\\n{'='*60}\")\n",
                "logger.info(f\"TRAINING COMPLETE\")\n",
                "logger.info(f\"{'='*60}\")\n",
                "logger.info(f\"Best Epoch: {best_epoch}\")\n",
                "logger.info(f\"Best Val F1: {best_f1:.4f}\")\n",
                "\n",
                "tracker.mark('Training Complete', 'PASS', {\n",
                "    'best_epoch': best_epoch,\n",
                "    'best_f1': f\"{best_f1:.4f}\"\n",
                "})\n",
                "\n",
                "print(f\"\\nCELL 12 COMPLETE: Training finished. Best F1={best_f1:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 13: Threshold Optimization\n",
                "# ============================================================================\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"THRESHOLD OPTIMIZATION\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "# Load best model\n",
                "checkpoint = torch.load(f'{CONFIG.output_dir}/best_model.pt')\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "logger.info(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
                "\n",
                "# Get predictions on validation set\n",
                "eval_results = evaluate(model, val_loader, verbose=False)\n",
                "all_probs = eval_results['all_probs']\n",
                "all_labels = eval_results['all_labels']\n",
                "\n",
                "# Find optimal thresholds\n",
                "optimal_thresholds = []\n",
                "\n",
                "logger.info(\"\\nFinding optimal thresholds per maxim:\")\n",
                "for i, name in enumerate(maxim_names):\n",
                "    best_f1 = 0\n",
                "    best_thresh = 0.5\n",
                "    \n",
                "    for thresh in np.arange(0.1, 0.9, 0.05):\n",
                "        preds = (all_probs[:, i] >= thresh).astype(int)\n",
                "        f1 = f1_score(all_labels[:, i], preds, zero_division=0)\n",
                "        \n",
                "        if f1 > best_f1:\n",
                "            best_f1 = f1\n",
                "            best_thresh = thresh\n",
                "    \n",
                "    optimal_thresholds.append(best_thresh)\n",
                "    \n",
                "    # Compare with default\n",
                "    default_preds = (all_probs[:, i] >= 0.5).astype(int)\n",
                "    default_f1 = f1_score(all_labels[:, i], default_preds, zero_division=0)\n",
                "    \n",
                "    improvement = best_f1 - default_f1\n",
                "    logger.info(f\"  {name}: thresh={best_thresh:.2f} (F1: {default_f1:.3f} -> {best_f1:.3f}, +{improvement:.3f})\")\n",
                "\n",
                "# Final evaluation with optimal thresholds\n",
                "logger.info(\"\\n\" + \"=\"*60)\n",
                "logger.info(\"FINAL EVALUATION (Optimal Thresholds)\")\n",
                "logger.info(\"=\"*60)\n",
                "\n",
                "final_results = evaluate(model, val_loader, thresholds=optimal_thresholds)\n",
                "\n",
                "logger.info(f\"\\nMacro F1 with optimal thresholds: {final_results['macro_f1']:.4f}\")\n",
                "\n",
                "# Save thresholds\n",
                "threshold_config = {\n",
                "    'thresholds': {name: thresh for name, thresh in zip(maxim_names, optimal_thresholds)},\n",
                "    'macro_f1': final_results['macro_f1']\n",
                "}\n",
                "\n",
                "with open(f'{CONFIG.output_dir}/optimal_thresholds.json', 'w') as f:\n",
                "    json.dump(threshold_config, f, indent=2)\n",
                "\n",
                "tracker.mark('Threshold Optimization', 'PASS', {\n",
                "    'final_f1': f\"{final_results['macro_f1']:.4f}\"\n",
                "})\n",
                "\n",
                "print(f\"\\nCELL 13 COMPLETE: Optimal F1={final_results['macro_f1']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 14: Final Test Set Evaluation\n",
                "# ============================================================================\n",
                "\n",
                "logger.info(\"=\" * 60)\n",
                "logger.info(\"TEST SET EVALUATION\")\n",
                "logger.info(\"=\" * 60)\n",
                "\n",
                "test_results = evaluate(model, test_loader, thresholds=optimal_thresholds)\n",
                "\n",
                "logger.info(f\"\\nTest Set Macro F1: {test_results['macro_f1']:.4f}\")\n",
                "\n",
                "# Save final results\n",
                "final_report = {\n",
                "    'model': CONFIG.model_name,\n",
                "    'best_epoch': best_epoch,\n",
                "    'thresholds': {name: thresh for name, thresh in zip(maxim_names, optimal_thresholds)},\n",
                "    'validation': {\n",
                "        'macro_f1': final_results['macro_f1'],\n",
                "        'per_class': final_results['per_class']\n",
                "    },\n",
                "    'test': {\n",
                "        'macro_f1': test_results['macro_f1'],\n",
                "        'per_class': test_results['per_class']\n",
                "    },\n",
                "    'training_history': training_history\n",
                "}\n",
                "\n",
                "with open(f'{CONFIG.output_dir}/detector_v2_results.json', 'w') as f:\n",
                "    json.dump(final_report, f, indent=2, default=str)\n",
                "\n",
                "tracker.mark('Test Evaluation', 'PASS', {\n",
                "    'test_f1': f\"{test_results['macro_f1']:.4f}\"\n",
                "})\n",
                "\n",
                "print(f\"\\nCELL 14 COMPLETE: Test F1={test_results['macro_f1']:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 15: Final Summary\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"PHASE 6 DETECTOR V2 TRAINING COMPLETE\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "# Checkpoint summary\n",
                "tracker.summary()\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"FINAL RESULTS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(f\"\\nValidation Macro F1: {final_results['macro_f1']:.4f}\")\n",
                "print(f\"Test Macro F1:       {test_results['macro_f1']:.4f}\")\n",
                "\n",
                "print(f\"\\nPer-Class Test Results:\")\n",
                "for name, metrics in test_results['per_class'].items():\n",
                "    print(f\"  {name}: F1={metrics['f1']:.3f} (P={metrics['precision']:.3f}, R={metrics['recall']:.3f})\")\n",
                "\n",
                "print(f\"\\nOptimal Thresholds:\")\n",
                "for name, thresh in zip(maxim_names, optimal_thresholds):\n",
                "    print(f\"  {name}: {thresh:.2f}\")\n",
                "\n",
                "print(f\"\\nOutput Files:\")\n",
                "print(f\"  {CONFIG.output_dir}/best_model.pt\")\n",
                "print(f\"  {CONFIG.output_dir}/detector_v2_results.json\")\n",
                "print(f\"  {CONFIG.output_dir}/optimal_thresholds.json\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"\u2705 ALL COMPLETE - Download results from /kaggle/working/\")\n",
                "print(\"=\"*70)"
            ]
        }
    ]
}