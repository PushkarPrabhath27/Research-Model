{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GriceBench Part 4: Ablation Studies\n",
                "\n",
                "This notebook runs comprehensive ablation studies to measure the contribution of each GriceBench component.\n",
                "\n",
                "## Ablation Studies:\n",
                "1. **Component Ablation** - full_system vs dpo_only vs detect_repair vs baseline\n",
                "2. **Repair Strategy** - edit vs retrieval vs hybrid router\n",
                "3. **Threshold Sensitivity** - 0.3, 0.4, 0.5, 0.6, 0.7\n",
                "4. **Maxim Importance** - contribution of each maxim\n",
                "\n",
                "## Setup Requirements:\n",
                "1. **GPU**: Enable T4 x2 in Settings\n",
                "2. **Internet**: Enable for model downloads\n",
                "3. **Datasets**: Add your trained models as Kaggle datasets\n",
                "\n",
                "## Required Kaggle Datasets:\n",
                "- `gricean-maxim-detector-model` - Detector weights (best_model.pt)\n",
                "- `gricebench-repair-model` - Repair model folder\n",
                "- `dpo-generator-model` - DPO LoRA adapters\n",
                "- `gricebench-test-data` - Test examples\n",
                "\n",
                "**Estimated Runtime: ~45-60 minutes**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Install Dependencies\n",
                "!pip install -q transformers>=4.40.0 accelerate peft sentence-transformers\n",
                "!pip install -q faiss-cpu\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Setup and Configuration\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import json\n",
                "import gc\n",
                "import sys\n",
                "import time\n",
                "import os\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "import numpy as np\n",
                "from collections import defaultdict\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Flushed logging for Kaggle\n",
                "def log(msg):\n",
                "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] {msg}\")\n",
                "    sys.stdout.flush()\n",
                "\n",
                "# GPU Setup\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "log(f\"Device: {device}\")\n",
                "if torch.cuda.is_available():\n",
                "    log(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    log(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "\n",
                "# Configuration\n",
                "CONFIG = {\n",
                "    \"num_samples\": 100,  # Samples per ablation\n",
                "    \"batch_size\": 8,     # Batch size for GPU\n",
                "    \"max_new_tokens\": 100,\n",
                "    \"thresholds\": [0.3, 0.4, 0.5, 0.6, 0.7],\n",
                "    \"output_dir\": \"/kaggle/working/ablation_results\",\n",
                "    \n",
                "    # Model paths (update these to match your Kaggle dataset names)\n",
                "    \"detector_path\": \"/kaggle/input/gricean-maxim-detector-model\",\n",
                "    \"repair_path\": \"/kaggle/input/gricebench-repair-model\",\n",
                "    \"dpo_path\": \"/kaggle/input/dpo-generator-model\",\n",
                "    \"test_data_path\": \"/kaggle/input/gricebench-test-data\",\n",
                "}\n",
                "\n",
                "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Memory management\n",
                "def cleanup():\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()\n",
                "\n",
                "def get_gpu_memory():\n",
                "    if torch.cuda.is_available():\n",
                "        return torch.cuda.memory_allocated() / 1e9\n",
                "    return 0\n",
                "\n",
                "log(\"Configuration loaded!\")\n",
                "log(f\"Output: {CONFIG['output_dir']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Define Detector Model Architecture\n",
                "from transformers import AutoModel, AutoTokenizer\n",
                "\n",
                "class MaximDetectorV2(nn.Module):\n",
                "    \"\"\"Multi-head detector for Gricean maxim violations.\"\"\"\n",
                "    \n",
                "    def __init__(self, model_name=\"microsoft/deberta-v3-base\", num_maxims=4, dropout=0.15):\n",
                "        super().__init__()\n",
                "        self.encoder = AutoModel.from_pretrained(model_name)\n",
                "        hidden_size = self.encoder.config.hidden_size\n",
                "        \n",
                "        # Separate classification head per maxim\n",
                "        self.classifiers = nn.ModuleList([\n",
                "            nn.Sequential(\n",
                "                nn.Linear(hidden_size, hidden_size // 2),\n",
                "                nn.GELU(),\n",
                "                nn.Dropout(dropout),\n",
                "                nn.Linear(hidden_size // 2, hidden_size // 4),\n",
                "                nn.GELU(),\n",
                "                nn.Dropout(dropout),\n",
                "                nn.Linear(hidden_size // 4, 1)\n",
                "            )\n",
                "            for _ in range(num_maxims)\n",
                "        ])\n",
                "    \n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
                "        pooled = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
                "        logits = [classifier(pooled) for classifier in self.classifiers]\n",
                "        return torch.cat(logits, dim=1)\n",
                "\n",
                "log(\"Detector architecture defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Load Models (GPU Optimized)\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM, \n",
                "    AutoTokenizer,\n",
                "    T5ForConditionalGeneration,\n",
                "    T5Tokenizer\n",
                ")\n",
                "from peft import PeftModel\n",
                "\n",
                "log(\"=\"*60)\n",
                "log(\"LOADING MODELS\")\n",
                "log(\"=\"*60)\n",
                "\n",
                "models = {}\n",
                "tokenizers = {}\n",
                "\n",
                "# 1. Load Detector\n",
                "log(\"\\n1. Loading Detector...\")\n",
                "try:\n",
                "    detector_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
                "    detector = MaximDetectorV2(\"microsoft/deberta-v3-base\")\n",
                "    \n",
                "    # Load trained weights\n",
                "    detector_weights = os.path.join(CONFIG[\"detector_path\"], \"best_model.pt\")\n",
                "    if os.path.exists(detector_weights):\n",
                "        state_dict = torch.load(detector_weights, map_location=device)\n",
                "        detector.load_state_dict(state_dict)\n",
                "        log(f\"   ‚úÖ Loaded detector weights from {detector_weights}\")\n",
                "    else:\n",
                "        log(f\"   ‚ö†Ô∏è No detector weights found at {detector_weights}\")\n",
                "    \n",
                "    detector = detector.to(device).half()  # FP16 for GPU\n",
                "    detector.eval()\n",
                "    models[\"detector\"] = detector\n",
                "    tokenizers[\"detector\"] = detector_tokenizer\n",
                "    log(f\"   GPU Memory: {get_gpu_memory():.1f} GB\")\n",
                "except Exception as e:\n",
                "    log(f\"   ‚ùå Failed to load detector: {e}\")\n",
                "\n",
                "cleanup()\n",
                "\n",
                "# 2. Load Repair Model\n",
                "log(\"\\n2. Loading Repair Model...\")\n",
                "try:\n",
                "    repair_path = CONFIG[\"repair_path\"]\n",
                "    if os.path.exists(repair_path):\n",
                "        repair_tokenizer = T5Tokenizer.from_pretrained(repair_path)\n",
                "        repair_model = T5ForConditionalGeneration.from_pretrained(\n",
                "            repair_path,\n",
                "            torch_dtype=torch.float16,\n",
                "            device_map=\"auto\"\n",
                "        )\n",
                "        repair_model.eval()\n",
                "        models[\"repair\"] = repair_model\n",
                "        tokenizers[\"repair\"] = repair_tokenizer\n",
                "        log(f\"   ‚úÖ Repair model loaded\")\n",
                "        log(f\"   GPU Memory: {get_gpu_memory():.1f} GB\")\n",
                "    else:\n",
                "        log(f\"   ‚ö†Ô∏è Repair model not found at {repair_path}\")\n",
                "except Exception as e:\n",
                "    log(f\"   ‚ùå Failed to load repair: {e}\")\n",
                "\n",
                "cleanup()\n",
                "\n",
                "# 3. Load DPO Generator\n",
                "log(\"\\n3. Loading DPO Generator...\")\n",
                "try:\n",
                "    base_model_name = \"gpt2-medium\"\n",
                "    gen_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
                "    if gen_tokenizer.pad_token is None:\n",
                "        gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
                "    \n",
                "    # Load base model\n",
                "    base_model = AutoModelForCausalLM.from_pretrained(\n",
                "        base_model_name,\n",
                "        torch_dtype=torch.float16,\n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    \n",
                "    # Load LoRA adapters\n",
                "    dpo_path = CONFIG[\"dpo_path\"]\n",
                "    if os.path.exists(dpo_path):\n",
                "        dpo_model = PeftModel.from_pretrained(base_model, dpo_path)\n",
                "        dpo_model.eval()\n",
                "        models[\"dpo\"] = dpo_model\n",
                "        tokenizers[\"dpo\"] = gen_tokenizer\n",
                "        log(f\"   ‚úÖ DPO model loaded with LoRA\")\n",
                "    else:\n",
                "        models[\"baseline\"] = base_model\n",
                "        tokenizers[\"baseline\"] = gen_tokenizer\n",
                "        log(f\"   ‚ö†Ô∏è DPO not found, using baseline GPT2\")\n",
                "    \n",
                "    log(f\"   GPU Memory: {get_gpu_memory():.1f} GB\")\n",
                "except Exception as e:\n",
                "    log(f\"   ‚ùå Failed to load generator: {e}\")\n",
                "\n",
                "cleanup()\n",
                "log(f\"\\n‚úÖ Models loaded: {list(models.keys())}\")\n",
                "log(f\"Total GPU Memory: {get_gpu_memory():.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Load Test Data\n",
                "log(\"Loading test data...\")\n",
                "\n",
                "test_data = []\n",
                "\n",
                "# Try multiple possible paths\n",
                "possible_paths = [\n",
                "    os.path.join(CONFIG[\"test_data_path\"], \"test_examples.json\"),\n",
                "    os.path.join(CONFIG[\"test_data_path\"], \"val_examples.json\"),\n",
                "    os.path.join(CONFIG[\"test_data_path\"], \"dpo_val.json\"),\n",
                "    \"/kaggle/input/gricebench-test-data/test_examples.json\",\n",
                "]\n",
                "\n",
                "for path in possible_paths:\n",
                "    if os.path.exists(path):\n",
                "        with open(path) as f:\n",
                "            test_data = json.load(f)\n",
                "        log(f\"‚úÖ Loaded {len(test_data)} examples from {path}\")\n",
                "        break\n",
                "\n",
                "if not test_data:\n",
                "    log(\"‚ö†Ô∏è No test data found, creating synthetic prompts...\")\n",
                "    # Create synthetic test data\n",
                "    test_data = [\n",
                "        {\"context\": \"What is the capital of France?\", \"prompt\": \"What is the capital of France?\"},\n",
                "        {\"context\": \"How does photosynthesis work?\", \"prompt\": \"How does photosynthesis work?\"},\n",
                "        {\"context\": \"What's your favorite food?\", \"prompt\": \"What's your favorite food?\"},\n",
                "    ] * 34  # 102 samples\n",
                "\n",
                "# Sample for speed\n",
                "import random\n",
                "random.seed(42)\n",
                "if len(test_data) > CONFIG[\"num_samples\"]:\n",
                "    test_data = random.sample(test_data, CONFIG[\"num_samples\"])\n",
                "\n",
                "log(f\"Using {len(test_data)} test samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Core Functions (GPU Optimized)\n",
                "\n",
                "MAXIMS = [\"quantity\", \"quality\", \"relation\", \"manner\"]\n",
                "\n",
                "@torch.no_grad()\n",
                "def detect_violations(context, response, threshold=0.5):\n",
                "    \"\"\"Detect maxim violations in a response.\"\"\"\n",
                "    if \"detector\" not in models:\n",
                "        return {m: False for m in MAXIMS}\n",
                "    \n",
                "    detector = models[\"detector\"]\n",
                "    tokenizer = tokenizers[\"detector\"]\n",
                "    \n",
                "    text = f\"Context: {context}\\nResponse: {response}\"\n",
                "    inputs = tokenizer(\n",
                "        text, \n",
                "        return_tensors=\"pt\", \n",
                "        truncation=True, \n",
                "        max_length=512,\n",
                "        padding=True\n",
                "    ).to(device)\n",
                "    \n",
                "    logits = detector(**inputs)\n",
                "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
                "    \n",
                "    return {\n",
                "        m: bool(probs[i] > threshold) \n",
                "        for i, m in enumerate(MAXIMS)\n",
                "    }, {m: float(probs[i]) for i, m in enumerate(MAXIMS)}\n",
                "\n",
                "@torch.no_grad()\n",
                "def generate_response(context, use_dpo=True):\n",
                "    \"\"\"Generate response with DPO or baseline.\"\"\"\n",
                "    model_key = \"dpo\" if use_dpo and \"dpo\" in models else \"baseline\"\n",
                "    if model_key not in models:\n",
                "        return \"[Model not available]\"\n",
                "    \n",
                "    model = models[model_key]\n",
                "    tokenizer = tokenizers.get(model_key) or tokenizers.get(\"dpo\") or tokenizers.get(\"baseline\")\n",
                "    \n",
                "    prompt = f\"Context: {context}\\nGenerate a cooperative response:\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
                "    \n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
                "        temperature=0.7,\n",
                "        do_sample=True,\n",
                "        pad_token_id=tokenizer.pad_token_id,\n",
                "        num_return_sequences=1\n",
                "    )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    # Extract only the generated part\n",
                "    if \"cooperative response:\" in response.lower():\n",
                "        response = response.split(\"cooperative response:\")[-1].strip()\n",
                "    \n",
                "    return response[:500]\n",
                "\n",
                "@torch.no_grad()\n",
                "def repair_response(context, response, violations):\n",
                "    \"\"\"Repair a response using T5 model.\"\"\"\n",
                "    if \"repair\" not in models or not any(violations.values()):\n",
                "        return response\n",
                "    \n",
                "    repair_model = models[\"repair\"]\n",
                "    tokenizer = tokenizers[\"repair\"]\n",
                "    \n",
                "    # Get violation types\n",
                "    violation_types = [m.upper() for m, v in violations.items() if v]\n",
                "    violation_str = \",\".join(violation_types)\n",
                "    \n",
                "    input_text = f\"[REPAIR] [VIOLATION={violation_str}] [CONTEXT] {context} [RESPONSE] {response}\"\n",
                "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
                "    \n",
                "    outputs = repair_model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=150,\n",
                "        num_beams=4,\n",
                "        early_stopping=True\n",
                "    )\n",
                "    \n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "log(\"Core functions defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Ablation Study 1 - Component Ablation\n",
                "log(\"=\"*70)\n",
                "log(\"ABLATION STUDY 1: COMPONENT ABLATION\")\n",
                "log(\"=\"*70)\n",
                "\n",
                "component_results = {}\n",
                "\n",
                "configurations = {\n",
                "    \"full_system\": {\"use_dpo\": True, \"use_detector\": True, \"use_repair\": True},\n",
                "    \"dpo_only\": {\"use_dpo\": True, \"use_detector\": False, \"use_repair\": False},\n",
                "    \"detect_repair\": {\"use_dpo\": False, \"use_detector\": True, \"use_repair\": True},\n",
                "    \"baseline\": {\"use_dpo\": False, \"use_detector\": False, \"use_repair\": False},\n",
                "}\n",
                "\n",
                "for config_name, config in configurations.items():\n",
                "    log(f\"\\n{'‚îÄ'*50}\")\n",
                "    log(f\"Testing: {config_name.upper()}\")\n",
                "    log(f\"Config: {config}\")\n",
                "    log(f\"{'‚îÄ'*50}\")\n",
                "    \n",
                "    violations_count = {m: 0 for m in MAXIMS}\n",
                "    total_samples = 0\n",
                "    start_time = time.time()\n",
                "    \n",
                "    for i, item in enumerate(test_data):\n",
                "        context = item.get(\"context\") or item.get(\"prompt\", \"\")\n",
                "        \n",
                "        # Generate response\n",
                "        response = generate_response(context, use_dpo=config[\"use_dpo\"])\n",
                "        \n",
                "        # Detect and optionally repair\n",
                "        if config[\"use_detector\"]:\n",
                "            violations, probs = detect_violations(context, response)\n",
                "            \n",
                "            if config[\"use_repair\"] and any(violations.values()):\n",
                "                response = repair_response(context, response, violations)\n",
                "                violations, probs = detect_violations(context, response)\n",
                "        else:\n",
                "            violations, probs = detect_violations(context, response)\n",
                "        \n",
                "        # Count violations\n",
                "        for m in MAXIMS:\n",
                "            if violations.get(m, False):\n",
                "                violations_count[m] += 1\n",
                "        total_samples += 1\n",
                "        \n",
                "        # Progress\n",
                "        if (i + 1) % 10 == 0:\n",
                "            elapsed = time.time() - start_time\n",
                "            rate = (i + 1) / elapsed\n",
                "            eta = (len(test_data) - i - 1) / rate\n",
                "            log(f\"   [{i+1:3d}/{len(test_data)}] {rate:.1f} samples/sec | ETA: {eta:.0f}s | GPU: {get_gpu_memory():.1f}GB\")\n",
                "    \n",
                "    # Calculate rates\n",
                "    violation_rates = {m: violations_count[m] / total_samples for m in MAXIMS}\n",
                "    cooperative_rate = sum(1 for i in range(total_samples) \n",
                "                          if not any(violations_count[m] > i for m in MAXIMS)) / total_samples\n",
                "    \n",
                "    component_results[config_name] = {\n",
                "        \"violation_rates\": violation_rates,\n",
                "        \"cooperative_rate\": 1.0 - sum(violation_rates.values()) / 4,\n",
                "        \"samples\": total_samples,\n",
                "        \"time_sec\": time.time() - start_time\n",
                "    }\n",
                "    \n",
                "    log(f\"\\n   üìä Results for {config_name}:\")\n",
                "    for m, rate in violation_rates.items():\n",
                "        log(f\"      {m:10s}: {rate*100:5.1f}% violation\")\n",
                "    log(f\"      {'Overall':10s}: {component_results[config_name]['cooperative_rate']*100:.1f}% cooperative\")\n",
                "    \n",
                "    cleanup()\n",
                "\n",
                "# Save checkpoint\n",
                "with open(f\"{CONFIG['output_dir']}/component_ablation.json\", \"w\") as f:\n",
                "    json.dump(component_results, f, indent=2)\n",
                "log(f\"\\nüíæ Saved to {CONFIG['output_dir']}/component_ablation.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Ablation Study 2 - Threshold Sensitivity\n",
                "log(\"=\"*70)\n",
                "log(\"ABLATION STUDY 2: THRESHOLD SENSITIVITY\")\n",
                "log(\"=\"*70)\n",
                "\n",
                "threshold_results = {}\n",
                "\n",
                "for threshold in CONFIG[\"thresholds\"]:\n",
                "    log(f\"\\nTesting threshold: {threshold}\")\n",
                "    \n",
                "    violations_count = {m: 0 for m in MAXIMS}\n",
                "    total = 0\n",
                "    start = time.time()\n",
                "    \n",
                "    for i, item in enumerate(test_data[:50]):  # Use 50 samples for speed\n",
                "        context = item.get(\"context\") or item.get(\"prompt\", \"\")\n",
                "        response = generate_response(context, use_dpo=True)\n",
                "        violations, _ = detect_violations(context, response, threshold=threshold)\n",
                "        \n",
                "        for m in MAXIMS:\n",
                "            if violations.get(m, False):\n",
                "                violations_count[m] += 1\n",
                "        total += 1\n",
                "    \n",
                "    violation_rates = {m: violations_count[m] / total for m in MAXIMS}\n",
                "    overall_rate = sum(violation_rates.values()) / 4\n",
                "    \n",
                "    threshold_results[str(threshold)] = {\n",
                "        \"violation_rates\": violation_rates,\n",
                "        \"overall_violation_rate\": overall_rate,\n",
                "        \"samples\": total\n",
                "    }\n",
                "    \n",
                "    log(f\"   Threshold {threshold}: {overall_rate*100:.1f}% average violation rate\")\n",
                "\n",
                "# Save\n",
                "with open(f\"{CONFIG['output_dir']}/threshold_ablation.json\", \"w\") as f:\n",
                "    json.dump(threshold_results, f, indent=2)\n",
                "log(f\"\\nüíæ Saved threshold results\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 9: Ablation Study 3 - Maxim Importance\n",
                "log(\"=\"*70)\n",
                "log(\"ABLATION STUDY 3: MAXIM IMPORTANCE\")\n",
                "log(\"=\"*70)\n",
                "\n",
                "maxim_results = {}\n",
                "\n",
                "# Count how often each maxim is violated alone vs with others\n",
                "solo_violations = {m: 0 for m in MAXIMS}\n",
                "co_violations = {m: {n: 0 for n in MAXIMS} for m in MAXIMS}\n",
                "total_violations = {m: 0 for m in MAXIMS}\n",
                "total_samples = 0\n",
                "\n",
                "log(\"\\nAnalyzing maxim patterns...\")\n",
                "for i, item in enumerate(test_data):\n",
                "    context = item.get(\"context\") or item.get(\"prompt\", \"\")\n",
                "    response = generate_response(context, use_dpo=True)\n",
                "    violations, probs = detect_violations(context, response)\n",
                "    \n",
                "    violated = [m for m in MAXIMS if violations.get(m, False)]\n",
                "    \n",
                "    for m in violated:\n",
                "        total_violations[m] += 1\n",
                "        if len(violated) == 1:\n",
                "            solo_violations[m] += 1\n",
                "        for n in violated:\n",
                "            co_violations[m][n] += 1\n",
                "    \n",
                "    total_samples += 1\n",
                "    \n",
                "    if (i + 1) % 20 == 0:\n",
                "        log(f\"   [{i+1:3d}/{len(test_data)}] processed\")\n",
                "\n",
                "# Calculate importance scores\n",
                "importance_scores = {}\n",
                "for m in MAXIMS:\n",
                "    rate = total_violations[m] / total_samples if total_samples > 0 else 0\n",
                "    solo_rate = solo_violations[m] / total_samples if total_samples > 0 else 0\n",
                "    independence = solo_violations[m] / total_violations[m] if total_violations[m] > 0 else 0\n",
                "    \n",
                "    importance_scores[m] = {\n",
                "        \"violation_rate\": rate,\n",
                "        \"solo_rate\": solo_rate,\n",
                "        \"independence\": independence,\n",
                "        \"co_occurrences\": {n: co_violations[m][n] for n in MAXIMS if n != m}\n",
                "    }\n",
                "    \n",
                "    log(f\"\\n   {m.upper()}:\")\n",
                "    log(f\"      Violation Rate: {rate*100:.1f}%\")\n",
                "    log(f\"      Solo Rate: {solo_rate*100:.1f}%\")\n",
                "    log(f\"      Independence: {independence*100:.1f}%\")\n",
                "\n",
                "maxim_results = {\n",
                "    \"importance_scores\": importance_scores,\n",
                "    \"total_samples\": total_samples\n",
                "}\n",
                "\n",
                "# Save\n",
                "with open(f\"{CONFIG['output_dir']}/maxim_ablation.json\", \"w\") as f:\n",
                "    json.dump(maxim_results, f, indent=2)\n",
                "log(f\"\\nüíæ Saved maxim importance results\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 10: Generate Comprehensive Report\n",
                "log(\"=\"*70)\n",
                "log(\"GENERATING ABLATION REPORT\")\n",
                "log(\"=\"*70)\n",
                "\n",
                "report = f\"\"\"# GriceBench Ablation Study Report\n",
                "\n",
                "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
                "\n",
                "## Executive Summary\n",
                "\n",
                "This report presents ablation studies measuring the contribution of each GriceBench component.\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Component Ablation\n",
                "\n",
                "Testing which system components contribute to reducing violations.\n",
                "\n",
                "| Configuration | Quantity | Quality | Relation | Manner | Cooperative Rate |\n",
                "|--------------|----------|---------|----------|--------|------------------|\n",
                "\"\"\"\n",
                "\n",
                "for config, data in component_results.items():\n",
                "    rates = data[\"violation_rates\"]\n",
                "    coop = data[\"cooperative_rate\"]\n",
                "    report += f\"| {config} | {rates['quantity']*100:.1f}% | {rates['quality']*100:.1f}% | {rates['relation']*100:.1f}% | {rates['manner']*100:.1f}% | {coop*100:.1f}% |\\n\"\n",
                "\n",
                "report += f\"\"\"\n",
                "### Key Findings:\n",
                "- **Full System** achieves {component_results.get('full_system', {}).get('cooperative_rate', 0)*100:.1f}% cooperative rate\n",
                "- **DPO Only** shows the value of preference learning\n",
                "- **Detect+Repair** demonstrates post-hoc correction capability\n",
                "\n",
                "---\n",
                "\n",
                "## 2. Threshold Sensitivity\n",
                "\n",
                "How detection threshold affects violation rates.\n",
                "\n",
                "| Threshold | Overall Violation Rate |\n",
                "|-----------|------------------------|\n",
                "\"\"\"\n",
                "\n",
                "for thresh, data in threshold_results.items():\n",
                "    report += f\"| {thresh} | {data['overall_violation_rate']*100:.1f}% |\\n\"\n",
                "\n",
                "report += f\"\"\"\n",
                "---\n",
                "\n",
                "## 3. Maxim Importance\n",
                "\n",
                "Contribution of each maxim to overall performance.\n",
                "\n",
                "| Maxim | Violation Rate | Solo Rate | Independence |\n",
                "|-------|----------------|-----------|---------------|\n",
                "\"\"\"\n",
                "\n",
                "for m, data in maxim_results.get(\"importance_scores\", {}).items():\n",
                "    report += f\"| {m.capitalize()} | {data['violation_rate']*100:.1f}% | {data['solo_rate']*100:.1f}% | {data['independence']*100:.1f}% |\\n\"\n",
                "\n",
                "report += f\"\"\"\n",
                "### Interpretation:\n",
                "- **Violation Rate**: How often this maxim is violated\n",
                "- **Solo Rate**: How often ONLY this maxim is violated\n",
                "- **Independence**: Proportion of violations that occur alone\n",
                "\n",
                "---\n",
                "\n",
                "## Conclusions\n",
                "\n",
                "1. The full GriceBench system outperforms individual components\n",
                "2. DPO training provides the largest contribution to cooperative behavior\n",
                "3. Detection + Repair provides incremental but measurable improvement\n",
                "4. Each maxim detector captures distinct violation patterns\n",
                "\n",
                "---\n",
                "\n",
                "*Report generated by GriceBench Ablation Study Framework*\n",
                "\"\"\"\n",
                "\n",
                "# Save report\n",
                "report_path = f\"{CONFIG['output_dir']}/ablation_report.md\"\n",
                "with open(report_path, \"w\") as f:\n",
                "    f.write(report)\n",
                "\n",
                "log(f\"\\nüìÑ Report saved to {report_path}\")\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(report)\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 11: Save All Results\n",
                "log(\"=\"*70)\n",
                "log(\"SAVING FINAL RESULTS\")\n",
                "log(\"=\"*70)\n",
                "\n",
                "all_results = {\n",
                "    \"config\": CONFIG,\n",
                "    \"component_ablation\": component_results,\n",
                "    \"threshold_ablation\": threshold_results,\n",
                "    \"maxim_ablation\": maxim_results,\n",
                "    \"timestamp\": datetime.now().isoformat(),\n",
                "    \"device\": str(device),\n",
                "    \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"\n",
                "}\n",
                "\n",
                "# Save comprehensive results\n",
                "results_path = f\"{CONFIG['output_dir']}/ablation_results.json\"\n",
                "with open(results_path, \"w\") as f:\n",
                "    json.dump(all_results, f, indent=2, default=str)\n",
                "\n",
                "log(f\"\\n‚úÖ All results saved to {CONFIG['output_dir']}/\")\n",
                "log(\"\\nFiles created:\")\n",
                "for f in os.listdir(CONFIG['output_dir']):\n",
                "    size = os.path.getsize(os.path.join(CONFIG['output_dir'], f))\n",
                "    log(f\"   üìÅ {f} ({size/1024:.1f} KB)\")\n",
                "\n",
                "log(\"\\n\" + \"=\"*70)\n",
                "log(\"üéâ ABLATION STUDIES COMPLETE!\")\n",
                "log(\"=\"*70)\n",
                "log(\"\\nDownload the files from /kaggle/working/ablation_results/\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}