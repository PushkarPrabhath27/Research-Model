{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GriceBench Part 3: Production Baseline Comparisons\n",
                "\n",
                "## Models Tested (Industry Standard):\n",
                "| Model | Params | Why Include |\n",
                "|-------|--------|-------------|\n",
                "| **Mistral-7B-Instruct** | 7B | Top open-source, beats many 13B models |\n",
                "| **Qwen2.5-7B-Instruct** | 7B | Alibaba's latest, excellent performance |\n",
                "| **Llama-3.2-3B-Instruct** | 3B | Meta's latest efficient model |\n",
                "| **Phi-3-mini** | 3.8B | Microsoft's strong reasoning |\n",
                "| **Gemma-2-2B-it** | 2B | Google's latest efficient model |\n",
                "\n",
                "## GPU Optimizations:\n",
                "- 4-bit quantization (BitsAndBytes) for 7B models\n",
                "- Flash Attention 2 when available\n",
                "- Automatic memory management\n",
                "\n",
                "**Estimated Runtime**: ~1.5-2 hours\n",
                "\n",
                "## Setup:\n",
                "1. Upload to Kaggle\n",
                "2. Enable **GPU T4 x2**\n",
                "3. Enable **Internet**\n",
                "4. Run All"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Install Dependencies\n",
                "print(\"Installing optimized dependencies...\")\n",
                "!pip install -q transformers>=4.40.0 accelerate>=0.27.0\n",
                "!pip install -q bitsandbytes>=0.43.0\n",
                "!pip install -q sentence-transformers\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Configuration\n",
                "import torch\n",
                "import json\n",
                "import gc\n",
                "from pathlib import Path\n",
                "from datetime import datetime\n",
                "import numpy as np\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "CONFIG = {\n",
                "    \"num_samples\": 150,\n",
                "    \"max_new_tokens\": 200,\n",
                "    \"temperature\": 0.7,\n",
                "    \"output_dir\": \"/kaggle/working/baseline_comparison\",\n",
                "}\n",
                "\n",
                "BASELINES = {\n",
                "    \"mistral_7b\": {\"id\": \"mistralai/Mistral-7B-Instruct-v0.3\", \"4bit\": True},\n",
                "    \"qwen2.5_7b\": {\"id\": \"Qwen/Qwen2.5-7B-Instruct\", \"4bit\": True},\n",
                "    \"llama3.2_3b\": {\"id\": \"meta-llama/Llama-3.2-3B-Instruct\", \"4bit\": False},\n",
                "    \"phi3_mini\": {\"id\": \"microsoft/Phi-3-mini-4k-instruct\", \"4bit\": False},\n",
                "    \"gemma2_2b\": {\"id\": \"google/gemma-2-2b-it\", \"4bit\": False},\n",
                "}\n",
                "\n",
                "Path(CONFIG[\"output_dir\"]).mkdir(parents=True, exist_ok=True)\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
                "print(f\"Testing {len(BASELINES)} models on {CONFIG['num_samples']} prompts\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Test Prompts\n",
                "TEST_PROMPTS = [\n",
                "    {\"context\": \"What is the capital of France?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"How many planets are in our solar system?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"Who wrote Romeo and Juliet?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"What year did World War II end?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"What is the chemical symbol for gold?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"What is the speed of light?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"Who discovered penicillin?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"What is the largest organ in the human body?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"Which planet has the most moons?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"What is the smallest country by area?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"What is the Pythagorean theorem?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"Who painted the Mona Lisa?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"What is the boiling point of water?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"What does DNA stand for?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"Who invented the telephone?\", \"type\": \"factual\"},\n",
                "    {\"context\": \"How does photosynthesis work?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"Why is the sky blue?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"How do vaccines work?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"What causes earthquakes?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"How does the internet work?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"Why do we dream?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"How does a computer processor work?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"What is machine learning?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"How do airplanes fly?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"Why do leaves change color?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"How does electricity work?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"What causes inflation?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"How does GPS work?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"How do black holes form?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"What is blockchain technology?\", \"type\": \"explanation\"},\n",
                "    {\"context\": \"How can I improve my sleep quality?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"What's a good way to learn a new language?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"How do I make friends in a new city?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"What's the best way to save money?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"How can I be more productive?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"How should I prepare for a job interview?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"What's a healthy diet look like?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"How can I reduce stress?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"What's the best way to learn to code?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"How do I negotiate a salary raise?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"How can I improve public speaking?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"What should I consider when buying a house?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"How do I start investing?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"How can I build better habits?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"What's a good exercise routine for beginners?\", \"type\": \"advice\"},\n",
                "    {\"context\": \"What's your favorite food?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"How was your day?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"What do you like to do for fun?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"Do you have any hobbies?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"What kind of music do you enjoy?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"Have you seen any good movies lately?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"What's your opinion on remote work?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"Do you prefer cats or dogs?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"What's your dream vacation destination?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"If you could have any superpower, what would it be?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"What's your favorite book?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"Do you prefer mornings or nights?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"What's something you're grateful for?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"What would you do if you won the lottery?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"What's your favorite season and why?\", \"type\": \"conversational\"},\n",
                "    {\"context\": \"What is the difference between Python and JavaScript?\", \"type\": \"technical\"},\n",
                "    {\"context\": \"Explain object-oriented programming.\", \"type\": \"technical\"},\n",
                "    {\"context\": \"What is a REST API?\", \"type\": \"technical\"},\n",
                "    {\"context\": \"How do neural networks learn?\", \"type\": \"technical\"},\n",
                "    {\"context\": \"What's the difference between SQL and NoSQL?\", \"type\": \"technical\"},\n",
                "    {\"context\": \"Explain recursion in programming.\", \"type\": \"technical\"},\n",
                "    {\"context\": \"What is version control and why is Git popular?\", \"type\": \"technical\"},\n",
                "    {\"context\": \"What is containerization and Docker?\", \"type\": \"technical\"},\n",
                "    {\"context\": \"What is Big O notation?\", \"type\": \"technical\"},\n",
                "    {\"context\": \"How does HTTPS encryption work?\", \"type\": \"technical\"},\n",
                "    {\"context\": \"Write a short poem about the ocean.\", \"type\": \"creative\"},\n",
                "    {\"context\": \"Tell me a story about a robot learning emotions.\", \"type\": \"creative\"},\n",
                "    {\"context\": \"Describe a perfect day.\", \"type\": \"creative\"},\n",
                "    {\"context\": \"Write a haiku about autumn.\", \"type\": \"creative\"},\n",
                "    {\"context\": \"Describe an imaginary planet.\", \"type\": \"creative\"},\n",
                "    {\"context\": \"Write a motivational quote about perseverance.\", \"type\": \"creative\"},\n",
                "    {\"context\": \"Describe the taste of your favorite food vividly.\", \"type\": \"creative\"},\n",
                "    {\"context\": \"Describe what happiness looks like.\", \"type\": \"creative\"},\n",
                "    {\"context\": \"Describe a futuristic city in year 3000.\", \"type\": \"creative\"},\n",
                "    {\"context\": \"Write a letter to your future self.\", \"type\": \"creative\"},\n",
                "]\n",
                "\n",
                "import random\n",
                "random.seed(42)\n",
                "while len(TEST_PROMPTS) < CONFIG[\"num_samples\"]:\n",
                "    TEST_PROMPTS.append(random.choice(TEST_PROMPTS[:80]).copy())\n",
                "TEST_PROMPTS = TEST_PROMPTS[:CONFIG[\"num_samples\"]]\n",
                "print(f\"Test prompts: {len(TEST_PROMPTS)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Optimized Generator\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "\n",
                "def load_model(model_id, use_4bit=False):\n",
                "    print(f\"Loading {model_id}...\")\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
                "    if tokenizer.pad_token is None:\n",
                "        tokenizer.pad_token = tokenizer.eos_token\n",
                "    \n",
                "    kwargs = {\n",
                "        \"trust_remote_code\": True,\n",
                "        \"device_map\": \"auto\",\n",
                "        \"torch_dtype\": torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
                "    }\n",
                "    \n",
                "    if use_4bit:\n",
                "        kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
                "            load_in_4bit=True,\n",
                "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "            bnb_4bit_use_double_quant=True,\n",
                "            bnb_4bit_quant_type=\"nf4\"\n",
                "        )\n",
                "    \n",
                "    model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
                "    model.eval()\n",
                "    \n",
                "    mem = torch.cuda.memory_allocated() / 1e9\n",
                "    print(f\"  Loaded! GPU Memory: {mem:.1f} GB\")\n",
                "    return model, tokenizer\n",
                "\n",
                "def generate(model, tokenizer, context):\n",
                "    if hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template:\n",
                "        msgs = [{\"role\": \"user\", \"content\": context}]\n",
                "        prompt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
                "    else:\n",
                "        prompt = f\"User: {context}\\nAssistant:\"\n",
                "    \n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        out = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
                "            temperature=CONFIG[\"temperature\"],\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.pad_token_id,\n",
                "        )\n",
                "    \n",
                "    resp = tokenizer.decode(out[0], skip_special_tokens=True)\n",
                "    if prompt in resp:\n",
                "        resp = resp[len(prompt):].strip()\n",
                "    return resp[:500]\n",
                "\n",
                "def unload(model, tokenizer):\n",
                "    del model, tokenizer\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Evaluator\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "print(\"Loading evaluator...\")\n",
                "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "\n",
                "def evaluate(context, response):\n",
                "    emb = encoder.encode([context, response], normalize_embeddings=True)\n",
                "    relevance = float(np.dot(emb[0], emb[1]))\n",
                "    \n",
                "    words = len(response.split())\n",
                "    quantity = 1.0 if 15 <= words <= 150 else 0.5 if words < 15 else 0.7\n",
                "    \n",
                "    unique = len(set(response.lower().split())) / max(1, len(response.split()))\n",
                "    clarity = min(1.0, unique * 1.2)\n",
                "    \n",
                "    overall = relevance * 0.4 + quantity * 0.3 + clarity * 0.3\n",
                "    return {\"relevance\": relevance, \"quantity\": quantity, \"clarity\": clarity, \"overall\": overall}\n",
                "\n",
                "print(\"Evaluator ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Run All Baselines\n",
                "all_results = {}\n",
                "\n",
                "for name, cfg in BASELINES.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Testing: {name}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    try:\n",
                "        model, tokenizer = load_model(cfg[\"id\"], cfg[\"4bit\"])\n",
                "        results = []\n",
                "        \n",
                "        for p in tqdm(TEST_PROMPTS, desc=name):\n",
                "            try:\n",
                "                resp = generate(model, tokenizer, p[\"context\"])\n",
                "                metrics = evaluate(p[\"context\"], resp)\n",
                "                results.append({\"context\": p[\"context\"], \"type\": p[\"type\"], \"response\": resp, \"metrics\": metrics})\n",
                "            except Exception as e:\n",
                "                print(f\"Error: {e}\")\n",
                "        \n",
                "        all_results[name] = results\n",
                "        \n",
                "        if results:\n",
                "            print(f\"\\nResults for {name}:\")\n",
                "            print(f\"  Relevance: {np.mean([r['metrics']['relevance'] for r in results]):.3f}\")\n",
                "            print(f\"  Overall:   {np.mean([r['metrics']['overall'] for r in results]):.3f}\")\n",
                "        \n",
                "        unload(model, tokenizer)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Failed to load {name}: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"ALL BASELINES COMPLETE!\")\n",
                "print(f\"{'='*60}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Generate Report\n",
                "summary = {}\n",
                "for name, results in all_results.items():\n",
                "    if results:\n",
                "        summary[name] = {\n",
                "            \"n\": len(results),\n",
                "            \"relevance\": np.mean([r[\"metrics\"][\"relevance\"] for r in results]),\n",
                "            \"quantity\": np.mean([r[\"metrics\"][\"quantity\"] for r in results]),\n",
                "            \"clarity\": np.mean([r[\"metrics\"][\"clarity\"] for r in results]),\n",
                "            \"overall\": np.mean([r[\"metrics\"][\"overall\"] for r in results]),\n",
                "        }\n",
                "\n",
                "report = f\"\"\"# Baseline Comparison Report\n",
                "\n",
                "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
                "\n",
                "## Overall Scores (Higher is Better)\n",
                "\n",
                "| Model | Relevance | Quantity | Clarity | Overall | N |\n",
                "|-------|-----------|----------|---------|---------|---|\n",
                "\"\"\"\n",
                "\n",
                "for name, s in sorted(summary.items(), key=lambda x: x[1][\"overall\"], reverse=True):\n",
                "    report += f\"| {name} | {s['relevance']:.3f} | {s['quantity']:.3f} | {s['clarity']:.3f} | **{s['overall']:.3f}** | {s['n']} |\\n\"\n",
                "\n",
                "report += \"\\n## Best Model by Metric\\n\\n\"\n",
                "for m in [\"relevance\", \"quantity\", \"clarity\", \"overall\"]:\n",
                "    best = max(summary.items(), key=lambda x: x[1][m])\n",
                "    report += f\"- **{m.capitalize()}**: {best[0]} ({best[1][m]:.3f})\\n\"\n",
                "\n",
                "with open(f\"{CONFIG['output_dir']}/baseline_comparison_report.md\", \"w\") as f:\n",
                "    f.write(report)\n",
                "\n",
                "with open(f\"{CONFIG['output_dir']}/baseline_results.json\", \"w\") as f:\n",
                "    json.dump({\"summary\": summary, \"config\": CONFIG}, f, indent=2, default=str)\n",
                "\n",
                "print(report)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Save All Responses\n",
                "responses = {}\n",
                "for name, results in all_results.items():\n",
                "    responses[name] = [{\"context\": r[\"context\"], \"type\": r[\"type\"], \"response\": r[\"response\"], \n",
                "                        \"metrics\": {k: float(v) for k,v in r[\"metrics\"].items()}} for r in results]\n",
                "\n",
                "with open(f\"{CONFIG['output_dir']}/all_responses.json\", \"w\") as f:\n",
                "    json.dump(responses, f, indent=2)\n",
                "\n",
                "print(\"\\nFiles saved:\")\n",
                "for f in Path(CONFIG[\"output_dir\"]).iterdir():\n",
                "    print(f\"  {f.name} ({f.stat().st_size/1024:.1f} KB)\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"PART 3 COMPLETE!\")\n",
                "print(\"=\"*60)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}