{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GriceBench Part 5: Error Analysis\n",
        "\n",
        "**Objective:** Comprehensive error analysis of the Gricean Maxim Detector\n",
        "\n",
        "## Analysis Components:\n",
        "1. Confusion matrices per maxim\n",
        "2. Hardest examples (confident errors)\n",
        "3. Failure mode categorization\n",
        "4. Qualitative examples for paper\n",
        "\n",
        "**Runtime:** ~10-15 minutes on Kaggle GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Imports and Setup\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
        "plt.rcParams[\"font.size\"] = 11\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "def log(msg):\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    print(f\"[{timestamp}] {msg}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "log(\"✅ Setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Configuration\n",
        "CONFIG = {\n",
        "    \"detector_path\": \"/kaggle/input/gricean-maxim-detector-model\",\n",
        "    \"val_data_path\": \"/kaggle/input/gricebench-test-data/val_examples.json\",\n",
        "    \"output_dir\": \"/kaggle/working/error_analysis\",\n",
        "    \"threshold\": 0.5,\n",
        "    \"top_k_errors\": 10,\n",
        "    \"batch_size\": 16,\n",
        "    \"max_length\": 512,\n",
        "}\n",
        "\n",
        "MAXIMS = [\"quantity\", \"quality\", \"relation\", \"manner\"]\n",
        "\n",
        "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
        "os.makedirs(f\"{CONFIG['output_dir']}/confusion_matrices\", exist_ok=True)\n",
        "\n",
        "log(f\"✅ Config loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Define Detector Architecture\n",
        "class ViolationDetector(nn.Module):\n",
        "    \"\"\"Gricean Maxim Violation Detector\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name=\"microsoft/deberta-v3-base\", num_labels=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled = outputs.last_hidden_state[:, 0, :]\n",
        "        pooled = self.dropout(pooled)\n",
        "        logits = self.classifier(pooled)\n",
        "        probs = self.sigmoid(logits)\n",
        "        return {'logits': logits, 'probs': probs}\n",
        "\n",
        "log(\"✅ Detector architecture defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Load Detector Model\n",
        "log(\"=\"*60)\n",
        "log(\"LOADING DETECTOR MODEL\")\n",
        "log(\"=\"*60)\n",
        "\n",
        "log(\"\\nLoading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
        "log(f\"  ✅ Tokenizer loaded (vocab: {len(tokenizer)})\")\n",
        "\n",
        "log(\"\\nCreating model...\")\n",
        "detector = ViolationDetector(\"microsoft/deberta-v3-base\")\n",
        "total_params = sum(p.numel() for p in detector.parameters())\n",
        "log(f\"  ✅ Model created ({total_params:,} params)\")\n",
        "\n",
        "log(\"\\nLoading weights...\")\n",
        "checkpoint_path = f\"{CONFIG['detector_path']}/best_model.pt\"\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "    if 'metrics' in checkpoint:\n",
        "        macro_f1 = checkpoint['metrics'].get('macro_f1', 'N/A')\n",
        "        log(f\"  Training F1: {macro_f1}\")\n",
        "else:\n",
        "    state_dict = checkpoint\n",
        "\n",
        "detector.load_state_dict(state_dict, strict=True)\n",
        "log(\"  ✅ Weights loaded\")\n",
        "\n",
        "detector = detector.to(device)\n",
        "detector.eval()\n",
        "log(f\"  ✅ Model on {device}\")\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Load Validation Data\n",
        "log(\"\\n\" + \"=\"*60)\n",
        "log(\"LOADING VALIDATION DATA\")\n",
        "log(\"=\"*60)\n",
        "\n",
        "val_path = CONFIG[\"val_data_path\"]\n",
        "\n",
        "if not os.path.exists(val_path):\n",
        "    alternatives = [\n",
        "        \"/kaggle/input/gricebench-test-data/test_examples.json\",\n",
        "    ]\n",
        "    for alt in alternatives:\n",
        "        if os.path.exists(alt):\n",
        "            val_path = alt\n",
        "            break\n",
        "\n",
        "log(f\"Loading: {val_path}\")\n",
        "\n",
        "with open(val_path, 'r', encoding='utf-8') as f:\n",
        "    val_examples = json.load(f)\n",
        "\n",
        "log(f\"  ✅ Loaded {len(val_examples)} examples\")\n",
        "\n",
        "log(f\"\\nDistribution:\")\n",
        "for maxim in MAXIMS:\n",
        "    count = sum(1 for ex in val_examples if ex.get('labels', {}).get(maxim, 0) == 1)\n",
        "    pct = count / len(val_examples) * 100\n",
        "    log(f\"  {maxim}: {count} ({pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Run Inference\n",
        "log(\"\\n\" + \"=\"*60)\n",
        "log(\"RUNNING INFERENCE\")\n",
        "log(\"=\"*60)\n",
        "\n",
        "@torch.no_grad()\n",
        "def batch_predict(examples, batch_size=16):\n",
        "    all_probs = []\n",
        "    all_preds = []\n",
        "    \n",
        "    for i in range(0, len(examples), batch_size):\n",
        "        batch = examples[i:i+batch_size]\n",
        "        texts = [ex['input_text'] for ex in batch]\n",
        "        \n",
        "        inputs = tokenizer(\n",
        "            texts,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            max_length=CONFIG['max_length'],\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "        \n",
        "        outputs = detector(inputs['input_ids'], inputs['attention_mask'])\n",
        "        probs = outputs['probs'].cpu().numpy()\n",
        "        preds = (probs > CONFIG['threshold']).astype(int)\n",
        "        \n",
        "        all_probs.append(probs)\n",
        "        all_preds.append(preds)\n",
        "        \n",
        "        if (i // batch_size + 1) % 10 == 0:\n",
        "            log(f\"  {i + len(batch)}/{len(examples)}...\")\n",
        "    \n",
        "    return np.vstack(all_probs), np.vstack(all_preds)\n",
        "\n",
        "all_probs, all_preds = batch_predict(val_examples, CONFIG['batch_size'])\n",
        "all_labels = np.array([[ex['labels'].get(m, 0) for m in MAXIMS] for ex in val_examples])\n",
        "\n",
        "exact_match = (all_preds == all_labels).all(axis=1).mean()\n",
        "log(f\"\\n  ✅ Done! Exact match: {exact_match:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Confusion Matrices\n",
        "log(\"\\n\" + \"=\"*60)\n",
        "log(\"GENERATING CONFUSION MATRICES\")\n",
        "log(\"=\"*60)\n",
        "\n",
        "confusion_matrices = {}\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, maxim in enumerate(MAXIMS):\n",
        "    y_true = all_labels[:, i]\n",
        "    y_pred = all_preds[:, i]\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    confusion_matrices[maxim] = cm\n",
        "    \n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    \n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=['No Violation', 'Violation'],\n",
        "        yticklabels=['No Violation', 'Violation'],\n",
        "        ax=axes[i],\n",
        "        cbar_kws={'label': 'Count'}\n",
        "    )\n",
        "    axes[i].set_title(f'{maxim.capitalize()} (F1={f1:.3f})', fontweight='bold')\n",
        "    axes[i].set_ylabel('True')\n",
        "    axes[i].set_xlabel('Predicted')\n",
        "    \n",
        "    tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "    log(f\"\\n{maxim}: TN={tn}, FP={fp}, FN={fn}, TP={tp}, F1={f1:.3f}\")\n",
        "\n",
        "plt.suptitle('GriceBench Detector - Confusion Matrices', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{CONFIG['output_dir']}/confusion_matrices/all_confusion_matrices.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "log(f\"\\n  ✅ Saved confusion matrices\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Hardest Examples\n",
        "log(\"\\n\" + \"=\"*60)\n",
        "log(\"IDENTIFYING HARDEST EXAMPLES\")\n",
        "log(\"=\"*60)\n",
        "\n",
        "hardest = {}\n",
        "\n",
        "for i, maxim in enumerate(MAXIMS):\n",
        "    y_true = all_labels[:, i]\n",
        "    y_pred = all_preds[:, i]\n",
        "    probs = all_probs[:, i]\n",
        "    \n",
        "    fp_mask = (y_pred == 1) & (y_true == 0)\n",
        "    fp_idx = np.where(fp_mask)[0]\n",
        "    top_fp = fp_idx[np.argsort(-probs[fp_idx])[:CONFIG['top_k_errors']]] if len(fp_idx) > 0 else []\n",
        "    \n",
        "    fn_mask = (y_pred == 0) & (y_true == 1)\n",
        "    fn_idx = np.where(fn_mask)[0]\n",
        "    top_fn = fn_idx[np.argsort(probs[fn_idx])[:CONFIG['top_k_errors']]] if len(fn_idx) > 0 else []\n",
        "    \n",
        "    hardest[maxim] = {\n",
        "        'false_positives': [\n",
        "            {\n",
        "                'index': int(idx),\n",
        "                'confidence': float(probs[idx]),\n",
        "                'text': val_examples[idx]['input_text'][:200],\n",
        "                'type': val_examples[idx].get('violation_type', 'unknown')\n",
        "            }\n",
        "            for idx in top_fp\n",
        "        ],\n",
        "        'false_negatives': [\n",
        "            {\n",
        "                'index': int(idx),\n",
        "                'confidence': float(probs[idx]),\n",
        "                'text': val_examples[idx]['input_text'][:200],\n",
        "                'type': val_examples[idx].get('violation_type', 'unknown')\n",
        "            }\n",
        "            for idx in top_fn\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    log(f\"\\n{maxim}: FP={len(top_fp)}, FN={len(top_fn)}\")\n",
        "\n",
        "with open(f\"{CONFIG['output_dir']}/hardest_examples.json\", 'w') as f:\n",
        "    json.dump(hardest, f, indent=2)\n",
        "\n",
        "log(f\"\\n  ✅ Saved hardest examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Generate Report\n",
        "log(\"\\n\" + \"=\"*60)\n",
        "log(\"GENERATING REPORT\")\n",
        "log(\"=\"*60)\n",
        "\n",
        "report = []\n",
        "report.append(f\"# Error Analysis Report\\n\\nGenerated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\")\n",
        "report.append(f\"**Exact Match Accuracy:** {exact_match:.3f}\\n\\n\")\n",
        "\n",
        "report.append(\"## Per-Maxim Performance\\n\\n\")\n",
        "report.append(\"| Maxim | F1 | Errors | TN | FP | FN | TP |\\n\")\n",
        "report.append(\"|-------|----|----|----|----|----|----|\\n\")\n",
        "\n",
        "for i, maxim in enumerate(MAXIMS):\n",
        "    y_true = all_labels[:, i]\n",
        "    y_pred = all_preds[:, i]\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    cm = confusion_matrices[maxim]\n",
        "    errors = (y_true != y_pred).sum()\n",
        "    \n",
        "    report.append(f\"| {maxim.capitalize()} | {f1:.3f} | {errors} | \")\n",
        "    report.append(f\"{cm[0,0]} | {cm[0,1]} | {cm[1,0]} | {cm[1,1]} |\\n\")\n",
        "\n",
        "report.append(\"\\n![Confusion Matrices](confusion_matrices/all_confusion_matrices.png)\\n\")\n",
        "\n",
        "with open(f\"{CONFIG['output_dir']}/error_report.md\", 'w') as f:\n",
        "    f.writelines(report)\n",
        "\n",
        "log(\"\\n  ✅ Report saved\")\n",
        "log(\"\\n\" + \"=\"*60)\n",
        "log(\"ERROR ANALYSIS COMPLETE\")\n",
        "log(\"=\"*60)\n",
        "log(f\"Output: {CONFIG['output_dir']}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
