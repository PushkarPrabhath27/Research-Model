{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ STEP 1-3: Score, Filter, Merge Synthetic Data\n",
                "\n",
                "**Pipeline:**\n",
                "1. Load 4151 synthetic pairs\n",
                "2. Score with Gricean reward models (4 maxims)\n",
                "3. Filter for all margins > 0\n",
                "4. Merge with 411 clean pairs\n",
                "5. Save final DPO dataset\n",
                "\n",
                "**Expected Output:** ~3,400 high-quality DPO pairs\n",
                "\n",
                "**Setup:**\n",
                "- GPU: T4 x2\n",
                "- Datasets: `synthetic_candidates.json`, `clean_dpo_pairs.json`\n",
                "- Models: Your 4 partial reward models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Install & Import\n",
                "!pip install -q transformers torch accelerate datasets\n",
                "\n",
                "import json, torch, os\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
                "print(\"‚úÖ Ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Configuration\n",
                "\n",
                "# Input files\n",
                "SYNTHETIC_FILE = None\n",
                "CLEAN_FILE = None\n",
                "\n",
                "# Try to find synthetic candidates\n",
                "for p in [\"/kaggle/input/synthetic-candidates/synthetic_candidates.json\",\n",
                "          \"/kaggle/input/synthetic-data/synthetic_candidates.json\"]:\n",
                "    if os.path.exists(p): SYNTHETIC_FILE = p; break\n",
                "\n",
                "# Try to find clean pairs\n",
                "for p in [\"/kaggle/input/clean-dpo-pairs/clean_dpo_pairs.json\",\n",
                "          \"/kaggle/input/clean-pairs/clean_dpo_pairs.json\"]:\n",
                "    if os.path.exists(p): CLEAN_FILE = p; break\n",
                "\n",
                "if not SYNTHETIC_FILE:\n",
                "    raise FileNotFoundError(\"Upload synthetic_candidates.json!\")\n",
                "if not CLEAN_FILE:\n",
                "    raise FileNotFoundError(\"Upload clean_dpo_pairs.json!\")\n",
                "\n",
                "OUTPUT_FILE = \"/kaggle/working/final_dpo_dataset.json\"\n",
                "\n",
                "print(f\"Synthetic: {SYNTHETIC_FILE}\")\n",
                "print(f\"Clean: {CLEAN_FILE}\")\n",
                "print(f\"Output: {OUTPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Load Gricean Reward Models\n",
                "\n",
                "print(\"Loading 4 Gricean reward models...\\n\")\n",
                "\n",
                "# Model paths (update these to your HuggingFace paths)\n",
                "MODEL_PATHS = {\n",
                "    'quantity': 'Pushkar27/MaxMargin-RM-Partial-Quantity',\n",
                "    'quality': 'Pushkar27/MaxMargin-RM-Partial-Quality',\n",
                "    'relation': 'Pushkar27/MaxMargin-RM-Partial-Relation',\n",
                "    'manner': 'Pushkar27/MaxMargin-RM-Partial-Manner'\n",
                "}\n",
                "\n",
                "models = {}\n",
                "tokenizers = {}\n",
                "\n",
                "for maxim, path in MODEL_PATHS.items():\n",
                "    print(f\"Loading {maxim}...\")\n",
                "    tokenizers[maxim] = AutoTokenizer.from_pretrained(path)\n",
                "    models[maxim] = AutoModelForSequenceClassification.from_pretrained(\n",
                "        path,\n",
                "        torch_dtype=torch.float16,\n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "    models[maxim].eval()\n",
                "\n",
                "print(\"\\n‚úÖ All 4 models loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Scoring Functions\n",
                "\n",
                "def score_response(prompt, response, maxim):\n",
                "    \"\"\"Score a single response for a given maxim\"\"\"\n",
                "    text = f\"{prompt}\\n\\nResponse: {response}\"\n",
                "    \n",
                "    inputs = tokenizers[maxim](\n",
                "        text,\n",
                "        return_tensors=\"pt\",\n",
                "        truncation=True,\n",
                "        max_length=512\n",
                "    ).to(models[maxim].device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = models[maxim](**inputs)\n",
                "        # Assuming logits[0] is the score\n",
                "        score = outputs.logits[0][0].cpu().item()\n",
                "    \n",
                "    return score\n",
                "\n",
                "def score_pair(prompt, chosen, rejected):\n",
                "    \"\"\"Score a DPO pair and return margins for all maxims\"\"\"\n",
                "    margins = {}\n",
                "    \n",
                "    for maxim in ['quantity', 'quality', 'relation', 'manner']:\n",
                "        chosen_score = score_response(prompt, chosen, maxim)\n",
                "        rejected_score = score_response(prompt, rejected, maxim)\n",
                "        margins[maxim] = chosen_score - rejected_score\n",
                "    \n",
                "    return margins\n",
                "\n",
                "print(\"‚úÖ Scoring functions defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: STEP 1 - Score Synthetic Pairs\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"STEP 1: SCORING SYNTHETIC PAIRS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Load synthetic candidates\n",
                "with open(SYNTHETIC_FILE) as f:\n",
                "    synthetic_data = json.load(f)\n",
                "\n",
                "print(f\"\\nLoaded {len(synthetic_data)} synthetic pairs\")\n",
                "print(f\"Expected time: ~{len(synthetic_data) * 2 / 3600:.1f} hours (8 model calls per pair)\\n\")\n",
                "\n",
                "scored_synthetic = []\n",
                "stats = {'total': 0, 'errors': 0}\n",
                "\n",
                "for i, item in enumerate(tqdm(synthetic_data, desc=\"Scoring\")):\n",
                "    try:\n",
                "        # Score the pair: synthetic (chosen) vs original failed (rejected)\n",
                "        margins = score_pair(\n",
                "            prompt=item['prompt'],\n",
                "            chosen=item['synthetic_chosen'],\n",
                "            rejected=item['original_chosen_failed']\n",
                "        )\n",
                "        \n",
                "        # Add margins to item\n",
                "        item['synthetic_margins'] = margins\n",
                "        scored_synthetic.append(item)\n",
                "        stats['total'] += 1\n",
                "        \n",
                "        # Progress update every 100 items\n",
                "        if (i+1) % 100 == 0:\n",
                "            print(f\"\\nProgress: {i+1}/{len(synthetic_data)}\")\n",
                "            print(f\"Errors: {stats['errors']}\")\n",
                "            print(f\"Last margins: {margins}\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        stats['errors'] += 1\n",
                "        print(f\"\\nError at {i}: {str(e)[:100]}\")\n",
                "        if stats['errors'] > 50:\n",
                "            print(\"\\n‚ö†Ô∏è Too many errors, stopping...\")\n",
                "            break\n",
                "\n",
                "print(f\"\\n‚úÖ Scored {stats['total']} pairs (Errors: {stats['errors']})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: STEP 2 - Filter for Positive Margins\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"STEP 2: FILTERING FOR POSITIVE MARGINS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Filter: ALL margins must be > 0\n",
                "filtered_synthetic = [\n",
                "    item for item in scored_synthetic\n",
                "    if all(item['synthetic_margins'][m] > 0 for m in ['quantity', 'quality', 'relation', 'manner'])\n",
                "]\n",
                "\n",
                "print(f\"\\nBefore filtering: {len(scored_synthetic)} pairs\")\n",
                "print(f\"After filtering: {len(filtered_synthetic)} pairs\")\n",
                "print(f\"Pass rate: {len(filtered_synthetic)/len(scored_synthetic)*100:.1f}%\")\n",
                "\n",
                "# Statistics on filtered data\n",
                "if filtered_synthetic:\n",
                "    avg_margins = {\n",
                "        m: sum(item['synthetic_margins'][m] for item in filtered_synthetic) / len(filtered_synthetic)\n",
                "        for m in ['quantity', 'quality', 'relation', 'manner']\n",
                "    }\n",
                "    print(f\"\\nAverage margins (filtered):\")\n",
                "    for m, val in avg_margins.items():\n",
                "        print(f\"  {m}: {val:.4f}\")\n",
                "\n",
                "print(f\"\\n‚úÖ Filtering complete: {len(filtered_synthetic)} high-quality synthetic pairs\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: STEP 3 - Merge with Clean Pairs\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"STEP 3: MERGING WITH CLEAN PAIRS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Load clean pairs\n",
                "with open(CLEAN_FILE) as f:\n",
                "    clean_data = json.load(f)\n",
                "\n",
                "print(f\"\\nClean pairs: {len(clean_data)}\")\n",
                "print(f\"Filtered synthetic: {len(filtered_synthetic)}\")\n",
                "\n",
                "# Prepare synthetic pairs in DPO format\n",
                "synthetic_dpo = []\n",
                "for item in filtered_synthetic:\n",
                "    synthetic_dpo.append({\n",
                "        'prompt': item['prompt'],\n",
                "        'chosen': item['synthetic_chosen'],\n",
                "        'rejected': item['original_chosen_failed'],\n",
                "        'margins': item['synthetic_margins'],\n",
                "        'source': 'synthetic'\n",
                "    })\n",
                "\n",
                "# Merge: Clean first, then synthetic\n",
                "final_dataset = clean_data + synthetic_dpo\n",
                "\n",
                "print(f\"\\n‚úÖ Final DPO dataset: {len(final_dataset)} pairs\")\n",
                "print(f\"   - Clean (human): {len(clean_data)} ({len(clean_data)/len(final_dataset)*100:.1f}%)\")\n",
                "print(f\"   - Synthetic: {len(synthetic_dpo)} ({len(synthetic_dpo)/len(final_dataset)*100:.1f}%)\")\n",
                "\n",
                "# Save final dataset\n",
                "with open(OUTPUT_FILE, 'w') as f:\n",
                "    json.dump(final_dataset, f, indent=2)\n",
                "\n",
                "print(f\"\\n‚úÖ Saved to: {OUTPUT_FILE}\")\n",
                "print(f\"   File size: {os.path.getsize(OUTPUT_FILE) / (1024**2):.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Summary & Validation\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"PIPELINE SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nüìä Processing Results:\")\n",
                "print(f\"   Synthetic candidates: {len(synthetic_data)}\")\n",
                "print(f\"   Successfully scored: {len(scored_synthetic)}\")\n",
                "print(f\"   Passed strict filter: {len(filtered_synthetic)}\")\n",
                "print(f\"   Filter pass rate: {len(filtered_synthetic)/len(scored_synthetic)*100:.1f}%\")\n",
                "\n",
                "print(f\"\\nüéØ Final Dataset:\")\n",
                "print(f\"   Total pairs: {len(final_dataset)}\")\n",
                "print(f\"   Human pairs: {len(clean_data)}\")\n",
                "print(f\"   Synthetic pairs: {len(synthetic_dpo)}\")\n",
                "print(f\"   Ratio: {len(synthetic_dpo)/len(clean_data):.1f}x synthetic data\")\n",
                "\n",
                "# Validate sample\n",
                "print(f\"\\nüîç Sample from final dataset:\")\n",
                "sample = final_dataset[len(clean_data)]  # First synthetic item\n",
                "print(f\"   Prompt: {sample['prompt'][:100]}...\")\n",
                "print(f\"   Chosen: {sample['chosen'][:100]}...\")\n",
                "print(f\"   Margins: {sample['margins']}\")\n",
                "\n",
                "print(f\"\\n‚úÖ STEPS 1-3 COMPLETE\")\n",
                "print(f\"\\nüì• Download {OUTPUT_FILE} and proceed to DPO training!\")\n",
                "print(f\"\\nExpected DPO performance:\")\n",
                "print(f\"   - Better than 411-only baseline (96.8% accuracy)\")\n",
                "print(f\"   - More robust generalization\")\n",
                "print(f\"   - Stronger alignment signal\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}