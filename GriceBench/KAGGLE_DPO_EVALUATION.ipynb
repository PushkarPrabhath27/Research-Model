{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üéØ PHASE 2: Triangulated DPO Evaluation\n",
                "\n",
                "## Gold-Standard Validation Pipeline\n",
                "\n",
                "This notebook answers the core question:\n",
                "> **\"Did DPO actually change model preferences in the intended way, without regressions?\"**\n",
                "\n",
                "### Three Independent Validations:\n",
                "1. **Preference Accuracy** ‚Äî Quantitative, on held-out data\n",
                "2. **Behavioral Evaluation** ‚Äî Qualitative, generated outputs\n",
                "3. **Failure Comparison** ‚Äî Direct comparison to original failures\n",
                "\n",
                "### Requirements:\n",
                "- **Datasets:** `final_dpo_dataset.json`, `dpo_merged_model/`\n",
                "- **GPU:** T4 (for fast inference)\n",
                "- **Runtime:** ~30-45 minutes\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Environment Setup\n",
                "import os\n",
                "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
                "\n",
                "!pip install -q transformers accelerate torch\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import torch\n",
                "import json\n",
                "import random\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "print(\"\\n‚úÖ Environment ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Load Models & Data\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"LOADING MODELS & DATA\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Find model path\n",
                "MODEL_PATH = None\n",
                "for p in [\"/kaggle/input/dpo-model/dpo_merged_model\",\n",
                "          \"/kaggle/input/dpo-merged-model/dpo_merged_model\",\n",
                "          \"/kaggle/input/aligned-model/dpo_merged_model\"]:\n",
                "    if os.path.exists(p): MODEL_PATH = p; break\n",
                "\n",
                "# Find dataset path\n",
                "DATA_PATH = None\n",
                "for p in [\"/kaggle/input/final-dpo-dataset/final_dpo_dataset.json\",\n",
                "          \"/kaggle/input/dpo-dataset/final_dpo_dataset.json\"]:\n",
                "    if os.path.exists(p): DATA_PATH = p; break\n",
                "\n",
                "if not MODEL_PATH:\n",
                "    print(\"Available inputs:\")\n",
                "    for item in os.listdir(\"/kaggle/input/\"):\n",
                "        print(f\"  {item}\")\n",
                "    raise FileNotFoundError(\"Upload dpo_merged_model folder!\")\n",
                "\n",
                "if not DATA_PATH:\n",
                "    raise FileNotFoundError(\"Upload final_dpo_dataset.json!\")\n",
                "\n",
                "print(f\"\\nüìÇ Model: {MODEL_PATH}\")\n",
                "print(f\"üìÇ Data: {DATA_PATH}\")\n",
                "\n",
                "# Load DPO-aligned model\n",
                "print(f\"\\nüì• Loading DPO-aligned model...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"left\"\n",
                "\n",
                "dpo_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_PATH,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "dpo_model.eval()\n",
                "print(f\"‚úÖ DPO model loaded on {dpo_model.device}\")\n",
                "\n",
                "# Load base model for comparison\n",
                "print(f\"\\nüì• Loading base model (for comparison)...\")\n",
                "BASE_MODEL = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    BASE_MODEL,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map=\"auto\"\n",
                ")\n",
                "base_model.eval()\n",
                "print(f\"‚úÖ Base model loaded\")\n",
                "\n",
                "# Load data\n",
                "print(f\"\\nüì• Loading evaluation data...\")\n",
                "with open(DATA_PATH) as f:\n",
                "    all_data = json.load(f)\n",
                "\n",
                "print(f\"‚úÖ Loaded {len(all_data)} pairs\")\n",
                "\n",
                "# Split into sources\n",
                "human_data = [d for d in all_data if d.get('source') == 'human_clean']\n",
                "synth_data = [d for d in all_data if d.get('source') != 'human_clean']\n",
                "\n",
                "print(f\"   Human pairs: {len(human_data)}\")\n",
                "print(f\"   Synthetic pairs: {len(synth_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Helper Functions\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SETTING UP EVALUATION FUNCTIONS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "def get_response_logprob(model, tokenizer, prompt, response):\n",
                "    \"\"\"Compute log probability of response given prompt\"\"\"\n",
                "    text = f\"{prompt}\\n\\nResponse: {response}\"\n",
                "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
                "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
                "        # Negative loss = log probability\n",
                "        return -outputs.loss.item()\n",
                "\n",
                "def generate_response(model, tokenizer, prompt, max_length=100):\n",
                "    \"\"\"Generate a response to a prompt\"\"\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
                "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_length,\n",
                "            temperature=0.7,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id,\n",
                "            top_p=0.9\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
                "    return response.strip()\n",
                "\n",
                "print(\"‚úÖ Helper functions defined\")\n",
                "print(\"   - get_response_logprob(): Computes log-prob of response\")\n",
                "print(\"   - generate_response(): Generates text from model\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: VALIDATION 1 ‚Äî Preference Accuracy (Quantitative)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üìä VALIDATION 1: PREFERENCE ACCURACY\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nGoal: Verify model prefers 'chosen' over 'rejected' on held-out data\")\n",
                "print(\"Method: Compare log-probabilities\")\n",
                "print(\"Success: ‚â•95% on human data, ‚â•85% on synthetic data\\n\")\n",
                "\n",
                "# Sample held-out data\n",
                "random.seed(42)  # Reproducibility\n",
                "human_sample = random.sample(human_data, min(100, len(human_data)))\n",
                "synth_sample = random.sample(synth_data, min(200, len(synth_data)))\n",
                "\n",
                "print(f\"Evaluating: {len(human_sample)} human + {len(synth_sample)} synthetic pairs\\n\")\n",
                "\n",
                "# Evaluate human data\n",
                "print(\"üîç Evaluating on HUMAN data...\")\n",
                "human_correct = 0\n",
                "human_margins = []\n",
                "\n",
                "for item in tqdm(human_sample, desc=\"Human pairs\"):\n",
                "    chosen_logp = get_response_logprob(dpo_model, tokenizer, item['prompt'], item['chosen'])\n",
                "    rejected_logp = get_response_logprob(dpo_model, tokenizer, item['prompt'], item['rejected'])\n",
                "    \n",
                "    margin = chosen_logp - rejected_logp\n",
                "    human_margins.append(margin)\n",
                "    \n",
                "    if margin > 0:\n",
                "        human_correct += 1\n",
                "\n",
                "human_accuracy = 100 * human_correct / len(human_sample)\n",
                "human_avg_margin = sum(human_margins) / len(human_margins)\n",
                "\n",
                "print(f\"\\n   ‚úÖ Human Accuracy: {human_accuracy:.1f}%\")\n",
                "print(f\"   Average margin: {human_avg_margin:.4f}\")\n",
                "\n",
                "# Evaluate synthetic data\n",
                "print(\"\\nüîç Evaluating on SYNTHETIC data...\")\n",
                "synth_correct = 0\n",
                "synth_margins = []\n",
                "\n",
                "for item in tqdm(synth_sample, desc=\"Synthetic pairs\"):\n",
                "    chosen_logp = get_response_logprob(dpo_model, tokenizer, item['prompt'], item['chosen'])\n",
                "    rejected_logp = get_response_logprob(dpo_model, tokenizer, item['prompt'], item['rejected'])\n",
                "    \n",
                "    margin = chosen_logp - rejected_logp\n",
                "    synth_margins.append(margin)\n",
                "    \n",
                "    if margin > 0:\n",
                "        synth_correct += 1\n",
                "\n",
                "synth_accuracy = 100 * synth_correct / len(synth_sample)\n",
                "synth_avg_margin = sum(synth_margins) / len(synth_margins)\n",
                "\n",
                "print(f\"\\n   ‚úÖ Synthetic Accuracy: {synth_accuracy:.1f}%\")\n",
                "print(f\"   Average margin: {synth_avg_margin:.4f}\")\n",
                "\n",
                "# Overall results\n",
                "total_correct = human_correct + synth_correct\n",
                "total_samples = len(human_sample) + len(synth_sample)\n",
                "overall_accuracy = 100 * total_correct / total_samples\n",
                "\n",
                "print(f\"\\n\" + \"-\"*40)\n",
                "print(f\"üìä PREFERENCE ACCURACY SUMMARY\")\n",
                "print(f\"-\"*40)\n",
                "print(f\"   Human pairs: {human_accuracy:.1f}% ({human_correct}/{len(human_sample)})\")\n",
                "print(f\"   Synthetic pairs: {synth_accuracy:.1f}% ({synth_correct}/{len(synth_sample)})\")\n",
                "print(f\"   OVERALL: {overall_accuracy:.1f}% ({total_correct}/{total_samples})\")\n",
                "\n",
                "# Success check\n",
                "if human_accuracy >= 95:\n",
                "    print(f\"\\n   ‚úÖ PASSED: Human accuracy ‚â• 95%\")\n",
                "else:\n",
                "    print(f\"\\n   ‚ö†Ô∏è WARNING: Human accuracy < 95%\")\n",
                "\n",
                "if synth_accuracy >= 85:\n",
                "    print(f\"   ‚úÖ PASSED: Synthetic accuracy ‚â• 85%\")\n",
                "else:\n",
                "    print(f\"   ‚ö†Ô∏è WARNING: Synthetic accuracy < 85%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: VALIDATION 2 ‚Äî Behavioral Evaluation (Qualitative)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üîç VALIDATION 2: BEHAVIORAL EVALUATION\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nGoal: Ensure alignment improved responses without regressions\")\n",
                "print(\"Method: Compare Base model vs DPO model on same prompts\")\n",
                "print(\"Check: Relevance, Clarity, Cooperation, No verbosity explosion\\n\")\n",
                "\n",
                "# Test prompts from different categories\n",
                "test_prompts = [\n",
                "    # From original dataset\n",
                "    \"Context: [agent_1]: Do you follow politics? [agent_2]: Sometimes, the electoral system is interesting.\\nEvidence: FS2\\n\\nGenerate a cooperative response:\",\n",
                "    \n",
                "    \"Context: [agent_1]: What's your favorite movie? [agent_2]: I love sci-fi. Star Wars is classic.\\nEvidence: FS1\\n\\nGenerate a cooperative response:\",\n",
                "    \n",
                "    \"Context: [agent_1]: I'm learning guitar. [agent_2]: That's cool! Music is therapeutic.\\nEvidence: Personal Knowledge\\n\\nGenerate a cooperative response:\",\n",
                "    \n",
                "    # Edge cases\n",
                "    \"Context: [agent_1]: Hi [agent_2]: Hello!\\nEvidence: Personal Knowledge\\n\\nGenerate a cooperative response:\",\n",
                "    \n",
                "    \"Context: [agent_1]: Did you know sharks have no bones? [agent_2]: That's fascinating!\\nEvidence: FS3\\n\\nGenerate a cooperative response:\"\n",
                "]\n",
                "\n",
                "behavioral_results = []\n",
                "\n",
                "for i, prompt in enumerate(test_prompts, 1):\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"TEST {i}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    # Extract context for display\n",
                "    context = prompt.split(\"Evidence:\")[0].strip()\n",
                "    print(f\"\\nüìù Context: {context[:100]}...\")\n",
                "    \n",
                "    # Generate from both models\n",
                "    base_response = generate_response(base_model, tokenizer, prompt)\n",
                "    dpo_response = generate_response(dpo_model, tokenizer, prompt)\n",
                "    \n",
                "    print(f\"\\nüî¥ BASE MODEL:\")\n",
                "    print(f\"   {base_response[:200]}\")\n",
                "    \n",
                "    print(f\"\\nüü¢ DPO MODEL:\")\n",
                "    print(f\"   {dpo_response[:200]}\")\n",
                "    \n",
                "    # Quick metrics\n",
                "    base_len = len(base_response.split())\n",
                "    dpo_len = len(dpo_response.split())\n",
                "    \n",
                "    print(f\"\\nüìä Metrics:\")\n",
                "    print(f\"   Base length: {base_len} words\")\n",
                "    print(f\"   DPO length: {dpo_len} words\")\n",
                "    print(f\"   Ratio: {dpo_len/max(base_len,1):.2f}x\")\n",
                "    \n",
                "    behavioral_results.append({\n",
                "        'prompt': prompt,\n",
                "        'base_response': base_response,\n",
                "        'dpo_response': dpo_response,\n",
                "        'base_len': base_len,\n",
                "        'dpo_len': dpo_len\n",
                "    })\n",
                "\n",
                "# Summary\n",
                "avg_base_len = sum(r['base_len'] for r in behavioral_results) / len(behavioral_results)\n",
                "avg_dpo_len = sum(r['dpo_len'] for r in behavioral_results) / len(behavioral_results)\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"BEHAVIORAL SUMMARY\")\n",
                "print(f\"{'='*60}\")\n",
                "print(f\"   Average base length: {avg_base_len:.1f} words\")\n",
                "print(f\"   Average DPO length: {avg_dpo_len:.1f} words\")\n",
                "print(f\"   Length ratio: {avg_dpo_len/avg_base_len:.2f}x\")\n",
                "\n",
                "if avg_dpo_len / avg_base_len < 2.0:\n",
                "    print(f\"\\n   ‚úÖ No verbosity explosion detected\")\n",
                "else:\n",
                "    print(f\"\\n   ‚ö†Ô∏è Warning: DPO responses may be too verbose\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: VALIDATION 3 ‚Äî Failure Comparison (Most Important)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üéØ VALIDATION 3: DIRECT FAILURE COMPARISON\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nGoal: Prove alignment actually FIXED the original problems\")\n",
                "print(\"Method: Compare DPO output to original failed responses\")\n",
                "print(\"Check: Is DPO more cooperative? Is meaning preserved?\\n\")\n",
                "\n",
                "# Sample pairs for comparison\n",
                "comparison_sample = random.sample(synth_data, min(10, len(synth_data)))\n",
                "\n",
                "comparison_results = []\n",
                "\n",
                "for i, item in enumerate(comparison_sample, 1):\n",
                "    print(f\"\\n{'='*70}\")\n",
                "    print(f\"COMPARISON {i}\")\n",
                "    print(f\"{'='*70}\")\n",
                "    \n",
                "    prompt = item['prompt']\n",
                "    original_failed = item['rejected']  # Original failed response\n",
                "    synthetic_chosen = item['chosen']   # What we trained as \"good\"\n",
                "    \n",
                "    # Generate DPO model's response\n",
                "    dpo_generated = generate_response(dpo_model, tokenizer, prompt)\n",
                "    \n",
                "    # Extract context\n",
                "    context = prompt.split(\"Evidence:\")[0].strip()[-150:]\n",
                "    print(f\"\\nüìù Context: ...{context}\")\n",
                "    \n",
                "    print(f\"\\n‚ùå ORIGINAL FAILED:\")\n",
                "    print(f\"   {original_failed[:150]}\")\n",
                "    \n",
                "    print(f\"\\n‚úÖ TRAINING TARGET (chosen):\")\n",
                "    print(f\"   {synthetic_chosen[:150]}\")\n",
                "    \n",
                "    print(f\"\\nüü¢ DPO MODEL GENERATED:\")\n",
                "    print(f\"   {dpo_generated[:150]}\")\n",
                "    \n",
                "    # Compute preference scores\n",
                "    failed_logp = get_response_logprob(dpo_model, tokenizer, prompt, original_failed)\n",
                "    dpo_logp = get_response_logprob(dpo_model, tokenizer, prompt, dpo_generated)\n",
                "    \n",
                "    prefers_generated = dpo_logp > failed_logp\n",
                "    \n",
                "    print(f\"\\nüìä Model Preference:\")\n",
                "    print(f\"   Failed logprob: {failed_logp:.4f}\")\n",
                "    print(f\"   Generated logprob: {dpo_logp:.4f}\")\n",
                "    print(f\"   ‚Üí Model prefers: {'‚úÖ Generated' if prefers_generated else '‚ùå Failed'}\")\n",
                "    \n",
                "    comparison_results.append({\n",
                "        'prompt': prompt,\n",
                "        'original_failed': original_failed,\n",
                "        'dpo_generated': dpo_generated,\n",
                "        'prefers_generated': prefers_generated\n",
                "    })\n",
                "\n",
                "# Summary\n",
                "prefers_count = sum(1 for r in comparison_results if r['prefers_generated'])\n",
                "\n",
                "print(f\"\\n{'='*70}\")\n",
                "print(f\"FAILURE COMPARISON SUMMARY\")\n",
                "print(f\"{'='*70}\")\n",
                "print(f\"   Model prefers generated over failed: {prefers_count}/{len(comparison_results)} ({100*prefers_count/len(comparison_results):.0f}%)\")\n",
                "\n",
                "if prefers_count / len(comparison_results) >= 0.8:\n",
                "    print(f\"\\n   ‚úÖ PASSED: DPO model consistently prefers cooperative responses\")\n",
                "else:\n",
                "    print(f\"\\n   ‚ö†Ô∏è WARNING: Model may not be fully aligned\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: FINAL REPORT\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üèÜ FINAL EVALUATION REPORT\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nüìä VALIDATION 1: PREFERENCE ACCURACY\")\n",
                "print(f\"   Human pairs: {human_accuracy:.1f}%\")\n",
                "print(f\"   Synthetic pairs: {synth_accuracy:.1f}%\")\n",
                "print(f\"   Overall: {overall_accuracy:.1f}%\")\n",
                "v1_pass = human_accuracy >= 95 and synth_accuracy >= 85\n",
                "print(f\"   Status: {'‚úÖ PASSED' if v1_pass else '‚ö†Ô∏è NEEDS REVIEW'}\")\n",
                "\n",
                "print(f\"\\nüìä VALIDATION 2: BEHAVIORAL EVALUATION\")\n",
                "print(f\"   Average response length: {avg_dpo_len:.1f} words\")\n",
                "print(f\"   Length ratio (vs base): {avg_dpo_len/avg_base_len:.2f}x\")\n",
                "v2_pass = avg_dpo_len / avg_base_len < 2.0\n",
                "print(f\"   Status: {'‚úÖ PASSED' if v2_pass else '‚ö†Ô∏è NEEDS REVIEW'}\")\n",
                "\n",
                "print(f\"\\nüìä VALIDATION 3: FAILURE COMPARISON\")\n",
                "print(f\"   Prefers generated over failed: {100*prefers_count/len(comparison_results):.0f}%\")\n",
                "v3_pass = prefers_count / len(comparison_results) >= 0.8\n",
                "print(f\"   Status: {'‚úÖ PASSED' if v3_pass else '‚ö†Ô∏è NEEDS REVIEW'}\")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "all_pass = v1_pass and v2_pass and v3_pass\n",
                "if all_pass:\n",
                "    print(f\"\\nüéâ ALL VALIDATIONS PASSED!\")\n",
                "    print(f\"\\nConclusion:\")\n",
                "    print(f\"   ‚úÖ DPO training successfully aligned the model\")\n",
                "    print(f\"   ‚úÖ Model now prefers Gricean-cooperative responses\")\n",
                "    print(f\"   ‚úÖ No significant regressions detected\")\n",
                "    print(f\"   ‚úÖ Ready for production use or further training\")\n",
                "else:\n",
                "    print(f\"\\n‚ö†Ô∏è SOME VALIDATIONS NEED REVIEW\")\n",
                "    print(f\"\\nRecommendations:\")\n",
                "    if not v1_pass:\n",
                "        print(f\"   - Preference accuracy below target\")\n",
                "    if not v2_pass:\n",
                "        print(f\"   - Response length may be too long\")\n",
                "    if not v3_pass:\n",
                "        print(f\"   - Model doesn't consistently prefer cooperative responses\")\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(f\"‚ú® PHASE 2 EVALUATION COMPLETE\")\n",
                "print(f\"={'='*80}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Save Results\n",
                "import json\n",
                "\n",
                "print(\"\\nüíæ SAVING EVALUATION RESULTS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Compile all results\n",
                "evaluation_report = {\n",
                "    'validation_1_preference_accuracy': {\n",
                "        'human_accuracy': human_accuracy,\n",
                "        'synthetic_accuracy': synth_accuracy,\n",
                "        'overall_accuracy': overall_accuracy,\n",
                "        'human_avg_margin': human_avg_margin,\n",
                "        'synth_avg_margin': synth_avg_margin,\n",
                "        'passed': v1_pass\n",
                "    },\n",
                "    'validation_2_behavioral': {\n",
                "        'avg_base_length': avg_base_len,\n",
                "        'avg_dpo_length': avg_dpo_len,\n",
                "        'length_ratio': avg_dpo_len / avg_base_len,\n",
                "        'passed': v2_pass,\n",
                "        'samples': behavioral_results\n",
                "    },\n",
                "    'validation_3_failure_comparison': {\n",
                "        'prefers_generated_pct': 100 * prefers_count / len(comparison_results),\n",
                "        'passed': v3_pass,\n",
                "        'samples': comparison_results\n",
                "    },\n",
                "    'overall': {\n",
                "        'all_passed': all_pass,\n",
                "        'total_pairs_evaluated': total_samples,\n",
                "        'model_path': MODEL_PATH\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save report\n",
                "report_path = \"/kaggle/working/evaluation_report.json\"\n",
                "with open(report_path, 'w') as f:\n",
                "    json.dump(evaluation_report, f, indent=2, default=str)\n",
                "\n",
                "print(f\"\\n‚úÖ Report saved: {report_path}\")\n",
                "print(f\"\\nDownload this file for your records.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}