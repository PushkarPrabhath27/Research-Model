{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DPO Training on 411 Clean Gricean Pairs\n",
                "\n",
                "**Purpose:** Establish baseline with conflict-free cooperative data\n",
                "\n",
                "This notebook trains a DPO model on 411 pairs where ALL 4 Gricean maxim margins are positive:\n",
                "- Quantity > 0\n",
                "- Quality > 0  \n",
                "- Relation > 0\n",
                "- Manner > 0\n",
                "\n",
                "**Expected outcome:** Small but consistent improvement in manner without regression on other maxims."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install -q transformers datasets accelerate peft trl bitsandbytes\n",
                "!pip install -q huggingface_hub\n",
                "\n",
                "import os\n",
                "import json\n",
                "import torch\n",
                "from datasets import Dataset\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from trl import DPOTrainer\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Clean Dataset\n",
                "\n",
                "⚠️ **Before running:** Upload `clean_dpo_pairs.json` to your Kaggle dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path to your uploaded dataset\n",
                "# Adjust this path based on your Kaggle dataset name\n",
                "DATA_PATH = \"/kaggle/input/gricebench-clean-dpo/clean_dpo_pairs.json\"\n",
                "\n",
                "# Load the clean pairs\n",
                "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
                "    clean_pairs = json.load(f)\n",
                "\n",
                "print(f\"Loaded {len(clean_pairs)} clean DPO pairs\")\n",
                "\n",
                "# Show sample\n",
                "if clean_pairs:\n",
                "    sample = clean_pairs[0]\n",
                "    print(f\"\\nSample pair:\")\n",
                "    print(f\"  Prompt: {sample['prompt'][:100]}...\")\n",
                "    print(f\"  Chosen: {sample['chosen'][:80]}...\")\n",
                "    print(f\"  Rejected: {sample['rejected'][:80]}...\")\n",
                "    print(f\"  Margins: qty={sample['margins']['quantity']:.4f}, qlt={sample['margins']['quality']:.4f}, rel={sample['margins']['relation']:.4f}, man={sample['margins']['manner']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prepare Dataset for DPO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_dpo_dataset(pairs):\n",
                "    \"\"\"Convert pairs to DPO format.\"\"\"\n",
                "    formatted = []\n",
                "    for pair in pairs:\n",
                "        formatted.append({\n",
                "            'prompt': pair['prompt'],\n",
                "            'chosen': pair['chosen'],\n",
                "            'rejected': pair['rejected']\n",
                "        })\n",
                "    return Dataset.from_list(formatted)\n",
                "\n",
                "# Create dataset\n",
                "dataset = prepare_dpo_dataset(clean_pairs)\n",
                "print(f\"Dataset size: {len(dataset)}\")\n",
                "\n",
                "# Split into train/eval (90/10)\n",
                "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
                "train_dataset = split['train']\n",
                "eval_dataset = split['test']\n",
                "\n",
                "print(f\"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Base Model with Quantization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model configuration\n",
                "MODEL_NAME = \"HuggingFaceTB/SmolLM2-360M-Instruct\"  # Small model for quick training\n",
                "# Alternative: \"microsoft/DialoGPT-medium\" or \"facebook/opt-350m\"\n",
                "\n",
                "# Quantization config for memory efficiency\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "# Load model\n",
                "print(f\"Loading {MODEL_NAME}...\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "if tokenizer.pad_token is None:\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "print(f\"Model loaded! Vocab size: {len(tokenizer)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Configure LoRA for Efficient Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare model for k-bit training\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "# LoRA configuration\n",
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
                ")\n",
                "\n",
                "# Create reference model (frozen copy for DPO)\n",
                "ref_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "print(\"LoRA config ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Output directory\n",
                "OUTPUT_DIR = \"/kaggle/working/dpo_411_clean\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# Training arguments - optimized for small dataset\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    num_train_epochs=5,           # More epochs for small dataset\n",
                "    per_device_train_batch_size=2,\n",
                "    per_device_eval_batch_size=2,\n",
                "    gradient_accumulation_steps=4,  # Effective batch size = 8\n",
                "    learning_rate=5e-5,\n",
                "    warmup_ratio=0.1,\n",
                "    logging_steps=10,\n",
                "    eval_strategy=\"steps\",\n",
                "    eval_steps=50,\n",
                "    save_strategy=\"steps\",\n",
                "    save_steps=100,\n",
                "    save_total_limit=2,\n",
                "    fp16=True,\n",
                "    report_to=\"none\",\n",
                "    remove_unused_columns=False\n",
                ")\n",
                "\n",
                "print(f\"Training config ready!\")\n",
                "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
                "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
                "print(f\"  Learning rate: {training_args.learning_rate}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Initialize DPO Trainer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize DPO trainer\n",
                "dpo_trainer = DPOTrainer(\n",
                "    model=model,\n",
                "    ref_model=ref_model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=eval_dataset,\n",
                "    tokenizer=tokenizer,\n",
                "    peft_config=lora_config,\n",
                "    beta=0.1,              # DPO temperature - lower = stronger preference\n",
                "    max_length=512,\n",
                "    max_prompt_length=256\n",
                ")\n",
                "\n",
                "print(\"DPO Trainer initialized!\")\n",
                "print(f\"  Beta (temperature): 0.1\")\n",
                "print(f\"  Max length: 512\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Train the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting DPO training on 411 clean pairs...\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "# Train\n",
                "train_result = dpo_trainer.train()\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"Training complete!\")\n",
                "print(f\"  Total steps: {train_result.global_step}\")\n",
                "print(f\"  Final loss: {train_result.training_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the trained model\n",
                "FINAL_MODEL_DIR = \"/kaggle/working/dpo_411_clean_final\"\n",
                "dpo_trainer.save_model(FINAL_MODEL_DIR)\n",
                "tokenizer.save_pretrained(FINAL_MODEL_DIR)\n",
                "\n",
                "print(f\"Model saved to {FINAL_MODEL_DIR}\")\n",
                "\n",
                "# List saved files\n",
                "import os\n",
                "for f in os.listdir(FINAL_MODEL_DIR):\n",
                "    size = os.path.getsize(os.path.join(FINAL_MODEL_DIR, f)) / 1024 / 1024\n",
                "    print(f\"  {f}: {size:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Quick Validation - Generate Sample Responses"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the trained model\n",
                "def generate_response(model, tokenizer, prompt, max_new_tokens=100):\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            do_sample=True,\n",
                "            temperature=0.7,\n",
                "            top_p=0.9,\n",
                "            pad_token_id=tokenizer.pad_token_id\n",
                "        )\n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "# Test prompts\n",
                "test_prompts = [\n",
                "    \"Context: [agent_1]: Do you like Star Wars? [agent_2]: Yes I love the original trilogy!\\nEvidence: Personal Knowledge\\n\\nGenerate a cooperative response:\",\n",
                "    \"Context: [agent_1]: What do you think about cats? [agent_2]: I have two cats myself.\\nEvidence: Personal Knowledge\\n\\nGenerate a cooperative response:\"\n",
                "]\n",
                "\n",
                "print(\"Sample generations from trained model:\")\n",
                "print(\"=\"*50)\n",
                "for i, prompt in enumerate(test_prompts, 1):\n",
                "    response = generate_response(model, tokenizer, prompt)\n",
                "    print(f\"\\nTest {i}:\")\n",
                "    print(f\"Prompt: {prompt[:80]}...\")\n",
                "    print(f\"Response: {response}\")\n",
                "    print(\"-\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Download Instructions\n",
                "\n",
                "After training completes:\n",
                "\n",
                "1. **Download the model files** from `/kaggle/working/dpo_411_clean_final/`\n",
                "2. **Key files to download:**\n",
                "   - `adapter_config.json`\n",
                "   - `adapter_model.safetensors` (or `.bin`)\n",
                "   - `tokenizer_config.json`\n",
                "   - `special_tokens_map.json`\n",
                "\n",
                "3. **Next steps based on results:**\n",
                "   - If manner improved → Relax manner threshold to get more data\n",
                "   - If weak improvement → Use synthetic generation\n",
                "   - If no effect → Skip to synthetic generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a zip file for easy download\n",
                "import shutil\n",
                "\n",
                "ZIP_PATH = \"/kaggle/working/dpo_411_clean_model.zip\"\n",
                "shutil.make_archive(\n",
                "    ZIP_PATH.replace('.zip', ''),\n",
                "    'zip',\n",
                "    FINAL_MODEL_DIR\n",
                ")\n",
                "\n",
                "print(f\"\\n✅ Model zipped to: {ZIP_PATH}\")\n",
                "print(\"Download this file from the Output tab!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}