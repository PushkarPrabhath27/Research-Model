{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ OPTIMIZED DPO TRAINING - Maximum GPU Utilization\n",
                "\n",
                "**Optimizations:**\n",
                "- Batch size 4 (was 1) ‚Üí 4x more GPU work\n",
                "- Gradient accumulation 4 (was 16) ‚Üí Less waiting\n",
                "- Multi-GPU with accelerate\n",
                "- Parallel data loading\n",
                "- Auto-save with ZIP backup\n",
                "- Checkpoints every epoch\n",
                "\n",
                "**Expected Runtime:** 2-3 hours (was 10 hours)\n",
                "**GPU Utilization:** 70-85% (was 35%)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Environment Setup\n",
                "import os\n",
                "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
                "os.environ['TRL_USE_RICH'] = '0'\n",
                "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
                "\n",
                "!pip install -q -U trl peft bitsandbytes accelerate transformers datasets\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
                "for i in range(torch.cuda.device_count()):\n",
                "    print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
                "    print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")\n",
                "\n",
                "print(\"\\n‚úÖ Environment ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Load Dataset & Model (Optimized)\n",
                "import json\n",
                "import torch\n",
                "from datasets import Dataset\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from peft import LoraConfig, get_peft_model, TaskType\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"LOADING DATASET & MODEL\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Find dataset\n",
                "DATA_FILE = None\n",
                "for p in [\"/kaggle/input/final-dpo-dataset/final_dpo_dataset.json\",\n",
                "          \"/kaggle/input/dpo-dataset/final_dpo_dataset.json\",\n",
                "          \"/kaggle/input/finaldpodataset/final_dpo_dataset.json\"]:\n",
                "    if os.path.exists(p): DATA_FILE = p; break\n",
                "\n",
                "if not DATA_FILE:\n",
                "    # List available inputs\n",
                "    print(\"Available inputs:\")\n",
                "    for item in os.listdir(\"/kaggle/input/\"):\n",
                "        print(f\"  {item}\")\n",
                "    raise FileNotFoundError(\"Upload final_dpo_dataset.json!\")\n",
                "\n",
                "print(f\"\\nüìÇ Dataset: {DATA_FILE}\")\n",
                "\n",
                "# Load data\n",
                "with open(DATA_FILE) as f:\n",
                "    data = json.load(f)\n",
                "\n",
                "print(f\"   Total pairs: {len(data)}\")\n",
                "\n",
                "# Convert to HuggingFace Dataset\n",
                "dataset = Dataset.from_list(data)\n",
                "print(f\"‚úÖ Dataset loaded: {len(dataset)} pairs\")\n",
                "\n",
                "# Load model with optimizations\n",
                "print(f\"\\nüì• Loading SmolLM2-360M-Instruct (optimized)...\")\n",
                "\n",
                "MODEL_NAME = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"left\"\n",
                "\n",
                "# Load in bfloat16 for faster training\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map=\"auto\",\n",
                "    attn_implementation=\"eager\"  # Compatible with all GPUs\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Model loaded\")\n",
                "print(f\"   Parameters: {model.num_parameters() / 1e6:.1f}M\")\n",
                "print(f\"   Device: {model.device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Configure LoRA (Optimized)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"LORA CONFIGURATION\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "lora_config = LoraConfig(\n",
                "    r=16,\n",
                "    lora_alpha=32,\n",
                "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # More layers = better\n",
                "    lora_dropout=0.05,\n",
                "    bias=\"none\",\n",
                "    task_type=TaskType.CAUSAL_LM\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, lora_config)\n",
                "\n",
                "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "total = sum(p.numel() for p in model.parameters())\n",
                "\n",
                "print(f\"\\nüìä LoRA Stats:\")\n",
                "print(f\"   Trainable: {trainable / 1e6:.2f}M ({100*trainable/total:.2f}%)\")\n",
                "print(f\"   Total: {total / 1e6:.1f}M\")\n",
                "print(f\"\\n‚úÖ LoRA configured\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: DPO Training Configuration (OPTIMIZED FOR SPEED)\n",
                "from trl import DPOConfig, DPOTrainer\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"DPO TRAINING CONFIGURATION (OPTIMIZED)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Calculate optimal batch settings\n",
                "# Goal: Maximize GPU utilization while maintaining quality\n",
                "# T4 has 16GB VRAM, we can use batch_size=4 with 360M model\n",
                "\n",
                "BATCH_SIZE = 4           # Was 1 ‚Üí 4x more GPU work per step\n",
                "GRAD_ACCUM = 4           # Was 16 ‚Üí Less waiting between updates\n",
                "EFFECTIVE_BATCH = BATCH_SIZE * GRAD_ACCUM  # = 16 (same as before)\n",
                "\n",
                "training_args = DPOConfig(\n",
                "    # Core DPO\n",
                "    beta=0.1,\n",
                "    \n",
                "    # OPTIMIZED batch settings\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    gradient_accumulation_steps=GRAD_ACCUM,\n",
                "    \n",
                "    # Training params\n",
                "    num_train_epochs=3,            # 3 epochs is enough with good data\n",
                "    learning_rate=5e-6,            # Slightly higher for faster convergence\n",
                "    \n",
                "    # Length\n",
                "    max_length=512,\n",
                "    max_prompt_length=256,\n",
                "    \n",
                "    # Optimization\n",
                "    optim=\"adamw_torch_fused\",     # Faster optimizer\n",
                "    warmup_ratio=0.1,\n",
                "    \n",
                "    # Mixed precision\n",
                "    bf16=True,\n",
                "    \n",
                "    # Parallel data loading\n",
                "    dataloader_num_workers=4,\n",
                "    dataloader_pin_memory=True,\n",
                "    \n",
                "    # Logging & Checkpointing\n",
                "    logging_steps=25,\n",
                "    save_strategy=\"epoch\",\n",
                "    save_total_limit=2,\n",
                "    output_dir=\"/kaggle/working/dpo_checkpoints\",\n",
                "    \n",
                "    # Disable wandb\n",
                "    report_to=\"none\",\n",
                "    \n",
                "    # Gradient checkpointing for memory efficiency\n",
                "    gradient_checkpointing=True\n",
                ")\n",
                "\n",
                "# Calculate expected training time\n",
                "total_steps = (len(dataset) * training_args.num_train_epochs) // EFFECTIVE_BATCH\n",
                "estimated_hours = total_steps * 4 / 3600  # ~4 seconds per step with optimization\n",
                "\n",
                "print(f\"\\nüìã Configuration:\")\n",
                "print(f\"   Batch size: {BATCH_SIZE} (was 1)\")\n",
                "print(f\"   Gradient accumulation: {GRAD_ACCUM} (was 16)\")\n",
                "print(f\"   Effective batch: {EFFECTIVE_BATCH}\")\n",
                "print(f\"   Total steps: ~{total_steps}\")\n",
                "print(f\"   Estimated time: ~{estimated_hours:.1f} hours\")\n",
                "print(f\"   Checkpoints: Every epoch\")\n",
                "print(f\"\\n‚úÖ Configuration ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Initialize & Train\n",
                "import time\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"INITIALIZING DPO TRAINER\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "trainer = DPOTrainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=dataset,\n",
                "    processing_class=tokenizer\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Trainer initialized\")\n",
                "print(f\"\\n\" + \"=\"*80)\n",
                "print(\"üöÄ STARTING OPTIMIZED DPO TRAINING\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\n‚è±Ô∏è  Estimated: 2-3 hours\")\n",
                "print(f\"üìä Dataset: {len(dataset)} pairs\")\n",
                "print(f\"üéØ GPU utilization target: 70-85%\")\n",
                "print(f\"üíæ Checkpoints: Every epoch (auto-saved)\\n\")\n",
                "\n",
                "start_time = time.time()\n",
                "\n",
                "# Train\n",
                "trainer.train()\n",
                "\n",
                "training_time = (time.time() - start_time) / 3600\n",
                "\n",
                "print(f\"\\n\" + \"=\"*80)\n",
                "print(f\"‚úÖ TRAINING COMPLETE in {training_time:.2f} hours\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: SAVE MODELS (Critical - Auto-save with verification)\n",
                "import shutil\n",
                "import os\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üíæ SAVING MODELS (Auto-save with verification)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "LORA_PATH = \"/kaggle/working/dpo_lora_adapter\"\n",
                "MERGED_PATH = \"/kaggle/working/dpo_merged_model\"\n",
                "ZIP_PATH = \"/kaggle/working/aligned_model.zip\"\n",
                "\n",
                "# Step 1: Save LoRA adapter\n",
                "print(f\"\\n1Ô∏è‚É£ Saving LoRA adapter...\")\n",
                "model.save_pretrained(LORA_PATH)\n",
                "tokenizer.save_pretrained(LORA_PATH)\n",
                "\n",
                "if os.path.exists(LORA_PATH):\n",
                "    files = os.listdir(LORA_PATH)\n",
                "    print(f\"   ‚úÖ LoRA saved: {len(files)} files\")\n",
                "else:\n",
                "    print(f\"   ‚ùå LoRA save FAILED!\")\n",
                "\n",
                "# Step 2: Merge LoRA with base model\n",
                "print(f\"\\n2Ô∏è‚É£ Merging LoRA with base model...\")\n",
                "merged_model = model.merge_and_unload()\n",
                "merged_model.save_pretrained(MERGED_PATH)\n",
                "tokenizer.save_pretrained(MERGED_PATH)\n",
                "\n",
                "if os.path.exists(MERGED_PATH):\n",
                "    files = os.listdir(MERGED_PATH)\n",
                "    total_size = sum(os.path.getsize(os.path.join(MERGED_PATH, f)) for f in files) / (1024**2)\n",
                "    print(f\"   ‚úÖ Merged model saved: {len(files)} files, {total_size:.1f} MB\")\n",
                "else:\n",
                "    print(f\"   ‚ùå Merged model save FAILED!\")\n",
                "\n",
                "# Step 3: Create ZIP backup\n",
                "print(f\"\\n3Ô∏è‚É£ Creating ZIP backup...\")\n",
                "shutil.make_archive(\"/kaggle/working/aligned_model\", 'zip', MERGED_PATH)\n",
                "\n",
                "if os.path.exists(ZIP_PATH):\n",
                "    zip_size = os.path.getsize(ZIP_PATH) / (1024**2)\n",
                "    print(f\"   ‚úÖ ZIP created: {zip_size:.1f} MB\")\n",
                "else:\n",
                "    print(f\"   ‚ùå ZIP creation FAILED!\")\n",
                "\n",
                "# Step 4: Verify all outputs\n",
                "print(f\"\\n4Ô∏è‚É£ Verifying all outputs...\")\n",
                "print(f\"\\nFiles in /kaggle/working/:\")\n",
                "for item in os.listdir(\"/kaggle/working/\"):\n",
                "    full_path = os.path.join(\"/kaggle/working/\", item)\n",
                "    if os.path.isdir(full_path):\n",
                "        subfiles = len(os.listdir(full_path))\n",
                "        print(f\"   üìÅ {item}/ ({subfiles} files)\")\n",
                "    else:\n",
                "        size = os.path.getsize(full_path) / (1024**2)\n",
                "        print(f\"   üìÑ {item} ({size:.1f} MB)\")\n",
                "\n",
                "print(f\"\\n\" + \"=\"*80)\n",
                "print(\"‚úÖ ALL MODELS SAVED SUCCESSFULLY\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Quick Evaluation\n",
                "import random\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üìä QUICK EVALUATION\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Sample 100 pairs for quick eval\n",
                "eval_sample = random.sample(data, min(100, len(data)))\n",
                "\n",
                "def score_response(prompt, response):\n",
                "    text = f\"{prompt}\\n\\nResponse: {response}\"\n",
                "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
                "    inputs = {k: v.to(merged_model.device) for k, v in inputs.items()}\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = merged_model(**inputs, labels=inputs[\"input_ids\"])\n",
                "        return -outputs.loss.item()\n",
                "\n",
                "correct = 0\n",
                "for item in tqdm(eval_sample, desc=\"Evaluating\"):\n",
                "    chosen_score = score_response(item['prompt'], item['chosen'])\n",
                "    rejected_score = score_response(item['prompt'], item['rejected'])\n",
                "    if chosen_score > rejected_score:\n",
                "        correct += 1\n",
                "\n",
                "accuracy = 100 * correct / len(eval_sample)\n",
                "\n",
                "print(f\"\\n‚úÖ Preference Accuracy: {accuracy:.1f}%\")\n",
                "print(f\"   (Baseline was 96.8% with 411 pairs)\")\n",
                "\n",
                "if accuracy > 95:\n",
                "    print(f\"   üéâ EXCELLENT - Training successful!\")\n",
                "elif accuracy > 85:\n",
                "    print(f\"   ‚úÖ Good performance\")\n",
                "else:\n",
                "    print(f\"   ‚ö†Ô∏è Lower than expected\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 8: Final Summary & Download Instructions\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üéâ TRAINING COMPLETE - DOWNLOAD YOUR MODEL\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nüìä Results:\")\n",
                "print(f\"   Training time: {training_time:.2f} hours\")\n",
                "print(f\"   Preference accuracy: {accuracy:.1f}%\")\n",
                "print(f\"   Dataset: {len(data)} pairs\")\n",
                "\n",
                "print(f\"\\nüì• Files to Download:\")\n",
                "print(f\"   1. aligned_model.zip ({zip_size:.1f} MB) ‚Üê Main model\")\n",
                "print(f\"   2. dpo_merged_model/ ‚Üê Full folder\")\n",
                "print(f\"   3. dpo_lora_adapter/ ‚Üê LoRA only\")\n",
                "\n",
                "print(f\"\\nüîß How to Download:\")\n",
                "print(f\"   Option 1: Right sidebar ‚Üí Output ‚Üí Click files\")\n",
                "print(f\"   Option 2: After 'Save Version' ‚Üí Notebook page ‚Üí Output tab\")\n",
                "\n",
                "print(f\"\\n‚ö†Ô∏è IMPORTANT: Click 'Save Version' NOW to persist outputs!\")\n",
                "print(f\"   (Top right button ‚Üí Save & Run All)\")\n",
                "\n",
                "print(f\"\\n\" + \"=\"*80)\n",
                "print(\"‚ú® Production-grade Gricean-aligned model ready!\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}