{
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GriceBench Phase 5: DPO Rebuild - MAX GPU OPTIMIZATION\n\n## GPU Optimizations Applied\n- **Batched Inference** - Process 32 samples simultaneously\n- **Multi-GPU DataParallel** - Use both T4 GPUs\n- **Larger Batch Sizes** - Fill GPU memory\n- **Async Data Loading** - Overlap CPU/GPU work\n- **Flash Attention** - Faster attention computation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 1: SETUP WITH MAX GPU CONFIG\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '0'\nos.environ['TOKENIZERS_PARALLELISM'] = 'true'\n\nimport torch\nimport torch.nn as nn\nimport json\nimport random\nimport numpy as np\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom tqdm.auto import tqdm\n\n# Multi-GPU setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_gpus = torch.cuda.device_count()\nprint(f'Device: {device}')\nprint(f'Number of GPUs: {num_gpus}')\n\nfor i in range(num_gpus):\n    print(f'  GPU {i}: {torch.cuda.get_device_name(i)}')\n    print(f'    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB')\n\n# Enable optimizations\ntorch.backends.cudnn.benchmark = True\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nDATA_INPUT = Path('/kaggle/input/gricebench-scientific-fix')\nOUTPUT_DIR = Path('/kaggle/working')\nrandom.seed(42)\ntorch.manual_seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 2: INSTALL DEPENDENCIES\n!pip install -q transformers>=4.36.0 accelerate>=0.25.0 trl>=0.7.0 peft>=0.7.0 bitsandbytes\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch.nn.functional as F\n\nprint('Dependencies installed')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 3: LOAD MODEL WITH MULTI-GPU\nprint('=' * 70)\nprint('LOADING MODEL WITH MULTI-GPU SUPPORT')\nprint('=' * 70)\n\nMODEL_NAME = 'gpt2'\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'left'\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n)\n\n# Multi-GPU with DataParallel\nif num_gpus > 1:\n    print(f'Using DataParallel with {num_gpus} GPUs')\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)\nmodel.eval()\n\nprint(f'Model loaded and distributed across GPUs')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 4: LOAD CONTEXTS\nprint('=' * 70)\nprint('LOADING CONTEXTS')\nprint('=' * 70)\n\ncontexts = []\n\nval_path = DATA_INPUT / 'val_examples.json'\nif val_path.exists():\n    with open(val_path, 'r', encoding='utf-8') as f:\n        val_data = json.load(f)\n    for item in val_data:\n        ctx = item.get('context_text', item.get('context', ''))\n        if ctx and isinstance(ctx, str) and len(ctx) > 20:\n            contexts.append(ctx[:300])  # Shorter for faster processing\n    print(f'Loaded {len(contexts)} contexts')\n\ncontexts = list(set(contexts))\nrandom.shuffle(contexts)\ncontexts = contexts[:500]\nprint(f'Using {len(contexts)} unique contexts')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 5: BATCHED GENERATION FUNCTION (GPU MAXIMIZED)\nprint('=' * 70)\nprint('SETTING UP BATCHED GENERATION')\nprint('=' * 70)\n\n# HIGH BATCH SIZE FOR GPU UTILIZATION\nBATCH_SIZE = 32  # Much larger for GPU saturation\nMAX_NEW_TOKENS = 80\nMAX_INPUT_LENGTH = 200\n\ndef generate_batch(prompts: List[str], temperature: float = 0.7) -> List[str]:\n    \"\"\"\n    Generate responses for a batch of prompts - GPU OPTIMIZED.\n    \"\"\"\n    # Tokenize all prompts at once\n    inputs = tokenizer(\n        prompts,\n        return_tensors='pt',\n        padding=True,\n        truncation=True,\n        max_length=MAX_INPUT_LENGTH\n    ).to(device)\n    \n    # Generate with autocast for mixed precision\n    with torch.amp.autocast('cuda'):\n        with torch.no_grad():\n            # Get the actual model (unwrap DataParallel if needed)\n            gen_model = model.module if hasattr(model, 'module') else model\n            \n            outputs = gen_model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                temperature=temperature,\n                do_sample=True,\n                top_p=0.9,\n                pad_token_id=tokenizer.pad_token_id,\n                num_return_sequences=1,\n            )\n    \n    # Decode all at once\n    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    \n    # Extract response parts\n    cleaned = []\n    for resp in responses:\n        if 'Response:' in resp:\n            resp = resp.split('Response:')[-1].strip()\n        cleaned.append(resp[:150])\n    \n    return cleaned\n\nprint(f'Batch size: {BATCH_SIZE}')\nprint(f'Max new tokens: {MAX_NEW_TOKENS}')\nprint('Ready for batched generation')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 6: GENERATE ALL PREFERENCE PAIRS (GPU MAXIMIZED)\nprint('=' * 70)\nprint('GENERATING PREFERENCE PAIRS - GPU MAXIMIZED')\nprint('=' * 70)\n\n# Prepare all prompts upfront\nall_prompts = [f\"Context: {ctx}\\n\\nResponse:\" for ctx in contexts]\nprint(f'Total prompts: {len(all_prompts)}')\n\n# Generate 3 responses per context in large batches\nall_responses = {i: [] for i in range(len(contexts))}\n\nfor temp_idx, temperature in enumerate([0.6, 0.7, 0.8]):\n    print(f'\\nTemperature {temperature}:')\n    \n    for batch_start in tqdm(range(0, len(all_prompts), BATCH_SIZE), desc=f'Temp {temperature}'):\n        batch_end = min(batch_start + BATCH_SIZE, len(all_prompts))\n        batch_prompts = all_prompts[batch_start:batch_end]\n        \n        # Generate batch\n        batch_responses = generate_batch(batch_prompts, temperature)\n        \n        # Store responses\n        for i, resp in enumerate(batch_responses):\n            ctx_idx = batch_start + i\n            if ctx_idx < len(contexts):\n                all_responses[ctx_idx].append(resp)\n\nprint(f'\\nGeneration complete!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 7: CREATE PREFERENCE PAIRS\nprint('=' * 70)\nprint('CREATING PREFERENCE PAIRS')\nprint('=' * 70)\n\npreference_pairs = []\n\nfor ctx_idx, responses in all_responses.items():\n    if len(responses) >= 3:\n        ctx = contexts[ctx_idx]\n        \n        # Create 3 pairs per context\n        preference_pairs.append({\n            'id': len(preference_pairs),\n            'context': ctx,\n            'response_A': responses[0],\n            'response_B': responses[1],\n            'pair_type': 'AB'\n        })\n        preference_pairs.append({\n            'id': len(preference_pairs),\n            'context': ctx,\n            'response_A': responses[1],\n            'response_B': responses[2],\n            'pair_type': 'BC'\n        })\n        preference_pairs.append({\n            'id': len(preference_pairs),\n            'context': ctx,\n            'response_A': responses[0],\n            'response_B': responses[2],\n            'pair_type': 'AC'\n        })\n\nprint(f'Created {len(preference_pairs)} preference pairs')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 8: SAVE OUTPUTS\nprint('=' * 70)\nprint('SAVING OUTPUTS')\nprint('=' * 70)\n\n# Save pairs\npairs_path = OUTPUT_DIR / 'preference_pairs_1500.json'\nwith open(pairs_path, 'w', encoding='utf-8') as f:\n    json.dump(preference_pairs, f, indent=2, ensure_ascii=False)\nprint(f'‚úÖ Saved {len(preference_pairs)} pairs')\n\n# Create annotation template\nannotation_template = [{\n    'id': p['id'],\n    'context': p['context'],\n    'response_A': p['response_A'],\n    'response_B': p['response_B'],\n    'preference': '',\n    'reason': [],\n    'annotated': False\n} for p in preference_pairs]\n\ntemplate_path = OUTPUT_DIR / 'annotation_template.json'\nwith open(template_path, 'w', encoding='utf-8') as f:\n    json.dump(annotation_template, f, indent=2, ensure_ascii=False)\nprint(f'‚úÖ Saved annotation template')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 9: GPU UTILIZATION CHECK\nprint('=' * 70)\nprint('GPU UTILIZATION')\nprint('=' * 70)\n\nfor i in range(num_gpus):\n    mem_used = torch.cuda.memory_allocated(i) / 1e9\n    mem_total = torch.cuda.get_device_properties(i).total_memory / 1e9\n    print(f'GPU {i}: {mem_used:.2f} / {mem_total:.1f} GB ({100*mem_used/mem_total:.1f}%)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 10: SUMMARY\nprint('\\n' + '=' * 70)\nprint('PHASE 5 PAIR GENERATION COMPLETE')\nprint('=' * 70)\n\nprint(f'\\nüìä RESULTS:')\nprint(f'   Preference pairs: {len(preference_pairs)}')\nprint(f'   Contexts used: {len(contexts)}')\n\nprint(f'\\nüìÅ OUTPUT FILES:')\nprint(f'   preference_pairs_1500.json')\nprint(f'   annotation_template.json')\n\nprint(f'\\nüìã NEXT STEPS:')\nprint(f'   1. Download files')\nprint(f'   2. Annotate (set preference field)')\nprint(f'   3. Upload as annotated_preferences.json')\nprint(f'   4. Run DPO training cells')"
            ]
        }
    ]
}