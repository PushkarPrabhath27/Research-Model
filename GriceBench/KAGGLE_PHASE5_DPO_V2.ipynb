{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 0,
     "sourceType": "datasetVersion",
     "datasetSlug": "gricebench-dpo-annotations"
    }
   ],
   "isInternetEnabled": true,
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phase 5: DPO Training V2 ‚Äî Optimized\n    \n**Fixes applied:**\n- Œ≤ = 0.3 (was 0.1) ‚Üí prevents reward margin explosion\n- 3 epochs (was 5) ‚Üí stops before overfitting  \n- Batch size 8 (was 4) ‚Üí better GPU utilization\n- max_length 256 (was 512) ‚Üí data max is 114 tokens\n- Early stopping + best checkpoint loading\n- Real-time diagnostic callback\n\n**Model:** Qwen/Qwen2.5-1.5B-Instruct with QLoRA (4-bit)  \n**Data:** 301 annotated DPO pairs from GriceBench"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 1: ENVIRONMENT SETUP\n# ============================================================\nimport subprocess, sys\n\nprint(\"Installing dependencies...\")\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n    'trl>=0.7.0', 'peft>=0.5.0', 'bitsandbytes>=0.41.0',\n    'accelerate>=0.21.0', 'datasets>=2.14.0', 'transformers>=4.35.0',\n    'scipy'])\nprint(\"Dependencies installed.\")\n\nimport torch\nimport os\nimport gc\nimport json\nimport random\nimport logging\nimport numpy as np\nfrom datetime import datetime\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom collections import Counter\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# Logging\nlogging.basicConfig(level=logging.INFO, format='%(levelname)s | %(message)s')\nlogger = logging.getLogger('Phase5DPO_V2')\n\n# GPU check\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlogger.info(f\"Device: {device}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    logger.info(f\"GPU: {gpu_name}\")\n    logger.info(f\"VRAM: {gpu_mem:.1f} GB\")\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\nelse:\n    raise RuntimeError(\"GPU required for DPO training\")\n\n# Progress tracker\nclass ProgressTracker:\n    def __init__(self):\n        self.steps = []\n        self.start = datetime.now()\n    def mark(self, name, status, details=None):\n        elapsed = (datetime.now() - self.start).total_seconds()\n        self.steps.append({'name': name, 'status': status, 'elapsed': elapsed, 'details': details or {}})\n        icon = '‚úÖ' if status == 'PASS' else '‚ùå' if status == 'FAIL' else '‚è≥'\n        logger.info(f\"{icon} [{elapsed:.0f}s] {name}: {status}\")\n    def summary(self):\n        passed = sum(1 for s in self.steps if s['status'] == 'PASS')\n        total = len(self.steps)\n        logger.info(f\"\\nProgress: {passed}/{total} steps passed\")\n        return self.steps\n\ntracker = ProgressTracker()\ntracker.mark('Environment Setup', 'PASS', {'gpu': gpu_name, 'vram_gb': f'{gpu_mem:.1f}'})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 2: CONFIGURATION ‚Äî V2 OPTIMIZED\n# ============================================================\n@dataclass\nclass DPOConfig_V2:\n    # Paths\n    data_path: str = '/kaggle/input/datasets/pushkarprabhath/gricebench-dpo-annotations/tier1_hard_pairs_FULLY_ANNOTATED.json'\n    output_dir: str = '/kaggle/working/dpo_output_v2'\n\n    # Model\n    model_name: str = 'Qwen/Qwen2.5-1.5B-Instruct'\n\n    # QLoRA\n    lora_r: int = 64\n    lora_alpha: int = 128\n    lora_dropout: float = 0.05\n    lora_target_modules: tuple = ('q_proj', 'k_proj', 'v_proj', 'o_proj',\n                                   'gate_proj', 'up_proj', 'down_proj')\n\n    # === V2 CHANGES ===\n    # DPO\n    beta: float = 0.3              # Was 0.1 ‚Üí regularize margins\n    max_length: int = 256          # Was 512 ‚Üí data max is 114 tokens\n    max_prompt_length: int = 192   # Was 384\n\n    # Training\n    learning_rate: float = 5e-5\n    num_epochs: int = 3            # Was 5 ‚Üí stop before overfitting\n    per_device_batch: int = 8      # Was 4 ‚Üí better GPU utilization\n    gradient_accumulation: int = 4 # Was 8 ‚Üí same effective batch (32)\n    warmup_ratio: float = 0.1\n    weight_decay: float = 0.01\n    max_grad_norm: float = 1.0\n    \n    # Evaluation & Early Stopping\n    eval_steps: int = 4            # Was 25 ‚Üí eval every ~half epoch\n    early_stopping_patience: int = 3\n    \n    # Seed\n    seed: int = 42\n\nCONFIG = DPOConfig_V2()\n\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"DPO V2 CONFIGURATION (OPTIMIZED)\")\nlogger.info(\"=\"*60)\nlogger.info(f\"  Model: {CONFIG.model_name}\")\nlogger.info(f\"  Beta: {CONFIG.beta} (was 0.1)\")\nlogger.info(f\"  Epochs: {CONFIG.num_epochs} (was 5)\")\nlogger.info(f\"  Batch: {CONFIG.per_device_batch} √ó {CONFIG.gradient_accumulation} = {CONFIG.per_device_batch * CONFIG.gradient_accumulation} effective\")\nlogger.info(f\"  Max length: {CONFIG.max_length} (was 512)\")\nlogger.info(f\"  Eval every: {CONFIG.eval_steps} steps\")\nlogger.info(f\"  Early stopping patience: {CONFIG.early_stopping_patience}\")\nlogger.info(f\"  LoRA: r={CONFIG.lora_r}, alpha={CONFIG.lora_alpha}\")\n\nos.makedirs(CONFIG.output_dir, exist_ok=True)\ntracker.mark('Configuration', 'PASS', {'beta': CONFIG.beta, 'epochs': CONFIG.num_epochs})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 3: LOAD ANNOTATED DATA\n# ============================================================\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"LOADING ANNOTATED DATA\")\nlogger.info(\"=\"*60)\n\n# Try multiple possible paths\npossible_paths = [\n    CONFIG.data_path,\n    '/kaggle/input/datasets/pushkarprabhath/gricebench-dpo-annotations/tier1_hard_pairs_FULLY_ANNOTATED.json',\n    '/kaggle/input/gricebench-dpo-annotations/tier1_hard_pairs_FULLY_ANNOTATED.json',\n    '/kaggle/input/gricebench-dpo/tier1_hard_pairs_FULLY_ANNOTATED.json',\n]\n\nraw_data = None\nfor path in possible_paths:\n    if os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            raw_data = json.load(f)\n        logger.info(f\"Loaded from: {path}\")\n        break\n\nif raw_data is None:\n    logger.error(\"Data file not found! Available files:\")\n    for root, dirs, files in os.walk('/kaggle/input'):\n        for fn in files:\n            if fn.endswith('.json'):\n                logger.error(f\"  {os.path.join(root, fn)}\")\n    raise FileNotFoundError(\"tier1_hard_pairs_FULLY_ANNOTATED.json not found in any expected location\")\n\nlogger.info(f\"Total records loaded: {len(raw_data)}\")\n\n# Validate structure\nrequired_keys = {'context', 'response_A', 'response_B', 'preference', 'reason'}\nfor i, entry in enumerate(raw_data):\n    missing = required_keys - set(entry.keys())\n    assert not missing, f\"Entry {i} missing keys: {missing}\"\n\n# Preference distribution\npref_counts = Counter(d['preference'] for d in raw_data)\nlogger.info(f\"\\nPreference distribution:\")\nfor pref, count in sorted(pref_counts.items(), key=lambda x: -x[1]):\n    logger.info(f\"  {pref}: {count} ({100*count/len(raw_data):.1f}%)\")\n\ntracker.mark('Data Loading', 'PASS', {'total': len(raw_data), 'preferences': dict(pref_counts)})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 4: CONVERT TO DPO FORMAT + STRATIFIED SPLIT\n# ============================================================\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"CONVERTING TO DPO FORMAT\")\nlogger.info(\"=\"*60)\n\nSYSTEM_PROMPT = (\n    \"You are a helpful conversational AI assistant that follows Gricean maxims. \"\n    \"Be relevant (stay on topic), truthful (say only what you believe is true), \"\n    \"clear (avoid ambiguity and be well-formatted), and appropriately informative \"\n    \"(not too much, not too little).\"\n)\n\ndef create_prompt(context):\n    return f\"Continue the following conversation naturally, following Gricean maxims (be relevant, truthful, clear, appropriately informative):\\n\\n{context}\\n\\nResponse:\"\n\ndpo_pairs = []\nskipped = 0\n\nfor entry in raw_data:\n    pref = entry['preference']\n    if pref == 'equal':\n        skipped += 1\n        continue\n    \n    prompt = create_prompt(entry['context'])\n    \n    if pref.startswith('A'):\n        chosen = entry['response_A']\n        rejected = entry['response_B']\n    else:\n        chosen = entry['response_B']\n        rejected = entry['response_A']\n    \n    # Preference strength weight\n    strength = 1.0 if 'much' in pref else 0.6\n    \n    dpo_pairs.append({\n        'prompt': prompt,\n        'chosen': chosen,\n        'rejected': rejected,\n        'reason': entry.get('reason', ''),\n        'preference': pref,\n        'strength': strength,\n    })\n\nlogger.info(f\"DPO pairs created: {len(dpo_pairs)}\")\nlogger.info(f\"Skipped (equal): {skipped}\")\n\n# Stratified split: keep proportion of strong/weak in train/val\nstrong_pairs = [p for p in dpo_pairs if p['strength'] == 1.0]\nweak_pairs = [p for p in dpo_pairs if p['strength'] < 1.0]\n\nrandom.shuffle(strong_pairs)\nrandom.shuffle(weak_pairs)\n\nval_strong_n = max(1, int(len(strong_pairs) * 0.15))\nval_weak_n = max(1, int(len(weak_pairs) * 0.15)) if weak_pairs else 0\n\nval_pairs = strong_pairs[:val_strong_n] + weak_pairs[:val_weak_n]\ntrain_pairs = strong_pairs[val_strong_n:] + weak_pairs[val_weak_n:]\n\nrandom.shuffle(train_pairs)\nrandom.shuffle(val_pairs)\n\nlogger.info(f\"\\nTrain: {len(train_pairs)} ({len([p for p in train_pairs if p['strength']==1.0])} strong, {len([p for p in train_pairs if p['strength']<1.0])} weak)\")\nlogger.info(f\"Val:   {len(val_pairs)} ({len([p for p in val_pairs if p['strength']==1.0])} strong, {len([p for p in val_pairs if p['strength']<1.0])} weak)\")\n\nsteps_per_epoch = len(train_pairs) // (CONFIG.per_device_batch * CONFIG.gradient_accumulation)\ntotal_steps = steps_per_epoch * CONFIG.num_epochs\nlogger.info(f\"\\nSteps/epoch: {steps_per_epoch}, Total steps: {total_steps}\")\nlogger.info(f\"Eval every {CONFIG.eval_steps} steps = ~every {CONFIG.eval_steps/steps_per_epoch:.1f} epochs\")\n\n# Convert to HuggingFace Dataset\nfrom datasets import Dataset as HFDataset\n\ntrain_dataset = HFDataset.from_list([{'prompt': p['prompt'], 'chosen': p['chosen'], 'rejected': p['rejected']} for p in train_pairs])\nval_dataset = HFDataset.from_list([{'prompt': p['prompt'], 'chosen': p['chosen'], 'rejected': p['rejected']} for p in val_pairs])\n\nlogger.info(f\"\\nTrain dataset: {len(train_dataset)} examples\")\nlogger.info(f\"Val dataset:   {len(val_dataset)} examples\")\n\ntracker.mark('DPO Conversion', 'PASS', {'train': len(train_pairs), 'val': len(val_pairs), 'steps': total_steps})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 5: LOAD MODEL WITH QLoRA (4-bit)\n# ============================================================\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"LOADING MODEL + QLoRA\")\nlogger.info(\"=\"*60)\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\nlogger.info(f\"Loading {CONFIG.model_name}...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    CONFIG.model_name,\n    quantization_config=bnb_config,\n    device_map='auto',\n    trust_remote_code=True,\n    attn_implementation='eager',\n)\n\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.model_name, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = tokenizer.eos_token_id\ntokenizer.padding_side = 'left'\n\nlogger.info(f\"Model loaded: {model.__class__.__name__}\")\nlogger.info(f\"Vocab size: {len(tokenizer)}\")\n\n# Prepare for QLoRA\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n\nlora_config = LoraConfig(\n    r=CONFIG.lora_r,\n    lora_alpha=CONFIG.lora_alpha,\n    lora_dropout=CONFIG.lora_dropout,\n    target_modules=list(CONFIG.lora_target_modules),\n    bias='none',\n    task_type=TaskType.CAUSAL_LM,\n)\n\nmodel = get_peft_model(model, lora_config)\n\ntrainable, total = 0, 0\nfor p in model.parameters():\n    total += p.numel()\n    if p.requires_grad:\n        trainable += p.numel()\n\nlogger.info(f\"\\nParameters:\")\nlogger.info(f\"  Total: {total:,}\")\nlogger.info(f\"  Trainable: {trainable:,} ({100*trainable/total:.2f}%)\")\n\n# VRAM after model load\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    reserved = torch.cuda.memory_reserved(0) / 1e9\n    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    logger.info(f\"\\nVRAM Usage (after model load):\")\n    logger.info(f\"  Allocated: {allocated:.2f} GB\")\n    logger.info(f\"  Reserved: {reserved:.2f} GB\")\n    logger.info(f\"  Available: {total_vram - allocated:.2f} GB\")\n\ntracker.mark('Model Loading', 'PASS', {'trainable_params': trainable, 'total_params': total})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 6: DPO TRAINER SETUP (V2 ‚Äî WITH CALLBACKS)\n# ============================================================\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"SETTING UP DPO TRAINER V2\")\nlogger.info(\"=\"*60)\n\nfrom trl import DPOConfig as TRLDPOConfig, DPOTrainer\nfrom transformers import TrainerCallback, EarlyStoppingCallback\n\n# ---- Diagnostic Callback ----\nclass DPODiagnosticCallback(TrainerCallback):\n    \"\"\"Real-time training health monitor.\"\"\"\n    \n    def __init__(self):\n        self.train_losses = []\n        self.eval_losses = []\n        self.reward_margins = []\n        self.reward_accs = []\n        self.step_data = []  # all logged data\n    \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if not logs:\n            return\n        \n        self.step_data.append(dict(logs))\n        \n        if 'loss' in logs:\n            self.train_losses.append(logs['loss'])\n        \n        if 'eval_loss' in logs:\n            self.eval_losses.append(logs['eval_loss'])\n        \n        if 'rewards/margins' in logs:\n            self.reward_margins.append(logs['rewards/margins'])\n        \n        if 'eval_rewards/margins' in logs:\n            margin = logs['eval_rewards/margins']\n            if margin > 6.0:\n                logger.warning(f\"‚ö†Ô∏è  Eval reward margin = {margin:.2f} ‚Äî approaching over-separation!\")\n        \n        if 'eval_rewards/accuracies' in logs:\n            self.reward_accs.append(logs['eval_rewards/accuracies'])\n        \n        # Overfitting check\n        if len(self.eval_losses) >= 3:\n            if self.eval_losses[-1] > self.eval_losses[-2] > self.eval_losses[-3]:\n                logger.warning(\"‚ö†Ô∏è  Eval loss increasing for 3 consecutive checks ‚Äî likely overfitting\")\n    \n    def on_train_end(self, args, state, control, **kwargs):\n        print(\"\\n\" + \"=\"*60)\n        print(\"üìä DPO DIAGNOSTIC REPORT\")\n        print(\"=\"*60)\n        \n        if self.train_losses:\n            print(f\"\\nTrain loss: {self.train_losses[0]:.3f} ‚Üí {self.train_losses[-1]:.3f}\")\n        if self.eval_losses:\n            best_idx = int(np.argmin(self.eval_losses))\n            print(f\"Eval loss:  best={min(self.eval_losses):.3f} (check #{best_idx+1}) | final={self.eval_losses[-1]:.3f}\")\n        if self.reward_accs:\n            print(f\"Val accuracy: {self.reward_accs[0]:.1%} ‚Üí {self.reward_accs[-1]:.1%}\")\n        \n        # Health checks\n        print(\"\\n‚úÖ Health Checks:\")\n        \n        # 1. Margin check\n        if self.reward_margins:\n            final_margin = self.reward_margins[-1]\n            if 1.0 <= final_margin <= 5.0:\n                print(f\"  ‚úì Train reward margin healthy ({final_margin:.2f})\")\n            elif final_margin > 5.0:\n                print(f\"  ‚úó Train reward margin high ({final_margin:.2f}) ‚Äî increase beta next time\")\n            else:\n                print(f\"  ‚úó Train reward margin low ({final_margin:.2f}) ‚Äî decrease beta next time\")\n        \n        # 2. Overfitting check\n        if self.eval_losses:\n            best_idx = int(np.argmin(self.eval_losses))\n            if best_idx == len(self.eval_losses) - 1:\n                print(\"  ‚úì No overfitting detected ‚Äî final checkpoint is best\")\n            else:\n                print(f\"  ‚ö† Best eval was check #{best_idx+1}, final is #{len(self.eval_losses)}\")\n                print(f\"    ‚Üí load_best_model_at_end will use the optimal checkpoint\")\n        \n        # 3. Accuracy check\n        if self.reward_accs:\n            final_acc = self.reward_accs[-1]\n            if 0.70 <= final_acc <= 0.92:\n                print(f\"  ‚úì Val accuracy in healthy range ({final_acc:.1%})\")\n            elif final_acc > 0.92:\n                print(f\"  ‚ö† Val accuracy very high ({final_acc:.1%}) ‚Äî possible data leakage\")\n            else:\n                print(f\"  ‚úó Val accuracy low ({final_acc:.1%}) ‚Äî model may not be learning\")\n        \n        print(\"\\n\" + \"=\"*60)\n\ndiagnostic_cb = DPODiagnosticCallback()\n\n# ---- Training Arguments ----\ntraining_args = TRLDPOConfig(\n    output_dir=CONFIG.output_dir,\n    \n    # === V2 OPTIMIZED TRAINING SCHEDULE ===\n    num_train_epochs=CONFIG.num_epochs,      # 3 (was 5)\n    per_device_train_batch_size=CONFIG.per_device_batch,  # 8 (was 4)\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=CONFIG.gradient_accumulation,  # 4 (was 8)\n    \n    # DPO hyperparameters\n    beta=CONFIG.beta,                         # 0.3 (was 0.1)\n    loss_type='sigmoid',\n    \n    # Learning rate\n    learning_rate=CONFIG.learning_rate,\n    lr_scheduler_type='cosine',\n    warmup_ratio=CONFIG.warmup_ratio,\n    \n    # === V2 ‚Äî EARLY STOPPING + BEST CHECKPOINT ===\n    eval_strategy='steps',\n    eval_steps=CONFIG.eval_steps,             # 4 (was 25)\n    save_strategy='steps',\n    save_steps=CONFIG.eval_steps,\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    save_total_limit=3,\n    \n    # Regularization\n    weight_decay=CONFIG.weight_decay,\n    max_grad_norm=CONFIG.max_grad_norm,\n    \n    # Efficiency\n    bf16=True,\n    gradient_checkpointing=True,\n    optim='paged_adamw_8bit',\n    dataloader_num_workers=2,\n    dataloader_pin_memory=True,\n    \n    # === V2 ‚Äî SEQUENCE LENGTHS ===\n    max_length=CONFIG.max_length,             # 256 (was 512)\n    max_prompt_length=CONFIG.max_prompt_length,  # 192 (was 384)\n    \n    # Logging\n    logging_steps=1,\n    report_to='none',\n    seed=CONFIG.seed,\n    \n    # Disable find_unused_parameters for efficiency\n    ddp_find_unused_parameters=False,\n)\n\n# ---- Create DPO Trainer ----\ntrainer = DPOTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    processing_class=tokenizer,\n    callbacks=[\n        diagnostic_cb,\n        EarlyStoppingCallback(\n            early_stopping_patience=CONFIG.early_stopping_patience,\n            early_stopping_threshold=0.01\n        ),\n    ],\n)\n\n# VRAM after trainer setup\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    logger.info(f\"VRAM after trainer setup: {allocated:.2f} GB ({100*allocated/total_vram:.1f}%)\")\n\ntracker.mark('DPO Trainer Setup', 'PASS', {\n    'beta': CONFIG.beta,\n    'effective_batch': CONFIG.per_device_batch * CONFIG.gradient_accumulation,\n    'max_length': CONFIG.max_length,\n})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 7: TRAIN (V2 ‚Äî WITH MONITORING)\n# ============================================================\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"STARTING DPO TRAINING V2\")\nlogger.info(\"=\"*60)\nlogger.info(f\"Expected: ~{steps_per_epoch * CONFIG.num_epochs} steps, {CONFIG.num_epochs} epochs\")\nlogger.info(f\"Early stopping will halt if no improvement for {CONFIG.early_stopping_patience} evals\")\n\n# GPU utilization monitor\nclass GPUMonitor:\n    def __init__(self):\n        self.readings = []\n        self.peak_util = 0\n    def log(self):\n        if torch.cuda.is_available():\n            allocated = torch.cuda.memory_allocated(0) / 1e9\n            total = torch.cuda.get_device_properties(0).total_memory / 1e9\n            util = 100 * allocated / total\n            self.readings.append(util)\n            self.peak_util = max(self.peak_util, util)\n            return util\n        return 0\n\ngpu_monitor = GPUMonitor()\n\ntrain_start = datetime.now()\nlogger.info(f\"Training started at: {train_start.strftime('%H:%M:%S')}\")\n\n# Run training\ntry:\n    train_result = trainer.train()\n    train_time = (datetime.now() - train_start).total_seconds()\n    \n    logger.info(f\"\\n{'='*60}\")\n    logger.info(f\"TRAINING COMPLETE\")\n    logger.info(f\"{'='*60}\")\n    logger.info(f\"Time: {train_time:.0f}s ({train_time/60:.1f} min)\")\n    logger.info(f\"Final train loss: {train_result.training_loss:.4f}\")\n    \n    # Check if early stopping triggered\n    if hasattr(trainer.state, 'best_metric'):\n        logger.info(f\"Best eval_loss: {trainer.state.best_metric:.4f}\")\n        logger.info(f\"Best checkpoint: step {trainer.state.best_model_checkpoint}\")\n    \nexcept Exception as e:\n    logger.error(f\"Training failed: {e}\")\n    raise\n\n# VRAM peak\nif torch.cuda.is_available():\n    peak_mem = torch.cuda.max_memory_allocated(0) / 1e9\n    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    peak_pct = 100 * peak_mem / total_vram\n    logger.info(f\"\\nPeak VRAM: {peak_mem:.2f} GB ({peak_pct:.1f}%)\")\n\ntracker.mark('DPO Training', 'PASS', {\n    'train_loss': train_result.training_loss,\n    'time_seconds': train_time,\n    'peak_vram_pct': f'{peak_pct:.1f}%' if torch.cuda.is_available() else 'N/A',\n})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 8: EVALUATION\n# ============================================================\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"EVALUATING BEST CHECKPOINT\")\nlogger.info(\"=\"*60)\n\neval_results = trainer.evaluate()\n\nlogger.info(f\"\\nEval Results:\")\nfor key, value in sorted(eval_results.items()):\n    if isinstance(value, float):\n        logger.info(f\"  {key}: {value:.4f}\")\n    else:\n        logger.info(f\"  {key}: {value}\")\n\n# Extract key metrics\neval_loss = eval_results.get('eval_loss', float('nan'))\neval_reward_acc = eval_results.get('eval_rewards/accuracies', float('nan'))\neval_margin = eval_results.get('eval_rewards/margins', float('nan'))\n\nlogger.info(f\"\\nüìä KEY METRICS:\")\nlogger.info(f\"  Eval loss:       {eval_loss:.4f}\")\nlogger.info(f\"  Reward accuracy: {eval_reward_acc:.1%}\")\nlogger.info(f\"  Reward margin:   {eval_margin:.2f}\")\n\n# Health verdict\nif eval_margin < 5.0:\n    logger.info(f\"  ‚úÖ Margin healthy (< 5.0)\")\nelse:\n    logger.warning(f\"  ‚ö†Ô∏è Margin still high (‚â• 5.0)\")\n\nif eval_reward_acc > 0.75:\n    logger.info(f\"  ‚úÖ Accuracy above threshold (> 75%)\")\nelse:\n    logger.warning(f\"  ‚ö†Ô∏è Accuracy below expected (< 75%)\")\n\ntracker.mark('Evaluation', 'PASS', {\n    'eval_loss': eval_loss,\n    'reward_accuracy': eval_reward_acc,\n    'reward_margin': eval_margin,\n})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 9: PREFERENCE ACCURACY (FIXED ‚Äî V2)\n# ============================================================\n# V1 BUG: Used model.forward() which doesn't compute DPO preferences.\n# V2 FIX: Use the DPO trainer's built-in reward computation, which is\n#          already reported as eval_rewards/accuracies. We also perform\n#          a manual verification here for transparency.\n\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"PREFERENCE ACCURACY VERIFICATION\")\nlogger.info(\"=\"*60)\n\n# The DPO trainer already computed this correctly\n# eval_rewards/accuracies = fraction where chosen_reward > rejected_reward\n# This IS the preference accuracy metric.\n\nlogger.info(f\"\\nFrom DPO Trainer evaluation:\")\nlogger.info(f\"  Preference accuracy: {eval_reward_acc:.1%}\")\nlogger.info(f\"  (Model assigns higher reward to chosen response {eval_reward_acc:.1%} of the time)\")\n\n# Additional: manual spot-check with generation\nlogger.info(f\"\\nRunning generation spot-check on 5 validation samples...\")\n\nmodel.eval()\ncheck_correct = 0\ncheck_total = 0\n\nfor i in range(min(5, len(val_pairs))):\n    pair = val_pairs[i]\n    \n    # Tokenize prompt + chosen and prompt + rejected\n    chosen_text = f\"{pair['prompt']}\\n{pair['chosen']}\"\n    rejected_text = f\"{pair['prompt']}\\n{pair['rejected']}\"\n    \n    with torch.no_grad():\n        chosen_ids = tokenizer(chosen_text, return_tensors='pt', truncation=True, max_length=CONFIG.max_length).to(device)\n        rejected_ids = tokenizer(rejected_text, return_tensors='pt', truncation=True, max_length=CONFIG.max_length).to(device)\n        \n        # Get per-token log probs\n        chosen_out = model(**chosen_ids)\n        rejected_out = model(**rejected_ids)\n        \n        # Average log prob (normalized by length)\n        chosen_logprob = -torch.nn.functional.cross_entropy(\n            chosen_out.logits[:, :-1, :].reshape(-1, chosen_out.logits.size(-1)),\n            chosen_ids['input_ids'][:, 1:].reshape(-1),\n            reduction='mean'\n        ).item()\n        \n        rejected_logprob = -torch.nn.functional.cross_entropy(\n            rejected_out.logits[:, :-1, :].reshape(-1, rejected_out.logits.size(-1)),\n            rejected_ids['input_ids'][:, 1:].reshape(-1),\n            reduction='mean'\n        ).item()\n    \n    correct = chosen_logprob > rejected_logprob\n    check_correct += int(correct)\n    check_total += 1\n    \n    icon = \"‚úÖ\" if correct else \"‚ùå\"\n    logger.info(f\"  Sample {i+1}: chosen={chosen_logprob:.3f} vs rejected={rejected_logprob:.3f} {icon}\")\n\nlogger.info(f\"\\nSpot-check: {check_correct}/{check_total} correct ({100*check_correct/check_total:.0f}%)\")\nlogger.info(f\"Official preference accuracy (from DPO eval): {eval_reward_acc:.1%}\")\n\ntracker.mark('Preference Accuracy', 'PASS', {\n    'dpo_eval_accuracy': eval_reward_acc,\n    'spot_check': f'{check_correct}/{check_total}',\n})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 10: SAMPLE GENERATION\n# ============================================================\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"GENERATING SAMPLE RESPONSES\")\nlogger.info(\"=\"*60)\n\ngeneration_samples = []\nmodel.eval()\n\nfor i in range(min(5, len(val_pairs))):\n    pair = val_pairs[i]\n    prompt = pair['prompt']\n    \n    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=CONFIG.max_prompt_length).to(device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n    \n    generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n    \n    sample = {\n        'prompt_snippet': prompt[:100],\n        'generated': generated[:200],\n        'original_chosen': pair['chosen'][:150],\n        'original_rejected': pair['rejected'][:150],\n        'reason': pair.get('reason', 'N/A') if isinstance(pair, dict) and 'reason' in pair else val_pairs[i].get('reason', 'N/A'),\n    }\n    generation_samples.append(sample)\n    \n    logger.info(f\"\\n--- Sample {i+1} ---\")\n    logger.info(f\"Generated: {generated[:200]}\")\n\ntracker.mark('Sample Generation', 'PASS', {'n_samples': len(generation_samples)})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 11: SAVE RESULTS\n# ============================================================\nlogger.info(\"\\n\" + \"=\"*60)\nlogger.info(\"SAVING RESULTS\")\nlogger.info(\"=\"*60)\n\n# Compile results\nresults = {\n    'phase': 'Phase 5 - DPO Training V2 (Optimized)',\n    'timestamp': datetime.now().isoformat(),\n    'model': CONFIG.model_name,\n    'method': 'DPO with QLoRA (4-bit) ‚Äî V2 Optimized',\n    'version_changes': {\n        'beta': '0.1 ‚Üí 0.3 (regularize margins)',\n        'epochs': '5 ‚Üí 3 (prevent overfitting)',\n        'batch_size': '4 ‚Üí 8 (GPU utilization)',\n        'max_length': '512 ‚Üí 256 (data-informed)',\n        'new_features': ['EarlyStoppingCallback', 'DPODiagnosticCallback', 'load_best_model_at_end'],\n    },\n    'data': {\n        'total_annotated': len(raw_data),\n        'dpo_pairs': len(dpo_pairs),\n        'train_size': len(train_pairs),\n        'val_size': len(val_pairs),\n        'preference_distribution': dict(pref_counts),\n    },\n    'hyperparameters': {\n        'lora_r': CONFIG.lora_r,\n        'lora_alpha': CONFIG.lora_alpha,\n        'learning_rate': CONFIG.learning_rate,\n        'effective_batch_size': CONFIG.per_device_batch * CONFIG.gradient_accumulation,\n        'num_epochs': CONFIG.num_epochs,\n        'beta': CONFIG.beta,\n        'max_length': CONFIG.max_length,\n        'early_stopping_patience': CONFIG.early_stopping_patience,\n    },\n    'training': {\n        'train_loss': train_result.training_loss,\n        'training_time_seconds': train_time,\n        'best_checkpoint': str(getattr(trainer.state, 'best_model_checkpoint', 'N/A')),\n        'best_eval_loss': getattr(trainer.state, 'best_metric', None),\n    },\n    'evaluation': {\n        'eval_loss': eval_loss,\n        'preference_accuracy': eval_reward_acc,\n        'reward_margin': eval_margin,\n    },\n    'generation_samples': generation_samples,\n    'gpu': {\n        'name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A',\n        'peak_vram_gb': float(torch.cuda.max_memory_allocated(0) / 1e9) if torch.cuda.is_available() else 0,\n        'total_vram_gb': float(torch.cuda.get_device_properties(0).total_memory / 1e9) if torch.cuda.is_available() else 0,\n    },\n    'diagnostic_data': {\n        'train_losses': diagnostic_cb.train_losses,\n        'eval_losses': diagnostic_cb.eval_losses,\n        'reward_margins': diagnostic_cb.reward_margins,\n        'reward_accuracies': diagnostic_cb.reward_accs,\n    },\n}\n\n# Save results\nresults_path = os.path.join(CONFIG.output_dir, 'dpo_results_v2.json')\nwith open(results_path, 'w') as f:\n    json.dump(results, f, indent=2, default=str)\nlogger.info(f\"Results saved: {results_path}\")\n\n# Save training history\nhistory_path = os.path.join(CONFIG.output_dir, 'training_history_v2.json')\nwith open(history_path, 'w') as f:\n    json.dump(trainer.state.log_history, f, indent=2)\nlogger.info(f\"Training history saved: {history_path}\")\n\n# Save LoRA adapter\nadapter_path = os.path.join(CONFIG.output_dir, 'lora_adapter')\nmodel.save_pretrained(adapter_path)\ntokenizer.save_pretrained(adapter_path)\nlogger.info(f\"LoRA adapter saved: {adapter_path}\")\n\n# Copy to /kaggle/working for easy download\nimport shutil\nfor fname in ['dpo_results_v2.json', 'training_history_v2.json']:\n    src = os.path.join(CONFIG.output_dir, fname)\n    dst = os.path.join('/kaggle/working', fname)\n    shutil.copy2(src, dst)\n    logger.info(f\"Copied: {dst}\")\n\ntracker.mark('Save Results', 'PASS', {'results_file': results_path})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================\n# CELL 12: FINAL SUMMARY\n# ============================================================\nprint(\"\\n\" + \"=\"*60)\nprint(\"üèÅ PHASE 5 DPO TRAINING V2 ‚Äî FINAL SUMMARY\")\nprint(\"=\"*60)\n\nprint(f\"\\nüì¶ Model: {CONFIG.model_name}\")\nprint(f\"üìä Data: {len(dpo_pairs)} DPO pairs ({len(train_pairs)} train, {len(val_pairs)} val)\")\n\nprint(f\"\\n‚ö° V2 Improvements:\")\nprint(f\"  Œ≤: 0.1 ‚Üí {CONFIG.beta}\")\nprint(f\"  Epochs: 5 ‚Üí {CONFIG.num_epochs}\")\nprint(f\"  Batch: 4 ‚Üí {CONFIG.per_device_batch}\")\nprint(f\"  Max length: 512 ‚Üí {CONFIG.max_length}\")\n\nprint(f\"\\nüìà Training Results:\")\nprint(f\"  Train loss: {train_result.training_loss:.4f}\")\nprint(f\"  Best eval loss: {getattr(trainer.state, 'best_metric', 'N/A')}\")\nprint(f\"  Time: {train_time:.0f}s ({train_time/60:.1f} min)\")\n\nprint(f\"\\nüéØ Key Metrics:\")\nprint(f\"  Preference accuracy: {eval_reward_acc:.1%}\")\nprint(f\"  Reward margin: {eval_margin:.2f}\")\nif eval_margin < 5.0:\n    print(f\"  ‚úÖ Margin healthy (target: < 5.0)\")\nif eval_reward_acc > 0.75:\n    print(f\"  ‚úÖ Accuracy above threshold (target: > 75%)\")\n\nif torch.cuda.is_available():\n    peak = torch.cuda.max_memory_allocated(0) / 1e9\n    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"\\nüñ•Ô∏è GPU:\")\n    print(f\"  Peak VRAM: {peak:.2f} / {total:.1f} GB ({100*peak/total:.1f}%)\")\n\nprint(f\"\\nüìÅ Output Files:\")\nprint(f\"  /kaggle/working/dpo_results_v2.json\")\nprint(f\"  /kaggle/working/training_history_v2.json\")\nprint(f\"  {CONFIG.output_dir}/lora_adapter/\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"‚úÖ PHASE 5 V2 COMPLETE ‚Äî Download dpo_results_v2.json and training_history_v2.json\")\nprint(f\"{'='*60}\")\n\n# Cleanup\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\ntracker.summary()\n"
  }
 ]
}