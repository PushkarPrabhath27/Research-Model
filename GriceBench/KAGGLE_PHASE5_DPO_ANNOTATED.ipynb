{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 14639592,
          "sourceType": "datasetVersion",
          "datasetId": 9329674
        }
      ],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Phase 5: DPO Training with Human-Annotated Preferences\n\n**Version**: 3.0 (Production-Grade)\n\n## Architecture\n- **Base Model**: Qwen/Qwen2.5-1.5B-Instruct (QLoRA 4-bit)\n- **Method**: Direct Preference Optimization (DPO)\n- **Data**: 500 human-annotated preference pairs (301 usable)\n- **GPU Target**: 90%+ utilization on T4\n\n## Key Design Decisions\n- **QLoRA**: 4-bit quantized base + LoRA adapters = fits T4 easily with large batches\n- **Gradient Accumulation**: Effective batch size of 32 for stable training\n- **Cosine LR Schedule**: With warmup for smooth convergence\n- **Per-sample logging**: Track every preference pair's contribution\n\n## Execution Order\nRun ALL cells sequentially. Do NOT skip cells.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 1: Environment Setup & Dependencies\n# ============================================================================\nimport subprocess\nimport sys\nimport time\n\ncell_start = time.time()\n\n# Install required packages\npackages = [\n    \"trl>=0.12.0\",\n    \"peft>=0.14.0\",\n    \"bitsandbytes>=0.45.0\",\n    \"accelerate>=1.2.0\",\n    \"datasets>=3.2.0\",\n    \"transformers>=4.47.0\",\n    \"scipy\",\n]\n\nprint(\"Installing dependencies...\")\nfor pkg in packages:\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\nprint(\"Dependencies installed.\\n\")\n\n# Core imports\nimport os\nimport json\nimport random\nimport logging\nimport gc\nimport numpy as np\nfrom datetime import datetime\nfrom dataclasses import dataclass, field\nfrom typing import List, Dict, Tuple, Optional, Any\nfrom pathlib import Path\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# HuggingFace imports\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\nfrom trl import DPOConfig, DPOTrainer\nfrom datasets import Dataset as HFDataset\n\n# Logging setup\nclass ColoredFormatter(logging.Formatter):\n    COLORS = {'DEBUG': '\\033[36m', 'INFO': '\\033[32m', 'WARNING': '\\033[33m', 'ERROR': '\\033[31m'}\n    RESET = '\\033[0m'\n    def format(self, record):\n        color = self.COLORS.get(record.levelname, '')\n        record.levelname = f\"{color}{record.levelname}{self.RESET}\"\n        return super().format(record)\n\nlogger = logging.getLogger('Phase5DPO')\nlogger.setLevel(logging.DEBUG)\nif not logger.handlers:\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.INFO)\n    ch.setFormatter(ColoredFormatter('%(levelname)s | %(message)s'))\n    logger.addHandler(ch)\n\n    os.makedirs('/kaggle/working/logs', exist_ok=True)\n    fh = logging.FileHandler(f'/kaggle/working/logs/dpo_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log')\n    fh.setLevel(logging.DEBUG)\n    fh.setFormatter(logging.Formatter('%(asctime)s | %(levelname)s | %(message)s'))\n    logger.addHandler(fh)\n\n# Checkpoint tracker\nclass CheckpointTracker:\n    def __init__(self):\n        self.checkpoints = {}\n        self.t0 = time.time()\n\n    def mark(self, name, status='PASS', details=None):\n        elapsed = time.time() - self.t0\n        self.checkpoints[name] = {'status': status, 'time': elapsed, 'details': details or {}}\n        icon = '‚úÖ' if status == 'PASS' else '‚ùå' if status == 'FAIL' else '‚ö†Ô∏è'\n        logger.info(f\"{icon} CHECKPOINT [{name}]: {status} ({elapsed:.1f}s)\")\n        if details:\n            for k, v in details.items():\n                logger.info(f\"   {k}: {v}\")\n\n    def summary(self):\n        logger.info(\"=\" * 60)\n        logger.info(\"CHECKPOINT SUMMARY\")\n        logger.info(\"=\" * 60)\n        for name, data in self.checkpoints.items():\n            icon = '‚úÖ' if data['status'] == 'PASS' else '‚ùå'\n            logger.info(f\"{icon} {name}: {data['status']} ({data['time']:.1f}s)\")\n\ntracker = CheckpointTracker()\n\n# Seeds\ndef set_seeds(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True  # Faster convolutions\n\nset_seeds(42)\n\n# GPU check\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nlogger.info(f\"Device: {device}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    logger.info(f\"GPU: {gpu_name}\")\n    logger.info(f\"VRAM: {gpu_mem:.1f} GB\")\n    # Enable TF32 for faster matmuls on Ampere+ GPUs\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\nelse:\n    raise RuntimeError(\"GPU required for DPO training!\")\n\ntracker.mark('Environment Setup', 'PASS', {'device': str(device), 'gpu': gpu_name})\nprint(f\"\\nCELL 1 COMPLETE ({time.time()-cell_start:.1f}s): Environment ready\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 2: Configuration\n# ============================================================================\n\n@dataclass\nclass DPOConfig_Custom:\n    # Paths\n    data_path: str = '/kaggle/input/datasets/pushkarprabhath/gricebench-dpo-annotations/tier1_hard_pairs_FULLY_ANNOTATED.json'\n    output_dir: str = '/kaggle/working/dpo_output'\n\n    # Model\n    model_name: str = 'Qwen/Qwen2.5-1.5B-Instruct'\n    max_length: int = 512\n    max_prompt_length: int = 384\n\n    # QLoRA\n    lora_r: int = 64\n    lora_alpha: int = 128\n    lora_dropout: float = 0.05\n    use_4bit: bool = True\n\n    # Training - optimized for T4 GPU at 90% utilization\n    per_device_batch_size: int = 4\n    gradient_accumulation_steps: int = 8   # effective batch = 32\n    learning_rate: float = 5e-5\n    num_epochs: int = 5\n    warmup_ratio: float = 0.1\n    weight_decay: float = 0.01\n    max_grad_norm: float = 1.0\n    beta: float = 0.1                      # DPO temperature parameter\n\n    # Precision\n    fp16: bool = False\n    bf16: bool = True                      # Better for DPO stability\n\n    # Data\n    val_ratio: float = 0.15\n    min_preference_strength: str = 'slight'  # Include both 'slight' and 'much'\n\n    # Logging\n    logging_steps: int = 5\n    eval_steps: int = 25\n    save_steps: int = 50\n\n    def __post_init__(self):\n        self.effective_batch = self.per_device_batch_size * self.gradient_accumulation_steps\n        os.makedirs(self.output_dir, exist_ok=True)\n\nCONFIG = DPOConfig_Custom()\n\n# Check if bf16 is supported, fallback to fp16\nif CONFIG.bf16 and not torch.cuda.is_bf16_supported():\n    logger.warning(\"bf16 not supported on this GPU, falling back to fp16\")\n    CONFIG.bf16 = False\n    CONFIG.fp16 = True\n\nlogger.info(\"Configuration:\")\nfor k, v in vars(CONFIG).items():\n    logger.info(f\"  {k}: {v}\")\n\ntracker.mark('Configuration', 'PASS')\nprint(\"\\nCELL 2 COMPLETE: Configuration set\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 3: Load & Validate Annotated Data\n# ============================================================================\n\nlogger.info(\"=\" * 60)\nlogger.info(\"LOADING ANNOTATED DATA\")\nlogger.info(\"=\" * 60)\n\n# Try multiple paths (Kaggle dataset or local)\npossible_paths = [\n    CONFIG.data_path,\n    '/kaggle/input/datasets/pushkarprabhath/gricebench-dpo-annotations/tier1_hard_pairs_FULLY_ANNOTATED.json',\n    '/kaggle/input/gricebench-dpo-annotations/tier1_hard_pairs_FULLY_ANNOTATED.json',\n    '/kaggle/input/gricebench-dpo/tier1_hard_pairs_FULLY_ANNOTATED.json',\n]\n\nraw_data = None\nfor path in possible_paths:\n    if os.path.exists(path):\n        logger.info(f\"Found data at: {path}\")\n        with open(path, 'r', encoding='utf-8') as f:\n            raw_data = json.load(f)\n        break\n\nif raw_data is None:\n    # List available files for debugging\n    logger.error(\"Data file not found! Available files:\")\n    for root_dir in ['/kaggle/input']:\n        if os.path.exists(root_dir):\n            for dirpath, dirnames, filenames in os.walk(root_dir):\n                for fn in filenames:\n                    if fn.endswith('.json'):\n                        logger.error(f\"  {os.path.join(dirpath, fn)}\")\n    raise FileNotFoundError(\"tier1_hard_pairs_FULLY_ANNOTATED.json not found in any expected location\")\n\nlogger.info(f\"Total records loaded: {len(raw_data)}\")\n\n# Validate structure\nrequired_keys = {'id', 'context', 'response_A', 'response_B', 'preference', 'reason', 'annotated'}\nsample = raw_data[0]\nmissing = required_keys - set(sample.keys())\nif missing:\n    raise ValueError(f\"Missing required keys: {missing}\")\n\n# Check all are annotated\nannotated_count = sum(1 for d in raw_data if d.get('annotated', False))\nlogger.info(f\"Annotated: {annotated_count}/{len(raw_data)}\")\nassert annotated_count == len(raw_data), f\"Not all records annotated! Only {annotated_count}/{len(raw_data)}\"\n\n# Preference distribution\npref_counts = Counter(d['preference'] for d in raw_data)\nlogger.info(\"\\nPreference Distribution:\")\nfor pref, count in pref_counts.most_common():\n    pct = 100 * count / len(raw_data)\n    logger.info(f\"  {pref}: {count} ({pct:.1f}%)\")\n\n# Reason analysis\nreason_counts = Counter(d.get('reason', '') for d in raw_data)\nlogger.info(f\"\\nUnique reasons: {len(reason_counts)}\")\nlogger.info(\"Top 5 reasons:\")\nfor reason, count in reason_counts.most_common(5):\n    logger.info(f\"  [{count}x] {reason[:80]}...\")\n\n# Maxim mentions in reasons\nmaxim_names = ['quantity', 'quality', 'relation', 'manner']\nmaxim_mentions = {m: 0 for m in maxim_names}\nfor d in raw_data:\n    reason_lower = d.get('reason', '').lower()\n    for m in maxim_names:\n        if m in reason_lower:\n            maxim_mentions[m] += 1\n\nlogger.info(\"\\nMaxim Mentions in Reasons:\")\nfor m, count in sorted(maxim_mentions.items(), key=lambda x: -x[1]):\n    logger.info(f\"  {m.capitalize()}: {count}\")\n\ntracker.mark('Data Loaded', 'PASS', {\n    'total': len(raw_data),\n    'annotated': annotated_count,\n    'preferences': dict(pref_counts)\n})\nprint(f\"\\nCELL 3 COMPLETE: {len(raw_data)} annotated pairs loaded\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 4: Convert to DPO Format (Chosen / Rejected)\n# ============================================================================\n\nlogger.info(\"=\" * 60)\nlogger.info(\"CONVERTING TO DPO FORMAT\")\nlogger.info(\"=\" * 60)\n\nPREFERENCE_MAP = {\n    'A_much': ('A', 'strong'),\n    'A_slight': ('A', 'weak'),\n    'B_much': ('B', 'strong'),\n    'B_slight': ('B', 'weak'),\n    'equal': (None, 'none'),\n}\n\ndpo_examples = []\nskipped = 0\nerrors = []\n\nfor item in raw_data:\n    try:\n        pref = item['preference']\n        winner, strength = PREFERENCE_MAP.get(pref, (None, 'none'))\n\n        if winner is None:\n            skipped += 1\n            continue\n\n        context = item['context'].strip()\n        resp_a = item['response_A'].strip()\n        resp_b = item['response_B'].strip()\n        reason = item.get('reason', '').strip()\n\n        # Skip if responses are too short or empty\n        if len(resp_a) < 10 or len(resp_b) < 10:\n            skipped += 1\n            continue\n\n        # Skip if responses are nearly identical\n        if resp_a[:50] == resp_b[:50] and pref != 'equal':\n            skipped += 1\n            continue\n\n        # Build prompt from context\n        prompt = f\"Continue the following conversation naturally, following Gricean maxims (be relevant, truthful, clear, and appropriately informative):\\n\\n{context}\\n\\nResponse:\"\n\n        # Determine chosen and rejected\n        if winner == 'A':\n            chosen = resp_a\n            rejected = resp_b\n        else:\n            chosen = resp_b\n            rejected = resp_a\n\n        dpo_examples.append({\n            'prompt': prompt,\n            'chosen': chosen,\n            'rejected': rejected,\n            'strength': strength,\n            'reason': reason,\n            'id': item['id'],\n        })\n\n    except Exception as e:\n        errors.append(f\"{item.get('id', '?')}: {str(e)}\")\n\nlogger.info(f\"\\nConversion Results:\")\nlogger.info(f\"  DPO pairs created: {len(dpo_examples)}\")\nlogger.info(f\"  Skipped (equal/invalid): {skipped}\")\nlogger.info(f\"  Errors: {len(errors)}\")\n\nif errors:\n    logger.warning(\"Sample errors:\")\n    for e in errors[:3]:\n        logger.warning(f\"  {e}\")\n\n# Strength distribution\nstrength_counts = Counter(ex['strength'] for ex in dpo_examples)\nlogger.info(f\"\\nStrength Distribution:\")\nfor s, c in strength_counts.most_common():\n    logger.info(f\"  {s}: {c}\")\n\n# Sample a DPO example\nif dpo_examples:\n    sample = dpo_examples[0]\n    logger.info(f\"\\nSample DPO Pair:\")\n    logger.info(f\"  Prompt: {sample['prompt'][:120]}...\")\n    logger.info(f\"  Chosen: {sample['chosen'][:80]}...\")\n    logger.info(f\"  Rejected: {sample['rejected'][:80]}...\")\n    logger.info(f\"  Reason: {sample['reason']}\")\n\ntracker.mark('DPO Conversion', 'PASS', {\n    'dpo_pairs': len(dpo_examples),\n    'skipped': skipped\n})\nprint(f\"\\nCELL 4 COMPLETE: {len(dpo_examples)} DPO pairs ready\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 5: Stratified Train/Val Split\n# ============================================================================\n\nlogger.info(\"=\" * 60)\nlogger.info(\"CREATING TRAIN/VAL SPLIT\")\nlogger.info(\"=\" * 60)\n\nrandom.seed(42)\n\n# Stratify by strength (strong vs weak preferences)\nstrong = [ex for ex in dpo_examples if ex['strength'] == 'strong']\nweak = [ex for ex in dpo_examples if ex['strength'] == 'weak']\n\nrandom.shuffle(strong)\nrandom.shuffle(weak)\n\n# Split each group\nval_strong_n = max(1, int(len(strong) * CONFIG.val_ratio))\nval_weak_n = max(1, int(len(weak) * CONFIG.val_ratio)) if weak else 0\n\nval_data = strong[:val_strong_n] + weak[:val_weak_n]\ntrain_data = strong[val_strong_n:] + weak[val_weak_n:]\n\nrandom.shuffle(train_data)\nrandom.shuffle(val_data)\n\nlogger.info(f\"Split Results:\")\nlogger.info(f\"  Train: {len(train_data)}\")\nlogger.info(f\"  Val: {len(val_data)}\")\nlogger.info(f\"  Train strong: {sum(1 for x in train_data if x['strength']=='strong')}\")\nlogger.info(f\"  Train weak: {sum(1 for x in train_data if x['strength']=='weak')}\")\nlogger.info(f\"  Val strong: {sum(1 for x in val_data if x['strength']=='strong')}\")\nlogger.info(f\"  Val weak: {sum(1 for x in val_data if x['strength']=='weak')}\")\n\n# Convert to HuggingFace Dataset format\ndef to_hf_dataset(examples):\n    return HFDataset.from_dict({\n        'prompt': [ex['prompt'] for ex in examples],\n        'chosen': [ex['chosen'] for ex in examples],\n        'rejected': [ex['rejected'] for ex in examples],\n    })\n\ntrain_dataset = to_hf_dataset(train_data)\nval_dataset = to_hf_dataset(val_data)\n\nlogger.info(f\"\\nHF Dataset columns: {train_dataset.column_names}\")\nlogger.info(f\"Train dataset size: {len(train_dataset)}\")\nlogger.info(f\"Val dataset size: {len(val_dataset)}\")\n\ntracker.mark('Train/Val Split', 'PASS', {\n    'train': len(train_data),\n    'val': len(val_data)\n})\nprint(f\"\\nCELL 5 COMPLETE: Train={len(train_data)}, Val={len(val_data)}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 6: Load Model & Tokenizer with QLoRA (4-bit)\n# ============================================================================\n\nlogger.info(\"=\" * 60)\nlogger.info(\"LOADING MODEL WITH QLoRA\")\nlogger.info(\"=\" * 60)\n\ncell_start = time.time()\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16 if CONFIG.bf16 else torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load tokenizer\nlogger.info(f\"Loading tokenizer: {CONFIG.model_name}\")\ntokenizer = AutoTokenizer.from_pretrained(\n    CONFIG.model_name,\n    trust_remote_code=True,\n    padding_side='left',\n)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nlogger.info(f\"Vocab size: {tokenizer.vocab_size}\")\nlogger.info(f\"Pad token: '{tokenizer.pad_token}' (id={tokenizer.pad_token_id})\")\n\n# Load model in 4-bit\nlogger.info(f\"\\nLoading model: {CONFIG.model_name} (4-bit quantized)\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    CONFIG.model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    torch_dtype=torch.bfloat16 if CONFIG.bf16 else torch.float16,\n    attn_implementation=\"eager\",  # Ensure compatibility\n)\n\n# Prepare for k-bit training\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n\n# LoRA config - target all linear layers for maximum expressivity\nlora_config = LoraConfig(\n    r=CONFIG.lora_r,\n    lora_alpha=CONFIG.lora_alpha,\n    lora_dropout=CONFIG.lora_dropout,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n    target_modules=\"all-linear\",\n)\n\n# Apply LoRA\nmodel = get_peft_model(model, lora_config)\n\n# Model stats\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrain_pct = 100.0 * trainable_params / total_params\n\nlogger.info(f\"\\nModel Parameters:\")\nlogger.info(f\"  Total: {total_params:,}\")\nlogger.info(f\"  Trainable (LoRA): {trainable_params:,}\")\nlogger.info(f\"  Trainable %: {train_pct:.2f}%\")\n\n# VRAM usage\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    reserved = torch.cuda.memory_reserved(0) / 1e9\n    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    logger.info(f\"\\nVRAM Usage (after model load):\")\n    logger.info(f\"  Allocated: {allocated:.2f} GB\")\n    logger.info(f\"  Reserved: {reserved:.2f} GB\")\n    logger.info(f\"  Total: {total_vram:.1f} GB\")\n    logger.info(f\"  Utilization: {100*allocated/total_vram:.1f}%\")\n\ntracker.mark('Model Loaded', 'PASS', {\n    'total_params': f\"{total_params:,}\",\n    'trainable_params': f\"{trainable_params:,}\",\n    'trainable_pct': f\"{train_pct:.2f}%\",\n    'vram_gb': f\"{allocated:.2f}\",\n})\n\nload_time = time.time() - cell_start\nprint(f\"\\nCELL 6 COMPLETE ({load_time:.1f}s): Model loaded with QLoRA\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 7: DPO Training Setup\n# ============================================================================\n\nlogger.info(\"=\" * 60)\nlogger.info(\"SETTING UP DPO TRAINER\")\nlogger.info(\"=\" * 60)\n\n# Calculate training steps\nsteps_per_epoch = len(train_dataset) // (CONFIG.per_device_batch_size * CONFIG.gradient_accumulation_steps)\ntotal_steps = steps_per_epoch * CONFIG.num_epochs\nwarmup_steps = int(total_steps * CONFIG.warmup_ratio)\n\nlogger.info(f\"Training Plan:\")\nlogger.info(f\"  Train examples: {len(train_dataset)}\")\nlogger.info(f\"  Batch size: {CONFIG.per_device_batch_size}\")\nlogger.info(f\"  Gradient accumulation: {CONFIG.gradient_accumulation_steps}\")\nlogger.info(f\"  Effective batch: {CONFIG.effective_batch}\")\nlogger.info(f\"  Steps per epoch: {steps_per_epoch}\")\nlogger.info(f\"  Total steps: {total_steps}\")\nlogger.info(f\"  Warmup steps: {warmup_steps}\")\nlogger.info(f\"  DPO beta: {CONFIG.beta}\")\n\n# DPO Training Arguments\ntraining_args = DPOConfig(\n    output_dir=CONFIG.output_dir,\n\n    # Batch & accumulation\n    per_device_train_batch_size=CONFIG.per_device_batch_size,\n    per_device_eval_batch_size=CONFIG.per_device_batch_size,\n    gradient_accumulation_steps=CONFIG.gradient_accumulation_steps,\n\n    # Learning rate\n    learning_rate=CONFIG.learning_rate,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=warmup_steps,\n    weight_decay=CONFIG.weight_decay,\n    max_grad_norm=CONFIG.max_grad_norm,\n\n    # Epochs\n    num_train_epochs=CONFIG.num_epochs,\n\n    # Precision\n    fp16=CONFIG.fp16,\n    bf16=CONFIG.bf16,\n\n    # DPO specific\n    beta=CONFIG.beta,\n    max_length=CONFIG.max_length,\n    max_prompt_length=CONFIG.max_prompt_length,\n    loss_type=\"sigmoid\",  # Standard DPO loss\n\n    # Logging & saving\n    logging_steps=CONFIG.logging_steps,\n    eval_strategy=\"steps\",\n    eval_steps=CONFIG.eval_steps,\n    save_strategy=\"steps\",\n    save_steps=CONFIG.save_steps,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n\n    # Performance\n    gradient_checkpointing=True,\n    dataloader_num_workers=2,\n    dataloader_pin_memory=True,\n    optim=\"paged_adamw_8bit\",      # Memory-efficient optimizer\n    remove_unused_columns=False,\n\n    # Reporting\n    report_to=\"none\",\n    run_name=\"gricebench_dpo_phase5\",\n)\n\n# Disable tokenizer parallelism warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Create DPO Trainer\ndpo_trainer = DPOTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    processing_class=tokenizer,\n)\n\nlogger.info(\"\\nDPO Trainer created successfully\")\n\n# VRAM after trainer setup\nif torch.cuda.is_available():\n    allocated = torch.cuda.memory_allocated(0) / 1e9\n    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    logger.info(f\"VRAM after trainer setup: {allocated:.2f} GB ({100*allocated/total_vram:.1f}%)\")\n\ntracker.mark('DPO Trainer Setup', 'PASS', {\n    'total_steps': total_steps,\n    'effective_batch': CONFIG.effective_batch,\n})\nprint(\"\\nCELL 7 COMPLETE: DPO Trainer ready\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 8: DPO Training Loop\n# ============================================================================\n\nlogger.info(\"=\" * 60)\nlogger.info(\"STARTING DPO TRAINING\")\nlogger.info(\"=\" * 60)\n\ntrain_start = time.time()\n\n# GPU monitoring callback\nclass GPUMonitor:\n    def __init__(self):\n        self.peak_util = 0\n        self.readings = []\n\n    def log(self):\n        if torch.cuda.is_available():\n            allocated = torch.cuda.memory_allocated(0) / 1e9\n            total = torch.cuda.get_device_properties(0).total_memory / 1e9\n            util = 100 * allocated / total\n            self.readings.append(util)\n            self.peak_util = max(self.peak_util, util)\n            return util\n        return 0\n\ngpu_monitor = GPUMonitor()\n\n# Pre-training eval\nlogger.info(\"\\nPre-training evaluation...\")\ntry:\n    pre_eval = dpo_trainer.evaluate()\n    logger.info(f\"Pre-training eval loss: {pre_eval.get('eval_loss', 'N/A')}\")\nexcept Exception as e:\n    logger.warning(f\"Pre-training eval failed (normal for first run): {e}\")\n\n# Train!\nlogger.info(\"\\n\" + \"=\" * 60)\nlogger.info(\"TRAINING IN PROGRESS...\")\nlogger.info(\"=\" * 60)\n\ntrain_result = dpo_trainer.train()\n\ntrain_time = time.time() - train_start\n\n# Log GPU utilization\ngpu_util = gpu_monitor.log()\nlogger.info(f\"\\nGPU VRAM utilization: {gpu_util:.1f}%\")\n\n# Training metrics\nlogger.info(f\"\\n{'='*60}\")\nlogger.info(f\"TRAINING COMPLETE\")\nlogger.info(f\"{'='*60}\")\nlogger.info(f\"Total training time: {train_time:.1f}s ({train_time/60:.1f}m)\")\nlogger.info(f\"Train loss: {train_result.training_loss:.4f}\")\n\n# Log training metrics\nmetrics = train_result.metrics\nfor key, value in metrics.items():\n    logger.info(f\"  {key}: {value}\")\n\n# VRAM peak\nif torch.cuda.is_available():\n    peak_mem = torch.cuda.max_memory_allocated(0) / 1e9\n    total_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n    peak_pct = 100 * peak_mem / total_vram\n    logger.info(f\"\\nPeak VRAM: {peak_mem:.2f} GB ({peak_pct:.1f}%)\")\n\ntracker.mark('DPO Training', 'PASS', {\n    'train_loss': f\"{train_result.training_loss:.4f}\",\n    'time': f\"{train_time:.1f}s\",\n    'peak_vram_pct': f\"{peak_pct:.1f}%\"\n})\nprint(f\"\\nCELL 8 COMPLETE: Training done in {train_time:.1f}s, loss={train_result.training_loss:.4f}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 9: Post-Training Evaluation\n# ============================================================================\n\nlogger.info(\"=\" * 60)\nlogger.info(\"POST-TRAINING EVALUATION\")\nlogger.info(\"=\" * 60)\n\n# Evaluate on validation set\neval_results = dpo_trainer.evaluate()\n\nlogger.info(\"\\nValidation Metrics:\")\nfor key, value in eval_results.items():\n    logger.info(f\"  {key}: {value}\")\n\n# Generate sample responses to compare\nlogger.info(\"\\n\" + \"=\" * 60)\nlogger.info(\"SAMPLE GENERATION COMPARISON\")\nlogger.info(\"=\" * 60)\n\nmodel.eval()\n\n# Take 5 samples from validation set\nn_samples = min(5, len(val_data))\nsample_indices = random.sample(range(len(val_data)), n_samples)\n\ngeneration_results = []\n\nfor idx in sample_indices:\n    sample = val_data[idx]\n    prompt = sample['prompt']\n\n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=CONFIG.max_prompt_length)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            do_sample=True,\n            temperature=0.7,\n            top_p=0.9,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n\n    generated = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n\n    result = {\n        'prompt_snippet': prompt[:100],\n        'generated': generated[:200],\n        'original_chosen': sample['chosen'][:200],\n        'original_rejected': sample['rejected'][:200],\n        'reason': sample['reason'],\n    }\n    generation_results.append(result)\n\n    logger.info(f\"\\n--- Sample {idx} ---\")\n    logger.info(f\"Context: {prompt[:100]}...\")\n    logger.info(f\"Generated: {generated[:150]}...\")\n    logger.info(f\"Chosen was: {sample['chosen'][:100]}...\")\n    logger.info(f\"Rejected was: {sample['rejected'][:100]}...\")\n    logger.info(f\"Reason: {sample['reason']}\")\n\ntracker.mark('Post-Training Eval', 'PASS', {\n    'eval_loss': f\"{eval_results.get('eval_loss', 'N/A')}\",\n    'samples_generated': n_samples,\n})\nprint(f\"\\nCELL 9 COMPLETE: Evaluation done, eval_loss={eval_results.get('eval_loss', 'N/A')}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 10: Compute Preference Accuracy (Reward Margin)\n# ============================================================================\n\nlogger.info(\"=\" * 60)\nlogger.info(\"COMPUTING PREFERENCE ACCURACY\")\nlogger.info(\"=\" * 60)\n\n# For each validation pair, check if model assigns higher probability to chosen vs rejected\ncorrect = 0\ntotal = 0\nmargins = []\n\nmodel.eval()\n\nfor i, sample in enumerate(val_data):\n    try:\n        prompt = sample['prompt']\n        chosen = sample['chosen']\n        rejected = sample['rejected']\n\n        # Tokenize prompt + chosen\n        chosen_input = tokenizer(\n            prompt + chosen,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=CONFIG.max_length,\n        )\n        chosen_input = {k: v.to(device) for k, v in chosen_input.items()}\n\n        # Tokenize prompt + rejected\n        rejected_input = tokenizer(\n            prompt + rejected,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=CONFIG.max_length,\n        )\n        rejected_input = {k: v.to(device) for k, v in rejected_input.items()}\n\n        with torch.no_grad():\n            chosen_out = model(**chosen_input)\n            rejected_out = model(**rejected_input)\n\n            # Get mean log probability (normalized by length)\n            chosen_logprob = -chosen_out.loss.item() if hasattr(chosen_out, 'loss') and chosen_out.loss is not None else 0\n            rejected_logprob = -rejected_out.loss.item() if hasattr(rejected_out, 'loss') and rejected_out.loss is not None else 0\n\n            # Model prefers chosen if its loss is lower (higher log prob)\n            chosen_loss = chosen_out.loss.item() if chosen_out.loss is not None else float('inf')\n            rejected_loss = rejected_out.loss.item() if rejected_out.loss is not None else float('inf')\n\n            margin = rejected_loss - chosen_loss  # positive = model prefers chosen\n            margins.append(margin)\n\n            if chosen_loss < rejected_loss:\n                correct += 1\n            total += 1\n\n    except Exception as e:\n        logger.debug(f\"Sample {i} error: {e}\")\n        continue\n\n    if (i + 1) % 10 == 0:\n        logger.info(f\"  Processed {i+1}/{len(val_data)} (accuracy so far: {100*correct/total:.1f}%)\")\n\naccuracy = 100 * correct / total if total > 0 else 0\navg_margin = np.mean(margins) if margins else 0\n\nlogger.info(f\"\\nPreference Accuracy Results:\")\nlogger.info(f\"  Correct: {correct}/{total}\")\nlogger.info(f\"  Accuracy: {accuracy:.1f}%\")\nlogger.info(f\"  Average margin: {avg_margin:.4f}\")\nlogger.info(f\"  Margin std: {np.std(margins):.4f}\" if margins else \"  No margins\")\n\n# Interpret results\nif accuracy > 70:\n    logger.info(\"\\n  ‚úÖ STRONG: Model clearly prefers chosen responses\")\nelif accuracy > 55:\n    logger.info(\"\\n  ‚ö†Ô∏è MODERATE: Model shows some preference learning\")\nelif accuracy > 50:\n    logger.info(\"\\n  ‚ö†Ô∏è WEAK: Model barely distinguishes preferences\")\nelse:\n    logger.info(\"\\n  ‚ùå FAILED: Model does not prefer chosen responses\")\n\ntracker.mark('Preference Accuracy', 'PASS' if accuracy > 55 else 'WARN', {\n    'accuracy': f\"{accuracy:.1f}%\",\n    'avg_margin': f\"{avg_margin:.4f}\",\n})\nprint(f\"\\nCELL 10 COMPLETE: Preference accuracy = {accuracy:.1f}%\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 11: Save Model, Adapter, and Results\n# ============================================================================\n\nlogger.info(\"=\" * 60)\nlogger.info(\"SAVING OUTPUTS\")\nlogger.info(\"=\" * 60)\n\n# Save LoRA adapter (small, easy to download)\nadapter_path = f\"{CONFIG.output_dir}/dpo_adapter\"\nmodel.save_pretrained(adapter_path)\ntokenizer.save_pretrained(adapter_path)\nlogger.info(f\"LoRA adapter saved to: {adapter_path}\")\n\n# Calculate adapter size\nadapter_size = 0\nfor root, dirs, files in os.walk(adapter_path):\n    for f in files:\n        adapter_size += os.path.getsize(os.path.join(root, f))\nlogger.info(f\"Adapter size: {adapter_size/1e6:.1f} MB\")\n\n# Save comprehensive results\nresults = {\n    'phase': 'Phase 5 - DPO Training',\n    'timestamp': datetime.now().isoformat(),\n    'model': CONFIG.model_name,\n    'method': 'DPO with QLoRA (4-bit)',\n    'data': {\n        'total_annotated': len(raw_data),\n        'dpo_pairs': len(dpo_examples),\n        'train_size': len(train_data),\n        'val_size': len(val_data),\n        'preference_distribution': dict(pref_counts),\n    },\n    'hyperparameters': {\n        'lora_r': CONFIG.lora_r,\n        'lora_alpha': CONFIG.lora_alpha,\n        'learning_rate': CONFIG.learning_rate,\n        'effective_batch_size': CONFIG.effective_batch,\n        'num_epochs': CONFIG.num_epochs,\n        'beta': CONFIG.beta,\n        'max_length': CONFIG.max_length,\n    },\n    'training': {\n        'train_loss': float(train_result.training_loss),\n        'training_time_seconds': float(train_time),\n    },\n    'evaluation': {\n        'eval_loss': float(eval_results.get('eval_loss', 0)),\n        'preference_accuracy': float(accuracy),\n        'avg_margin': float(avg_margin),\n    },\n    'generation_samples': generation_results,\n    'gpu': {\n        'name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A',\n        'peak_vram_gb': float(torch.cuda.max_memory_allocated(0) / 1e9) if torch.cuda.is_available() else 0,\n        'total_vram_gb': float(torch.cuda.get_device_properties(0).total_memory / 1e9) if torch.cuda.is_available() else 0,\n    },\n}\n\nresults_path = f\"{CONFIG.output_dir}/dpo_results.json\"\nwith open(results_path, 'w') as f:\n    json.dump(results, f, indent=2, default=str)\nlogger.info(f\"Results saved to: {results_path}\")\n\n# Also save to /kaggle/working for easy download\nimport shutil\nshutil.copy2(results_path, '/kaggle/working/dpo_results.json')\n\n# Save training history\nhistory_path = f\"{CONFIG.output_dir}/training_history.json\"\nif hasattr(dpo_trainer, 'state') and dpo_trainer.state.log_history:\n    with open(history_path, 'w') as f:\n        json.dump(dpo_trainer.state.log_history, f, indent=2, default=str)\n    shutil.copy2(history_path, '/kaggle/working/training_history.json')\n    logger.info(f\"Training history saved: {len(dpo_trainer.state.log_history)} entries\")\n\ntracker.mark('Outputs Saved', 'PASS', {\n    'adapter_size_mb': f\"{adapter_size/1e6:.1f}\",\n    'results_path': results_path,\n})\nprint(f\"\\nCELL 11 COMPLETE: All outputs saved\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# ============================================================================\n# CELL 12: Final Summary\n# ============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"PHASE 5: DPO TRAINING COMPLETE\")\nprint(\"=\" * 70)\n\n# Checkpoint summary\ntracker.summary()\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"RESULTS SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"\\nüìä Data:\")\nprint(f\"  Annotated pairs: {len(raw_data)}\")\nprint(f\"  DPO training pairs: {len(dpo_examples)}\")\nprint(f\"  Train / Val: {len(train_data)} / {len(val_data)}\")\n\nprint(f\"\\nüèãÔ∏è Training:\")\nprint(f\"  Model: {CONFIG.model_name}\")\nprint(f\"  Method: QLoRA (r={CONFIG.lora_r}, alpha={CONFIG.lora_alpha})\")\nprint(f\"  Final train loss: {train_result.training_loss:.4f}\")\nprint(f\"  Training time: {train_time:.1f}s ({train_time/60:.1f} min)\")\n\nprint(f\"\\nüìà Evaluation:\")\nprint(f\"  Eval loss: {eval_results.get('eval_loss', 'N/A')}\")\nprint(f\"  Preference accuracy: {accuracy:.1f}%\")\nprint(f\"  Average margin: {avg_margin:.4f}\")\n\nif torch.cuda.is_available():\n    peak = torch.cuda.max_memory_allocated(0) / 1e9\n    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"\\nüñ•Ô∏è GPU:\")\n    print(f\"  Peak VRAM: {peak:.2f} / {total:.1f} GB ({100*peak/total:.1f}%)\")\n\nprint(f\"\\nüìÅ Output Files:\")\nprint(f\"  /kaggle/working/dpo_results.json\")\nprint(f\"  /kaggle/working/training_history.json\")\nprint(f\"  {CONFIG.output_dir}/dpo_adapter/\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"‚úÖ DOWNLOAD: /kaggle/working/dpo_results.json\")\nprint(\"‚úÖ DOWNLOAD: /kaggle/working/training_history.json\")\nprint(\"=\" * 70)\n\n# Cleanup\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}