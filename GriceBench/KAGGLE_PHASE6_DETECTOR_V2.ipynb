{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 0,
     "sourceType": "datasetVersion",
     "datasetSlug": "gricebench-scientific-fix"
    }
   ],
   "isInternetEnabled": true,
   "isGpuEnabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phase 6: Detector V2 ‚Äî Verified Natural Violations Training\n\n**Goal:** Train a Gricean maxim violation detector on **real** Phase 4 natural violations (not synthetic).\n\n**Key Features:**\n- Hard assertions on data sources (will crash if Phase 4 data missing)\n- Source distribution proof logged in results\n- Held-out test set (500 examples never in training)  \n- Per-generation-method breakdown (injector / mined / adversarial)\n- Error analysis on worst misclassifications\n\n**Model:** DeBERTa-v3-small (multi-label: Quantity, Quality, Relation, Manner)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 1: ENVIRONMENT SETUP\n# ============================================================================\nimport subprocess, sys\n\nprint(\"Installing dependencies...\")\nsubprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n    'transformers>=4.35.0', 'accelerate>=0.21.0', 'datasets>=2.14.0',\n    'scikit-learn>=1.3.0', 'scipy>=1.11.0'])\nprint(\"Dependencies installed.\")\n\nimport torch\nimport torch.nn as nn\nimport os\nimport gc\nimport json\nimport random\nimport logging\nimport numpy as np\nfrom datetime import datetime\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nfrom collections import Counter, defaultdict\nfrom typing import List, Dict, Tuple, Optional\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# Logging\nlogging.basicConfig(level=logging.INFO, format='%(levelname)s | %(message)s')\nlogger = logging.getLogger('Phase6DetectorV2')\n\n# GPU Check\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nlogger.info(f\"Device: {device}\")\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n    logger.info(f\"GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\nelse:\n    raise RuntimeError(\"GPU required\")\n\n# Progress tracker\nclass Tracker:\n    def __init__(self):\n        self.steps = []\n        self.start = datetime.now()\n    def mark(self, name, status, details=None):\n        elapsed = (datetime.now() - self.start).total_seconds()\n        self.steps.append({'name': name, 'status': status, 'elapsed': elapsed, 'details': details or {}})\n        icon = '‚úÖ' if status == 'PASS' else '‚ùå' if status == 'FAIL' else '‚è≥'\n        logger.info(f\"{icon} [{elapsed:.0f}s] {name}: {status}\")\n\ntracker = Tracker()\ntracker.mark('Environment', 'PASS')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 2: CONFIGURATION\n# ============================================================================\n@dataclass\nclass Config:\n    # Data\n    data_dir: str = '/kaggle/input/gricebench-scientific-fix'\n    output_dir: str = '/kaggle/working/detector_v2'\n    \n    # Model\n    model_name: str = 'microsoft/deberta-v3-small'\n    num_labels: int = 4\n    max_length: int = 512\n    \n    # Training\n    learning_rate: float = 2e-5\n    num_epochs: int = 6\n    batch_size: int = 16\n    warmup_ratio: float = 0.1\n    weight_decay: float = 0.01\n    \n    # Splits\n    train_ratio: float = 0.70\n    val_ratio: float = 0.15\n    test_ratio: float = 0.15\n    \n    # Verification\n    min_phase4_violations: int = 1000  # HARD MINIMUM\n    \n    seed: int = 42\n\nCONFIG = Config()\nos.makedirs(CONFIG.output_dir, exist_ok=True)\n\nlogger.info(f\"Model: {CONFIG.model_name}\")\nlogger.info(f\"Data: {CONFIG.data_dir}\")\nlogger.info(f\"Min Phase 4 violations: {CONFIG.min_phase4_violations}\")\n\ntracker.mark('Configuration', 'PASS')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 3: DATA STRUCTURES\n# ============================================================================\n@dataclass\nclass Example:\n    text: str\n    labels: List[int]   # [quantity, quality, relation, manner]\n    source: str          # 'phase4_violation' or 'phase4_clean'\n    example_id: str = ''\n    generation_method: str = 'unknown'\n    violation_type: str = 'unknown'\n    maxim: str = 'unknown'\n    \n    def __post_init__(self):\n        assert len(self.labels) == 4, f\"Labels must have 4 elements, got {len(self.labels)}\"\n        assert self.source in ['phase4_violation', 'phase4_clean'], \\\n            f\"Source must be 'phase4_violation' or 'phase4_clean', got '{self.source}'\"\n\ndef normalize_text(text):\n    if not text:\n        return ''\n    text = str(text).strip()\n    text = ' '.join(text.split())\n    return text\n\nMAXIM_NAMES = ['Quantity', 'Quality', 'Relation', 'Manner']\n\ntracker.mark('Data Structures', 'PASS')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 4: LOAD & VERIFY PHASE 4 DATA (CRITICAL)\n# ============================================================================\nlogger.info(\"=\" * 60)\nlogger.info(\"üî¥ CRITICAL: LOADING & VERIFYING PHASE 4 DATA\")\nlogger.info(\"=\" * 60)\n\n# ---- Find the data file ----\npossible_paths = [\n    f\"{CONFIG.data_dir}/natural_violations.json\",\n    '/kaggle/input/gricebench-phase4/natural_violations.json',\n    '/kaggle/input/datasets/pushkarprabhath/gricebench-scientific-fix/natural_violations.json',\n    '/kaggle/input/gricebench-scientific-fix/natural_violations.json',\n]\n\nphase4_path = None\nfor path in possible_paths:\n    if os.path.exists(path):\n        phase4_path = path\n        break\n\nif phase4_path is None:\n    # List what's actually available\n    logger.error(\"‚ùå CRITICAL: natural_violations.json NOT FOUND!\")\n    logger.error(\"Available files in /kaggle/input:\")\n    for root, dirs, files in os.walk('/kaggle/input'):\n        for fn in files:\n            logger.error(f\"  {os.path.join(root, fn)}\")\n    raise FileNotFoundError(\n        \"natural_violations.json not found! \"\n        \"Upload Phase 4 output to your Kaggle dataset.\"\n    )\n\nlogger.info(f\"‚úÖ Found data: {phase4_path}\")\nfile_size = os.path.getsize(phase4_path) / 1024\nlogger.info(f\"File size: {file_size:.1f} KB\")\n\n# ---- Load raw data ----\nwith open(phase4_path, 'r', encoding='utf-8') as f:\n    raw_data = json.load(f)\n\nlogger.info(f\"Raw records: {len(raw_data)}\")\nlogger.info(f\"Sample keys: {list(raw_data[0].keys())}\")\n\n# ---- Process into Examples ----\nviolations = []\nclean_examples = []\nerrors = []\n\ngeneration_method_counts = Counter()\nviolation_type_counts = Counter()\n\nfor idx, item in enumerate(raw_data):\n    try:\n        context = normalize_text(item.get('context', ''))\n        gen_method = item.get('generation_method', 'unknown')\n        viol_type = item.get('violation_type', 'unknown')\n        maxim = item.get('maxim', 'unknown')\n        \n        # VIOLATION: violated_response\n        violated_response = normalize_text(item.get('violated_response', ''))\n        if violated_response:\n            text = f\"{context} [SEP] {violated_response}\" if context else violated_response\n            \n            labels_dict = item.get('labels', {})\n            if isinstance(labels_dict, dict):\n                labels = [\n                    int(labels_dict.get('quantity', 0)),\n                    int(labels_dict.get('quality', 0)),\n                    int(labels_dict.get('relation', 0)),\n                    int(labels_dict.get('manner', 0))\n                ]\n            else:\n                maxim_lower = str(maxim).lower()\n                labels = [\n                    1 if 'quantity' in maxim_lower else 0,\n                    1 if 'quality' in maxim_lower else 0,\n                    1 if 'relation' in maxim_lower else 0,\n                    1 if 'manner' in maxim_lower else 0\n                ]\n            \n            if sum(labels) > 0 and len(text) > 50:\n                violations.append(Example(\n                    text=text,\n                    labels=labels,\n                    source='phase4_violation',\n                    example_id=str(item.get('id', f'v_{idx}')),\n                    generation_method=gen_method,\n                    violation_type=viol_type,\n                    maxim=maxim,\n                ))\n                generation_method_counts[gen_method] += 1\n                violation_type_counts[viol_type] += 1\n        \n        # CLEAN: original_response\n        original_response = normalize_text(item.get('original_response', ''))\n        if original_response:\n            text = f\"{context} [SEP] {original_response}\" if context else original_response\n            if len(text) > 50:\n                clean_examples.append(Example(\n                    text=text,\n                    labels=[0, 0, 0, 0],\n                    source='phase4_clean',\n                    example_id=f\"{item.get('id', idx)}_clean\",\n                    generation_method='clean',\n                    violation_type='none',\n                    maxim='none',\n                ))\n    except Exception as e:\n        errors.append(f\"Item {idx}: {str(e)}\")\n\n# ---- MANDATORY ASSERTIONS ----\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üî¥ MANDATORY DATA VERIFICATION\")\nprint(\"=\" * 60)\n\nprint(f\"\\nüìä Data Loaded:\")\nprint(f\"  Phase 4 violations: {len(violations)}\")\nprint(f\"  Phase 4 clean:      {len(clean_examples)}\")\nprint(f\"  Errors:             {len(errors)}\")\n\n# ASSERTION 1: Must have enough violations\nassert len(violations) >= CONFIG.min_phase4_violations, \\\n    f\"‚ùå CRITICAL FAILURE: Only {len(violations)} violations loaded \" \\\n    f\"(need >= {CONFIG.min_phase4_violations}). \" \\\n    f\"Phase 4 data NOT loaded correctly!\"\n\nprint(f\"\\n‚úÖ ASSERTION 1 PASSED: {len(violations)} violations >= {CONFIG.min_phase4_violations} minimum\")\n\n# ASSERTION 2: Must have all 4 maxims represented\nmaxim_counts = Counter()\nfor ex in violations:\n    for i, name in enumerate(MAXIM_NAMES):\n        if ex.labels[i] == 1:\n            maxim_counts[name] += 1\n\nprint(f\"\\nüìä Maxim Distribution:\")\nfor name in MAXIM_NAMES:\n    count = maxim_counts.get(name, 0)\n    print(f\"  {name}: {count}\")\n    assert count >= 100, f\"‚ùå CRITICAL: {name} has only {count} violations (need >= 100)\"\n\nprint(f\"\\n‚úÖ ASSERTION 2 PASSED: All maxims have >= 100 violations\")\n\n# ASSERTION 3: Must have natural generation methods\nprint(f\"\\nüìä Generation Methods:\")\nfor method, count in generation_method_counts.most_common():\n    print(f\"  {method}: {count} ({100*count/len(violations):.1f}%)\")\n\nprint(f\"\\nüìä Violation Types (top 10):\")\nfor vtype, count in violation_type_counts.most_common(10):\n    print(f\"  {vtype}: {count}\")\n\n# ASSERTION 4: Must have clean examples\nassert len(clean_examples) >= 500, \\\n    f\"‚ùå CRITICAL: Only {len(clean_examples)} clean examples (need >= 500)\"\n\nprint(f\"\\n‚úÖ ASSERTION 3 PASSED: {len(clean_examples)} clean examples >= 500\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úÖ ALL DATA ASSERTIONS PASSED ‚Äî Phase 4 data confirmed!\")\nprint(\"=\" * 60)\n\ntracker.mark('Data Verification', 'PASS', {\n    'violations': len(violations),\n    'clean': len(clean_examples),\n    'generation_methods': dict(generation_method_counts),\n    'maxim_distribution': dict(maxim_counts),\n})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 5: STRATIFIED SPLIT WITH LEAKAGE CHECK\n# ============================================================================\nlogger.info(\"=\" * 60)\nlogger.info(\"CREATING STRATIFIED SPLITS\")\nlogger.info(\"=\" * 60)\n\n# Combine all data\nall_data = violations + clean_examples\nrandom.shuffle(all_data)\n\nlogger.info(f\"Total examples: {len(all_data)}\")\n\n# Stratified split by (source, maxim) to ensure representation\ngroups = defaultdict(list)\nfor ex in all_data:\n    key = (ex.source, ex.maxim)\n    groups[key].append(ex)\n\nlogger.info(f\"Unique (source, maxim) groups: {len(groups)}\")\n\ntrain_data, val_data, test_data = [], [], []\n\nfor key, examples in groups.items():\n    random.shuffle(examples)\n    n = len(examples)\n    n_train = max(1, int(n * CONFIG.train_ratio))\n    n_val = max(1, int(n * CONFIG.val_ratio))\n    n_test = n - n_train - n_val\n    \n    if n_test < 1:\n        n_test = 1\n        n_train = n - n_val - n_test\n    \n    train_data.extend(examples[:n_train])\n    val_data.extend(examples[n_train:n_train + n_val])\n    test_data.extend(examples[n_train + n_val:])\n\nrandom.shuffle(train_data)\nrandom.shuffle(val_data)\nrandom.shuffle(test_data)\n\nlogger.info(f\"\\nSplit sizes:\")\nlogger.info(f\"  Train: {len(train_data)}\")\nlogger.info(f\"  Val:   {len(val_data)}\")\nlogger.info(f\"  Test:  {len(test_data)}\")\n\n# ---- Source distribution per split ----\ndef source_dist(data):\n    counts = Counter(ex.source for ex in data)\n    return dict(counts)\n\ntrain_sources = source_dist(train_data)\nval_sources = source_dist(val_data)\ntest_sources = source_dist(test_data)\n\nprint(\"\\nüìä Source Distribution:\")\nprint(f\"  Train: {train_sources}\")\nprint(f\"  Val:   {val_sources}\")\nprint(f\"  Test:  {test_sources}\")\n\n# ---- LEAKAGE CHECK ----\ntrain_texts = {ex.text for ex in train_data}\nval_texts = {ex.text for ex in val_data}\ntest_texts = {ex.text for ex in test_data}\n\ntrain_val_overlap = len(train_texts & val_texts)\ntrain_test_overlap = len(train_texts & test_texts)\nval_test_overlap = len(val_texts & test_texts)\n\nprint(f\"\\nüîç Leakage Check:\")\nprint(f\"  Train-Val overlap:  {train_val_overlap}\")\nprint(f\"  Train-Test overlap: {train_test_overlap}\")\nprint(f\"  Val-Test overlap:   {val_test_overlap}\")\n\nassert train_test_overlap == 0, f\"‚ùå DATA LEAKAGE: {train_test_overlap} examples in both train and test!\"\nassert train_val_overlap == 0, f\"‚ùå DATA LEAKAGE: {train_val_overlap} examples in both train and val!\"\n\nprint(\"‚úÖ No data leakage detected!\")\n\n# ---- Generation method distribution in test set ----\ntest_gen_methods = Counter(ex.generation_method for ex in test_data if ex.source == 'phase4_violation')\nprint(f\"\\nTest set generation methods:\")\nfor method, count in test_gen_methods.most_common():\n    print(f\"  {method}: {count}\")\n\ntracker.mark('Data Split', 'PASS', {\n    'train': len(train_data),\n    'val': len(val_data),\n    'test': len(test_data),\n    'train_sources': train_sources,\n    'test_sources': test_sources,\n    'leakage': {'train_test': train_test_overlap, 'train_val': train_val_overlap},\n})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 6: DATASET & MODEL\n# ============================================================================\nlogger.info(\"=\" * 60)\nlogger.info(\"CREATING DATASET & LOADING MODEL\")\nlogger.info(\"=\" * 60)\n\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader\n\n# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(CONFIG.model_name)\n\nclass GriceDataset(Dataset):\n    def __init__(self, examples, tokenizer, max_length):\n        self.examples = examples\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        ex = self.examples[idx]\n        encoding = self.tokenizer(\n            ex.text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encoding['input_ids'].squeeze(0),\n            'attention_mask': encoding['attention_mask'].squeeze(0),\n            'labels': torch.tensor(ex.labels, dtype=torch.float),\n        }\n\n# Create datasets\ntrain_dataset = GriceDataset(train_data, tokenizer, CONFIG.max_length)\nval_dataset = GriceDataset(val_data, tokenizer, CONFIG.max_length)\ntest_dataset = GriceDataset(test_data, tokenizer, CONFIG.max_length)\n\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG.batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=CONFIG.batch_size * 2, shuffle=False, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG.batch_size * 2, shuffle=False, num_workers=2, pin_memory=True)\n\nlogger.info(f\"Train batches: {len(train_loader)}\")\nlogger.info(f\"Val batches:   {len(val_loader)}\")\nlogger.info(f\"Test batches:  {len(test_loader)}\")\n\n# ---- Model ----\nclass GriceDetector(nn.Module):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        hidden_size = self.encoder.config.hidden_size\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size // 2, num_labels),\n        )\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        cls_output = self.dropout(cls_output)\n        logits = self.classifier(cls_output)\n        return {'logits': logits}\n\nmodel = GriceDetector(CONFIG.model_name, CONFIG.num_labels).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nlogger.info(f\"Parameters: {trainable_params:,} / {total_params:,}\")\n\ntracker.mark('Model & Data', 'PASS', {'params': trainable_params})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 7: TRAINING LOOP\n# ============================================================================\nlogger.info(\"=\" * 60)\nlogger.info(\"TRAINING\")\nlogger.info(\"=\" * 60)\n\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\n# Optimizer\noptimizer = AdamW(model.parameters(), lr=CONFIG.learning_rate, weight_decay=CONFIG.weight_decay)\nscheduler = CosineAnnealingLR(optimizer, T_max=CONFIG.num_epochs * len(train_loader))\ncriterion = nn.BCEWithLogitsLoss()\n\n# ---- Evaluation function ----\ndef evaluate(model, loader, device, thresholds=None):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    all_probs = []\n    total_loss = 0\n    n_batches = 0\n    \n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs['logits'], labels)\n            \n            probs = torch.sigmoid(outputs['logits']).cpu().numpy()\n            all_probs.append(probs)\n            all_labels.append(labels.cpu().numpy())\n            total_loss += loss.item()\n            n_batches += 1\n    \n    all_probs = np.concatenate(all_probs)\n    all_labels = np.concatenate(all_labels)\n    \n    # Use thresholds or default 0.5\n    if thresholds is None:\n        thresholds = [0.5] * CONFIG.num_labels\n    \n    all_preds = (all_probs >= np.array(thresholds)).astype(int)\n    \n    # Per-class metrics\n    per_class = {}\n    for i, name in enumerate(MAXIM_NAMES):\n        if all_labels[:, i].sum() > 0:\n            f1 = f1_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n            prec = precision_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n            rec = recall_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n        else:\n            f1 = prec = rec = 0.0\n        per_class[name] = {'f1': f1, 'precision': prec, 'recall': rec}\n    \n    macro_f1 = np.mean([v['f1'] for v in per_class.values()])\n    avg_loss = total_loss / max(n_batches, 1)\n    \n    return macro_f1, per_class, avg_loss, all_probs, all_labels\n\n# ---- Threshold optimization ----\ndef optimize_thresholds(probs, labels):\n    best_thresholds = []\n    for i in range(CONFIG.num_labels):\n        best_f1 = 0\n        best_t = 0.5\n        for t in np.arange(0.1, 0.95, 0.05):\n            preds = (probs[:, i] >= t).astype(int)\n            f1 = f1_score(labels[:, i], preds, zero_division=0)\n            if f1 > best_f1:\n                best_f1 = f1\n                best_t = t\n        best_thresholds.append(round(best_t, 2))\n    return best_thresholds\n\n# ---- Training ----\ntraining_history = []\nbest_val_f1 = 0\nbest_epoch = 0\npatience = 0\nmax_patience = 2\n\ntrain_start = datetime.now()\n\nfor epoch in range(1, CONFIG.num_epochs + 1):\n    model.train()\n    epoch_loss = 0\n    n_batches = 0\n    \n    for batch_idx, batch in enumerate(train_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs['logits'], labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        epoch_loss += loss.item()\n        n_batches += 1\n        \n        if (batch_idx + 1) % 20 == 0:\n            logger.info(f\"  Epoch {epoch}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n    \n    avg_train_loss = epoch_loss / n_batches\n    \n    # Validate\n    val_f1, val_per_class, val_loss, val_probs, val_labels = evaluate(model, val_loader, device)\n    \n    # Optimize thresholds on validation set\n    optimal_thresholds = optimize_thresholds(val_probs, val_labels)\n    val_f1_opt, val_per_class_opt, _, _, _ = evaluate(model, val_loader, device, optimal_thresholds)\n    \n    epoch_result = {\n        'epoch': epoch,\n        'train_loss': avg_train_loss,\n        'val_loss': val_loss,\n        'val_macro_f1': val_f1,\n        'val_macro_f1_optimized': val_f1_opt,\n        'thresholds': dict(zip(MAXIM_NAMES, optimal_thresholds)),\n        'per_class': val_per_class_opt,\n    }\n    training_history.append(epoch_result)\n    \n    logger.info(f\"\\nEpoch {epoch}/{CONFIG.num_epochs}:\")\n    logger.info(f\"  Train loss: {avg_train_loss:.4f}\")\n    logger.info(f\"  Val loss:   {val_loss:.4f}\")\n    logger.info(f\"  Val F1:     {val_f1:.4f} (default) | {val_f1_opt:.4f} (optimized)\")\n    for name in MAXIM_NAMES:\n        sc = val_per_class_opt[name]\n        logger.info(f\"    {name}: F1={sc['f1']:.3f}, P={sc['precision']:.3f}, R={sc['recall']:.3f}\")\n    \n    # Save best\n    if val_f1_opt > best_val_f1:\n        best_val_f1 = val_f1_opt\n        best_epoch = epoch\n        best_thresholds = optimal_thresholds\n        torch.save(model.state_dict(), os.path.join(CONFIG.output_dir, 'best_model.pt'))\n        logger.info(f\"  ‚≠ê New best model! F1={val_f1_opt:.4f}\")\n        patience = 0\n    else:\n        patience += 1\n        logger.info(f\"  No improvement ({patience}/{max_patience})\")\n    \n    # Early stopping\n    if patience >= max_patience and epoch >= 3:\n        logger.info(f\"\\nEarly stopping at epoch {epoch}\")\n        break\n\ntrain_time = (datetime.now() - train_start).total_seconds()\nlogger.info(f\"\\nTraining complete: {train_time:.0f}s ({train_time/60:.1f} min)\")\nlogger.info(f\"Best epoch: {best_epoch} with F1={best_val_f1:.4f}\")\n\n# Load best model\nmodel.load_state_dict(torch.load(os.path.join(CONFIG.output_dir, 'best_model.pt'), weights_only=True))\nlogger.info(\"Loaded best model checkpoint\")\n\ntracker.mark('Training', 'PASS', {\n    'best_epoch': best_epoch,\n    'best_f1': best_val_f1,\n    'time_seconds': train_time,\n})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 8: TEST EVALUATION (HELD-OUT ‚Äî NEVER SEEN IN TRAINING)\n# ============================================================================\nlogger.info(\"=\" * 60)\nlogger.info(\"üéØ TEST SET EVALUATION (HELD-OUT)\")\nlogger.info(\"=\" * 60)\n\n# Overall test metrics\ntest_f1, test_per_class, test_loss, test_probs, test_labels = evaluate(\n    model, test_loader, device, best_thresholds\n)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"üìä HELD-OUT TEST RESULTS\")\nprint(f\"{'='*60}\")\nprint(f\"\\nMacro F1: {test_f1:.4f}\")\nprint(f\"Test Loss: {test_loss:.4f}\")\nprint(f\"\\nPer-Maxim Performance:\")\nfor name in MAXIM_NAMES:\n    sc = test_per_class[name]\n    # Flag suspicious scores\n    flag = \" ‚ö†Ô∏è SUSPICIOUS\" if sc['f1'] > 0.95 else \"\"\n    print(f\"  {name}: F1={sc['f1']:.3f}, P={sc['precision']:.3f}, R={sc['recall']:.3f}{flag}\")\n\nprint(f\"\\nThresholds used: {dict(zip(MAXIM_NAMES, best_thresholds))}\")\n\n# ---- Per-generation-method evaluation ----\nprint(f\"\\n{'='*60}\")\nprint(f\"üìä PER-GENERATION-METHOD BREAKDOWN\")\nprint(f\"{'='*60}\")\n\n# Group test examples by generation method\nmethod_examples = defaultdict(list)\ntest_preds = (test_probs >= np.array(best_thresholds)).astype(int)\n\nfor i, ex in enumerate(test_data):\n    if i < len(test_preds):\n        method_examples[ex.generation_method].append({\n            'true': test_labels[i] if i < len(test_labels) else ex.labels,\n            'pred': test_preds[i],\n            'probs': test_probs[i] if i < len(test_probs) else None,\n        })\n\nmethod_results = {}\nfor method, items in method_examples.items():\n    true_arr = np.array([item['true'] for item in items])\n    pred_arr = np.array([item['pred'] for item in items])\n    \n    method_f1s = {}\n    for j, name in enumerate(MAXIM_NAMES):\n        if true_arr[:, j].sum() > 0:\n            f1 = f1_score(true_arr[:, j], pred_arr[:, j], zero_division=0)\n            method_f1s[name] = f1\n    \n    macro = np.mean(list(method_f1s.values())) if method_f1s else 0\n    method_results[method] = {'macro_f1': macro, 'per_class': method_f1s, 'count': len(items)}\n    \n    print(f\"\\n  {method} ({len(items)} examples):\")\n    print(f\"    Macro F1: {macro:.3f}\")\n    for name, f1 in method_f1s.items():\n        print(f\"      {name}: {f1:.3f}\")\n\n# ---- Health check ----\nprint(f\"\\n{'='*60}\")\nprint(f\"‚úÖ HEALTH CHECKS\")\nprint(f\"{'='*60}\")\n\nif test_f1 > 0.95:\n    print(f\"  ‚ö†Ô∏è WARNING: F1={test_f1:.3f} is suspiciously high (>0.95)\")\n    print(f\"     This may indicate overfitting to synthetic patterns\")\nelif test_f1 > 0.80:\n    print(f\"  ‚úÖ EXCELLENT: F1={test_f1:.3f} is in the excellent range (0.80-0.95)\")\nelif test_f1 > 0.65:\n    print(f\"  ‚úÖ GOOD: F1={test_f1:.3f} is in the good range (0.65-0.80)\")\nelse:\n    print(f\"  ‚ö†Ô∏è LOW: F1={test_f1:.3f} ‚Äî model may need more data or tuning\")\n\ntracker.mark('Test Evaluation', 'PASS', {\n    'test_f1': test_f1,\n    'test_loss': test_loss,\n    'per_class': {k: v['f1'] for k, v in test_per_class.items()},\n    'per_method': {k: v['macro_f1'] for k, v in method_results.items()},\n})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 9: ERROR ANALYSIS\n# ============================================================================\nlogger.info(\"=\" * 60)\nlogger.info(\"ERROR ANALYSIS\")\nlogger.info(\"=\" * 60)\n\n# Find misclassified examples\nerrors = []\n\nfor i in range(min(len(test_data), len(test_preds))):\n    ex = test_data[i]\n    pred = test_preds[i]\n    true = np.array(ex.labels)\n    prob = test_probs[i]\n    \n    error_count = np.sum(pred != true)\n    if error_count > 0:\n        errors.append({\n            'idx': i,\n            'text': ex.text[:300],\n            'true_labels': true.tolist(),\n            'pred_labels': pred.tolist(),\n            'probs': prob.tolist(),\n            'source': ex.source,\n            'generation_method': ex.generation_method,\n            'violation_type': ex.violation_type,\n            'maxim': ex.maxim,\n            'error_count': error_count,\n        })\n\nerrors.sort(key=lambda x: x['error_count'], reverse=True)\n\nprint(f\"\\n‚ùå Total misclassified: {len(errors)} / {len(test_data)} ({100*len(errors)/len(test_data):.1f}%)\")\nprint(f\"‚úÖ Correctly classified: {len(test_data) - len(errors)} ({100*(len(test_data)-len(errors))/len(test_data):.1f}%)\")\n\n# Error type breakdown\nprint(f\"\\nüìä Error Type Breakdown:\")\nerror_by_maxim = defaultdict(lambda: {'false_pos': 0, 'false_neg': 0})\nfor err in errors:\n    for j, name in enumerate(MAXIM_NAMES):\n        if err['true_labels'][j] == 1 and err['pred_labels'][j] == 0:\n            error_by_maxim[name]['false_neg'] += 1\n        elif err['true_labels'][j] == 0 and err['pred_labels'][j] == 1:\n            error_by_maxim[name]['false_pos'] += 1\n\nfor name in MAXIM_NAMES:\n    fp = error_by_maxim[name]['false_pos']\n    fn = error_by_maxim[name]['false_neg']\n    print(f\"  {name}: {fp} false positives, {fn} false negatives\")\n\n# Error by generation method\nprint(f\"\\nüìä Errors by Generation Method:\")\nerror_by_method = Counter(err['generation_method'] for err in errors)\nfor method, count in error_by_method.most_common():\n    total_method = sum(1 for ex in test_data if ex.generation_method == method)\n    print(f\"  {method}: {count}/{total_method} errors ({100*count/max(total_method,1):.1f}%)\")\n\n# Top 10 worst errors\nprint(f\"\\nüìã Top 10 Worst Misclassifications:\")\nfor rank, err in enumerate(errors[:10], 1):\n    print(f\"\\n  #{rank} ({err['generation_method']}, {err['violation_type']})\")\n    print(f\"    Text: {err['text'][:150]}...\")\n    print(f\"    True: {err['true_labels']} ({', '.join(MAXIM_NAMES[j] for j in range(4) if err['true_labels'][j]==1) or 'Clean'})\")\n    print(f\"    Pred: {err['pred_labels']} ({', '.join(MAXIM_NAMES[j] for j in range(4) if err['pred_labels'][j]==1) or 'Clean'})\")\n    print(f\"    Probs: [{', '.join(f'{p:.2f}' for p in err['probs'])}]\")\n\ntracker.mark('Error Analysis', 'PASS', {\n    'total_errors': len(errors),\n    'error_rate': f\"{100*len(errors)/len(test_data):.1f}%\",\n    'error_by_maxim': {k: dict(v) for k, v in error_by_maxim.items()},\n})\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 10: SAVE RESULTS\n# ============================================================================\nlogger.info(\"=\" * 60)\nlogger.info(\"SAVING RESULTS\")\nlogger.info(\"=\" * 60)\n\n# Compile results\nresults = {\n    'phase': 'Phase 6 - Detector V2 (Verified Natural Violations)',\n    'timestamp': datetime.now().isoformat(),\n    'model': CONFIG.model_name,\n    'best_epoch': best_epoch,\n    'thresholds': dict(zip(MAXIM_NAMES, best_thresholds)),\n    'data_verification': {\n        'total_violations': len(violations),\n        'total_clean': len(clean_examples),\n        'generation_methods': dict(generation_method_counts),\n        'maxim_counts': dict(maxim_counts),\n        'source_file': phase4_path,\n        'assertions_passed': True,\n    },\n    'splits': {\n        'train': len(train_data),\n        'val': len(val_data),\n        'test': len(test_data),\n        'train_sources': train_sources,\n        'val_sources': val_sources,\n        'test_sources': test_sources,\n        'leakage_check': {\n            'train_val': train_val_overlap,\n            'train_test': train_test_overlap,\n            'val_test': val_test_overlap,\n        },\n    },\n    'validation': {\n        'macro_f1': best_val_f1,\n        'per_class': {name: training_history[best_epoch-1]['per_class'][name] for name in MAXIM_NAMES},\n    },\n    'test': {\n        'macro_f1': test_f1,\n        'loss': test_loss,\n        'per_class': {name: test_per_class[name] for name in MAXIM_NAMES},\n    },\n    'test_per_method': {method: {\n        'macro_f1': info['macro_f1'],\n        'count': info['count'],\n        'per_class': info['per_class'],\n    } for method, info in method_results.items()},\n    'error_analysis': {\n        'total_errors': len(errors),\n        'error_rate': round(100 * len(errors) / len(test_data), 2),\n        'error_by_maxim': {k: dict(v) for k, v in error_by_maxim.items()},\n        'error_by_method': dict(error_by_method),\n        'top_10_errors': errors[:10],\n    },\n    'training_history': training_history,\n    'gpu': {\n        'name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A',\n        'peak_vram_gb': float(torch.cuda.max_memory_allocated(0) / 1e9) if torch.cuda.is_available() else 0,\n    },\n    'training_time_seconds': train_time,\n}\n\n# Save results\nresults_path = os.path.join(CONFIG.output_dir, 'detector_v2_results.json')\nwith open(results_path, 'w') as f:\n    json.dump(results, f, indent=2, default=str)\nlogger.info(f\"Results saved: {results_path}\")\n\n# Save thresholds\nthresholds_path = os.path.join(CONFIG.output_dir, 'optimal_thresholds.json')\nwith open(thresholds_path, 'w') as f:\n    json.dump({\n        'thresholds': dict(zip(MAXIM_NAMES, best_thresholds)),\n        'macro_f1': best_val_f1,\n    }, f, indent=2)\nlogger.info(f\"Thresholds saved: {thresholds_path}\")\n\n# Copy to /kaggle/working for download\nimport shutil\nfor fname in ['detector_v2_results.json', 'optimal_thresholds.json', 'best_model.pt']:\n    src = os.path.join(CONFIG.output_dir, fname)\n    if os.path.exists(src):\n        dst = os.path.join('/kaggle/working', fname)\n        shutil.copy2(src, dst)\n        logger.info(f\"Copied: {dst}\")\n\ntracker.mark('Save Results', 'PASS')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CELL 11: FINAL SUMMARY\n# ============================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üèÅ PHASE 6 DETECTOR V2 ‚Äî FINAL SUMMARY\")\nprint(\"=\" * 60)\n\nprint(f\"\\nüì¶ Model: {CONFIG.model_name}\")\nprint(f\"üìä Data: {len(violations)} violations + {len(clean_examples)} clean from Phase 4\")\nprint(f\"üî¨ Generation methods: {dict(generation_method_counts)}\")\n\nprint(f\"\\nüìà Training:\")\nprint(f\"  Best epoch: {best_epoch}\")\nprint(f\"  Val F1: {best_val_f1:.4f}\")\nprint(f\"  Time: {train_time:.0f}s ({train_time/60:.1f} min)\")\n\nprint(f\"\\nüéØ TEST SET RESULTS (held-out, never in training):\")\nprint(f\"  Macro F1: {test_f1:.4f}\")\nfor name in MAXIM_NAMES:\n    sc = test_per_class[name]\n    print(f\"    {name}: F1={sc['f1']:.3f}\")\n\nprint(f\"\\nüîç Per-Method Performance:\")\nfor method, info in sorted(method_results.items(), key=lambda x: -x[1]['macro_f1']):\n    print(f\"  {method}: F1={info['macro_f1']:.3f} ({info['count']} examples)\")\n\nprint(f\"\\n‚ùå Errors: {len(errors)}/{len(test_data)} ({100*len(errors)/len(test_data):.1f}%)\")\n\nprint(f\"\\n‚úÖ DATA VERIFIED:\")\nprint(f\"  Phase 4 violations loaded: {len(violations)}\")\nprint(f\"  No data leakage: ‚úÖ\")\nprint(f\"  All maxims represented: ‚úÖ\")\n\nprint(f\"\\nüìÅ Output Files:\")\nprint(f\"  /kaggle/working/detector_v2_results.json\")\nprint(f\"  /kaggle/working/optimal_thresholds.json\")\nprint(f\"  /kaggle/working/best_model.pt\")\n\nif test_f1 > 0.95:\n    print(f\"\\n‚ö†Ô∏è  F1={test_f1:.3f} is very high ‚Äî review error analysis for overfitting signs\")\nelif test_f1 > 0.70:\n    print(f\"\\n‚úÖ Results look realistic and healthy\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  F1={test_f1:.3f} is below target ‚Äî may need more data or training\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"PHASE 6 V2 COMPLETE ‚Äî Download detector_v2_results.json\")\nprint(f\"{'='*60}\")\n\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\ntracker.mark('Complete', 'PASS')\nfor step in tracker.steps:\n    print(f\"  {step['status']}: {step['name']} ({step['elapsed']:.0f}s)\")\n"
  }
 ]
}