{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Synthetic Data Generation - FREE (Hugging Face)\n",
                "\n",
                "**100% Free - No Credit Card Needed**\n",
                "\n",
                "1. Get free API token from huggingface.co/settings/tokens\n",
                "2. Upload scored_data.json\n",
                "3. Run cells"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q huggingface_hub tqdm\n",
                "\n",
                "import os, json, time\n",
                "from tqdm import tqdm\n",
                "from huggingface_hub import InferenceClient\n",
                "\n",
                "print(\"✅ Ready\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# API Token (FREE from huggingface.co/settings/tokens)\n",
                "print(\"Enter your Hugging Face token:\")\n",
                "HF_TOKEN = input(\"Token: \").strip()\n",
                "\n",
                "client = InferenceClient(token=HF_TOKEN)\n",
                "\n",
                "# Find data\n",
                "INPUT_FILE = None\n",
                "for p in [\"/kaggle/input/gricebench-scored/scored_data.json\", \"/kaggle/input/scored-data/scored_data.json\"]:\n",
                "    if os.path.exists(p): INPUT_FILE = p; break\n",
                "\n",
                "if not INPUT_FILE: raise FileNotFoundError(\"Upload scored_data.json!\")\n",
                "\n",
                "OUTPUT_FILE = \"/kaggle/working/synthetic_candidates.json\"\n",
                "print(f\"✅ Input: {INPUT_FILE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# System Prompt\n",
                "SYSTEM = \"\"\"You are a Gricean Cooperative Assistant.\n",
                "Generate responses that strictly adhere to all four Gricean Maxims:\n",
                "1. Quantity: Be as informative as required, but no more.\n",
                "2. Quality: Do not say what you believe to be false.\n",
                "3. Relation: Be strictly relevant.\n",
                "4. Manner: Be clear, concise, and polite.\n",
                "\n",
                "Answer the prompt directly and cooperatively. Do not be chatty.\"\"\"\n",
                "\n",
                "def get_failed(path):\n",
                "    with open(path) as f: data = json.load(f)\n",
                "    return [e for e in data if not all(e.get('margins',{}).get(m,0)>0 for m in ['quantity','quality','relation','manner'])]\n",
                "\n",
                "def run():\n",
                "    all_cands = get_failed(INPUT_FILE)\n",
                "    print(f\"Target: {len(all_cands)}\")\n",
                "    \n",
                "    done = []\n",
                "    if os.path.exists(OUTPUT_FILE):\n",
                "        try:\n",
                "            with open(OUTPUT_FILE) as f: done = json.load(f)\n",
                "            print(f\"Resume: {len(done)} done\")\n",
                "        except: pass\n",
                "    \n",
                "    done_prompts = {d['prompt'] for d in done}\n",
                "    todos = [c for c in all_cands if c['prompt'] not in done_prompts]\n",
                "    print(f\"Remaining: {len(todos)}\\nStarting...\")\n",
                "\n",
                "    try:\n",
                "        for i, item in enumerate(tqdm(todos)):\n",
                "            try:\n",
                "                # Call HF Inference API (FREE)\n",
                "                messages = [\n",
                "                    {\"role\": \"system\", \"content\": SYSTEM},\n",
                "                    {\"role\": \"user\", \"content\": item['prompt']}\n",
                "                ]\n",
                "                \n",
                "                response = client.chat_completion(\n",
                "                    messages=messages,\n",
                "                    model=\"meta-llama/Llama-3.3-70B-Instruct\",  # FREE model\n",
                "                    max_tokens=1024,\n",
                "                    temperature=0.7\n",
                "                )\n",
                "                \n",
                "                text = response.choices[0].message.content.strip()\n",
                "                \n",
                "                res = item.copy()\n",
                "                res['synthetic_chosen'] = text\n",
                "                res['original_chosen_failed'] = item['chosen']\n",
                "                res['chosen'] = text\n",
                "                done.append(res)\n",
                "                \n",
                "                if (i+1) % 10 == 0:\n",
                "                    with open(OUTPUT_FILE, 'w') as f: json.dump(done, f, indent=2)\n",
                "                \n",
                "                time.sleep(1.0)  # HF free tier rate limit\n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"\\nErr {i}: {e}\")\n",
                "                time.sleep(30)\n",
                "                \n",
                "    except KeyboardInterrupt:\n",
                "        print(\"\\nStopped\")\n",
                "    finally:\n",
                "        with open(OUTPUT_FILE, 'w') as f: json.dump(done, f, indent=2)\n",
                "        print(f\"\\n✅ {len(done)} saved\")\n",
                "\n",
                "run()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}