{
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GriceBench Phase 3: Detector-Human Agreement Analysis\n",
                "\n",
                "## What This Notebook Does\n",
                "\n",
                "1. **Loads annotation sample** with detector labels\n",
                "2. **Calculates agreement metrics** between detector predictions and ground truth\n",
                "3. **Cohen's Kappa** for each maxim (Quantity, Quality, Relation, Manner)\n",
                "4. **Error analysis** - which violations is the detector missing?\n",
                "5. **Decision point** - determine if detector needs retraining\n",
                "\n",
                "---\n",
                "\n",
                "## Required Dataset: `gricebench-scientific-fix`\n",
                "\n",
                "**Files needed (from Phase 2):**\n",
                "\n",
                "| File | Description |\n",
                "|------|-------------|\n",
                "| `annotation_sample_1000.json` | 600+ examples with detector labels |\n",
                "| `relation_repair_mrr.json` | MRR results (for reference) |\n",
                "\n",
                "**Local paths (add these to your Kaggle dataset):**\n",
                "- `c:\\Users\\pushk\\OneDrive\\Documents\\Research Model\\GriceBench\\results\\phase2output\\annotation_sample_1000.json`\n",
                "- `c:\\Users\\pushk\\OneDrive\\Documents\\Research Model\\GriceBench\\results\\phase2output\\relation_repair_mrr.json`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 1: IMPORTS AND CONFIGURATION\n",
                "# ============================================================================\n",
                "\n",
                "import json\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from collections import defaultdict\n",
                "from typing import Dict, List\n",
                "\n",
                "# Paths\n",
                "DATA_INPUT = Path(\"/kaggle/input/gricebench-scientific-fix\")\n",
                "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
                "\n",
                "print(\"Configuration:\")\n",
                "print(f\"  Input: {DATA_INPUT}\")\n",
                "print(f\"  Output: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 2: LOAD ANNOTATION SAMPLE\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"LOADING ANNOTATION SAMPLE\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Find annotation sample\n",
                "annotation_path = DATA_INPUT / \"annotation_sample_1000.json\"\n",
                "if not annotation_path.exists():\n",
                "    # Try phase2output subfolder\n",
                "    annotation_path = DATA_INPUT / \"phase2output\" / \"annotation_sample_1000.json\"\n",
                "\n",
                "if annotation_path.exists():\n",
                "    with open(annotation_path, 'r', encoding='utf-8') as f:\n",
                "        annotations = json.load(f)\n",
                "    print(f\"‚úÖ Loaded {len(annotations)} examples\")\n",
                "else:\n",
                "    print(\"‚ùå annotation_sample_1000.json not found!\")\n",
                "    print(f\"   Checked: {annotation_path}\")\n",
                "    annotations = []\n",
                "\n",
                "# Preview structure\n",
                "if annotations:\n",
                "    print(f\"\\nSample keys: {list(annotations[0].keys())}\")\n",
                "    if 'labels' in annotations[0]:\n",
                "        print(f\"Labels structure: {annotations[0]['labels']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 3: ANALYZE LABEL DISTRIBUTION\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"LABEL DISTRIBUTION ANALYSIS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "maxims = ['quantity', 'quality', 'relation', 'manner']\n",
                "\n",
                "# Count detector predictions\n",
                "detector_counts = defaultdict(int)\n",
                "examples_with_labels = 0\n",
                "category_counts = defaultdict(int)\n",
                "\n",
                "for item in annotations:\n",
                "    labels = item.get('labels', {})\n",
                "    if labels:\n",
                "        examples_with_labels += 1\n",
                "        for maxim in maxims:\n",
                "            if labels.get(maxim, 0) == 1:\n",
                "                detector_counts[maxim] += 1\n",
                "    \n",
                "    category = item.get('annotation_category', 'unknown')\n",
                "    category_counts[category] += 1\n",
                "\n",
                "print(f\"\\nTotal examples: {len(annotations)}\")\n",
                "print(f\"Examples with detector labels: {examples_with_labels}\")\n",
                "\n",
                "print(\"\\nüìä Detector Predictions (positive count):\")\n",
                "for maxim in maxims:\n",
                "    count = detector_counts[maxim]\n",
                "    pct = count / len(annotations) * 100 if annotations else 0\n",
                "    print(f\"   {maxim.capitalize()}: {count} ({pct:.1f}%)\")\n",
                "\n",
                "print(\"\\nüìÅ Category Distribution:\")\n",
                "for cat, count in sorted(category_counts.items()):\n",
                "    print(f\"   {cat}: {count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 4: COHEN'S KAPPA CALCULATION\n",
                "# ============================================================================\n",
                "\n",
                "def calculate_cohen_kappa(y_true: List[int], y_pred: List[int]) -> float:\n",
                "    \"\"\"\n",
                "    Calculate Cohen's Kappa for binary classification.\n",
                "    \n",
                "    Œ∫ = (p_o - p_e) / (1 - p_e)\n",
                "    where p_o = observed agreement, p_e = expected agreement\n",
                "    \"\"\"\n",
                "    if len(y_true) != len(y_pred):\n",
                "        raise ValueError(\"Lists must be same length\")\n",
                "    \n",
                "    n = len(y_true)\n",
                "    if n == 0:\n",
                "        return 0.0\n",
                "    \n",
                "    # Observed agreement\n",
                "    p_o = sum(1 for a, b in zip(y_true, y_pred) if a == b) / n\n",
                "    \n",
                "    # Expected agreement\n",
                "    true_pos = sum(y_true) / n\n",
                "    true_neg = 1 - true_pos\n",
                "    pred_pos = sum(y_pred) / n\n",
                "    pred_neg = 1 - pred_pos\n",
                "    \n",
                "    p_e = (true_pos * pred_pos) + (true_neg * pred_neg)\n",
                "    \n",
                "    if p_e == 1.0:\n",
                "        return 1.0 if p_o == 1.0 else 0.0\n",
                "    \n",
                "    return (p_o - p_e) / (1 - p_e)\n",
                "\n",
                "print(\"Cohen's Kappa function defined ‚úÖ\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 5: CALCULATE AGREEMENT USING GROUND TRUTH\n",
                "# ============================================================================\n",
                "# \n",
                "# The annotation_sample has:\n",
                "# - 'labels': detector predictions (what we're evaluating)\n",
                "# - 'violation_type': ground truth of what violation was injected\n",
                "# \n",
                "# We compare detector predictions against the known violation type.\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"DETECTOR-GROUND TRUTH AGREEMENT\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Map violation types to maxims\n",
                "violation_to_maxim = {\n",
                "    'quantity_over': 'quantity',\n",
                "    'quantity_under': 'quantity',\n",
                "    'quality_unsupported': 'quality',\n",
                "    'quality_contradictory': 'quality',\n",
                "    'relation_off_topic': 'relation',\n",
                "    'relation_tangential': 'relation',\n",
                "    'manner_ambiguous': 'manner',\n",
                "    'manner_jargon': 'manner',\n",
                "    'manner_shuffled': 'manner',\n",
                "}\n",
                "\n",
                "# Build ground truth labels\n",
                "ground_truth = defaultdict(list)\n",
                "detector_pred = defaultdict(list)\n",
                "\n",
                "valid_count = 0\n",
                "for item in annotations:\n",
                "    detector_labels = item.get('labels', {})\n",
                "    violation_type = item.get('violation_type', '')\n",
                "    category = item.get('annotation_category', '')\n",
                "    \n",
                "    # Skip examples without detector labels\n",
                "    if not detector_labels:\n",
                "        continue\n",
                "    \n",
                "    valid_count += 1\n",
                "    \n",
                "    # Determine ground truth for each maxim\n",
                "    for maxim in maxims:\n",
                "        # Ground truth: was this maxim actually violated?\n",
                "        true_label = 0\n",
                "        \n",
                "        # Check violation_type\n",
                "        if violation_type:\n",
                "            if violation_type.startswith(maxim) or violation_to_maxim.get(violation_type) == maxim:\n",
                "                true_label = 1\n",
                "            elif violation_type.startswith('multi_'):\n",
                "                # Multi-violation, check metadata\n",
                "                metadata = item.get('metadata', {})\n",
                "                violated_maxims = metadata.get('maxims_violated', [])\n",
                "                if maxim in violated_maxims:\n",
                "                    true_label = 1\n",
                "        \n",
                "        # Check category for positive samples\n",
                "        if category == f\"{maxim}_positive\":\n",
                "            true_label = 1\n",
                "        \n",
                "        # Detector prediction\n",
                "        pred_label = int(detector_labels.get(maxim, 0))\n",
                "        \n",
                "        ground_truth[maxim].append(true_label)\n",
                "        detector_pred[maxim].append(pred_label)\n",
                "\n",
                "print(f\"\\nValid examples for agreement: {valid_count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 6: CALCULATE AND DISPLAY RESULTS\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"AGREEMENT RESULTS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "results = {}\n",
                "\n",
                "for maxim in maxims:\n",
                "    y_true = ground_truth[maxim]\n",
                "    y_pred = detector_pred[maxim]\n",
                "    \n",
                "    if not y_true:\n",
                "        continue\n",
                "    \n",
                "    # Calculate metrics\n",
                "    kappa = calculate_cohen_kappa(y_true, y_pred)\n",
                "    \n",
                "    tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n",
                "    tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)\n",
                "    fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n",
                "    fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n",
                "    \n",
                "    accuracy = (tp + tn) / len(y_true)\n",
                "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
                "    \n",
                "    results[maxim] = {\n",
                "        'kappa': round(kappa, 4),\n",
                "        'accuracy': round(accuracy, 4),\n",
                "        'precision': round(precision, 4),\n",
                "        'recall': round(recall, 4),\n",
                "        'f1': round(f1, 4),\n",
                "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn\n",
                "    }\n",
                "    \n",
                "    print(f\"\\n{maxim.upper()}:\")\n",
                "    print(f\"   Cohen's Œ∫:  {kappa:.4f}\")\n",
                "    print(f\"   Accuracy:   {accuracy:.4f}\")\n",
                "    print(f\"   Precision:  {precision:.4f}\")\n",
                "    print(f\"   Recall:     {recall:.4f}\")\n",
                "    print(f\"   F1:         {f1:.4f}\")\n",
                "    print(f\"   Confusion:  TP={tp}, TN={tn}, FP={fp}, FN={fn}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 7: OVERALL KAPPA AND VERDICT\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"OVERALL AGREEMENT AND VERDICT\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Calculate overall kappa (across all maxims)\n",
                "all_true = []\n",
                "all_pred = []\n",
                "for maxim in maxims:\n",
                "    all_true.extend(ground_truth[maxim])\n",
                "    all_pred.extend(detector_pred[maxim])\n",
                "\n",
                "overall_kappa = calculate_cohen_kappa(all_true, all_pred)\n",
                "\n",
                "# Mean kappa across maxims\n",
                "kappa_values = [results[m]['kappa'] for m in results]\n",
                "mean_kappa = np.mean(kappa_values) if kappa_values else 0\n",
                "\n",
                "print(f\"\\nOverall Cohen's Œ∫: {overall_kappa:.4f}\")\n",
                "print(f\"Mean Œ∫ (per maxim): {mean_kappa:.4f}\")\n",
                "\n",
                "# Interpretation\n",
                "print(\"\\n\" + \"-\" * 50)\n",
                "print(\"Œ∫ INTERPRETATION:\")\n",
                "print(\"   Œ∫ < 0.2:  Poor agreement\")\n",
                "print(\"   0.2-0.4:  Fair agreement\")\n",
                "print(\"   0.4-0.6:  Moderate agreement\")\n",
                "print(\"   0.6-0.8:  Substantial agreement\")\n",
                "print(\"   Œ∫ > 0.8:  Almost perfect agreement\")\n",
                "\n",
                "print(\"\\n\" + \"-\" * 50)\n",
                "print(\"VERDICT:\")\n",
                "\n",
                "if mean_kappa >= 0.7:\n",
                "    verdict = \"EXCELLENT\"\n",
                "    action = \"Detector is well-calibrated. Proceed to Phase 4.\"\n",
                "    emoji = \"‚úÖ\"\n",
                "elif mean_kappa >= 0.5:\n",
                "    verdict = \"ACCEPTABLE\"\n",
                "    action = \"Detector acceptable but could improve. Monitor errors.\"\n",
                "    emoji = \"‚ö†Ô∏è\"\n",
                "else:\n",
                "    verdict = \"NEEDS RETRAINING\"\n",
                "    action = \"Detector needs retraining on human annotations.\"\n",
                "    emoji = \"‚ùå\"\n",
                "\n",
                "print(f\"\\n{emoji} {verdict}\")\n",
                "print(f\"\\nRecommendation: {action}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 8: ERROR ANALYSIS\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"ERROR ANALYSIS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Find examples where detector made errors\n",
                "for maxim in maxims:\n",
                "    fn_examples = []  # False negatives (detector missed)\n",
                "    fp_examples = []  # False positives (detector wrong)\n",
                "    \n",
                "    for i, item in enumerate(annotations):\n",
                "        detector_labels = item.get('labels', {})\n",
                "        if not detector_labels:\n",
                "            continue\n",
                "        \n",
                "        true_label = ground_truth[maxim][i] if i < len(ground_truth[maxim]) else 0\n",
                "        pred_label = detector_pred[maxim][i] if i < len(detector_pred[maxim]) else 0\n",
                "        \n",
                "        if true_label == 1 and pred_label == 0:\n",
                "            fn_examples.append(item)\n",
                "        elif true_label == 0 and pred_label == 1:\n",
                "            fp_examples.append(item)\n",
                "    \n",
                "    print(f\"\\n{maxim.upper()}:\")\n",
                "    print(f\"   False Negatives (missed): {len(fn_examples)}\")\n",
                "    print(f\"   False Positives (wrong): {len(fp_examples)}\")\n",
                "    \n",
                "    # Show sample error\n",
                "    if fn_examples:\n",
                "        sample = fn_examples[0]\n",
                "        resp = sample.get('violated_response', sample.get('response', ''))[:100]\n",
                "        print(f\"   Sample FN: {resp}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 9: SAVE RESULTS\n",
                "# ============================================================================\n",
                "\n",
                "output = {\n",
                "    'per_maxim': results,\n",
                "    'overall_kappa': round(overall_kappa, 4),\n",
                "    'mean_kappa': round(mean_kappa, 4),\n",
                "    'verdict': verdict,\n",
                "    'recommendation': action,\n",
                "    'n_examples': len(annotations)\n",
                "}\n",
                "\n",
                "output_path = OUTPUT_DIR / \"detector_human_agreement.json\"\n",
                "with open(output_path, 'w') as f:\n",
                "    json.dump(output, f, indent=2)\n",
                "\n",
                "print(f\"\\n‚úÖ Results saved to: {output_path}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"PHASE 3 COMPLETE\")\n",
                "print(\"=\" * 70)\n",
                "print(f\"\\nDownload: detector_human_agreement.json\")\n",
                "print(f\"\\nNext Steps:\")\n",
                "if mean_kappa >= 0.5:\n",
                "    print(\"   ‚Üí Proceed to Phase 4: Natural Violation Collection\")\n",
                "else:\n",
                "    print(\"   ‚Üí Go to Phase 6: Retrain detector on annotations\")"
            ]
        }
    ]
}