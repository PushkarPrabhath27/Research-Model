{
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 5: 3-Tier Hybrid DPO - Tier 1 & 2 Generation\n\n## The 3-Tier Strategy\n\n| Tier | Type | Count | Method |\n|------|------|-------|--------|\n| 1 | Hard Pairs | 500 | Active Learning ‚Üí YOU annotate |\n| 2 | Obvious Pairs | 500 | Automated (extreme violations) |\n| 3 | Bootstrapped | 500 | After annotation (next notebook) |\n\n**This notebook**: Creates Tier 1 + Tier 2 pairs from your Phase 5 output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 1: SETUP\nimport json\nimport random\nimport numpy as np\nfrom pathlib import Path\nfrom collections import defaultdict\n\nDATA_INPUT = Path('/kaggle/input/gricebench-scientific-fix')\nOUTPUT_DIR = Path('/kaggle/working')\nrandom.seed(42)\n\nprint('3-Tier Hybrid DPO System')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 2: LOAD PHASE 5 OUTPUT (Your 1,500 pairs)\nprint('=' * 70)\nprint('LOADING PHASE 5 OUTPUT')\nprint('=' * 70)\n\n# Load the preference pairs you generated\npairs_path = DATA_INPUT / 'preference_pairs_1500.json'\nif pairs_path.exists():\n    with open(pairs_path, 'r', encoding='utf-8') as f:\n        all_pairs = json.load(f)\n    print(f'Loaded {len(all_pairs)} preference pairs')\nelse:\n    print('ERROR: preference_pairs_1500.json not found!')\n    print('Please add your Phase 5 output to the dataset')\n    all_pairs = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 3: TIER 1 - ACTIVE LEARNING (Select 500 Hardest Pairs)\nprint('=' * 70)\nprint('TIER 1: ACTIVE LEARNING - SELECTING HARD PAIRS')\nprint('=' * 70)\n\ndef compute_uncertainty(pair):\n    \"\"\"\n    Estimate uncertainty/difficulty of a pair.\n    Hard pairs = responses are similar in length, style, content\n    \"\"\"\n    resp_a = pair.get('response_A', '')\n    resp_b = pair.get('response_B', '')\n    \n    # Length similarity (hard if similar length)\n    len_diff = abs(len(resp_a) - len(resp_b)) / max(len(resp_a), len(resp_b), 1)\n    len_similarity = 1 - len_diff\n    \n    # Word overlap (hard if similar words)\n    words_a = set(resp_a.lower().split())\n    words_b = set(resp_b.lower().split())\n    if len(words_a | words_b) > 0:\n        jaccard = len(words_a & words_b) / len(words_a | words_b)\n    else:\n        jaccard = 0\n    \n    # Combine: Higher = harder to distinguish\n    uncertainty = (len_similarity + jaccard) / 2\n    \n    return uncertainty\n\n# Score all pairs\npair_scores = []\nfor pair in all_pairs:\n    uncertainty = compute_uncertainty(pair)\n    pair_scores.append((pair, uncertainty))\n\n# Sort by uncertainty (hardest first)\npair_scores.sort(key=lambda x: x[1], reverse=True)\n\n# Take top 500 hardest\ntier1_hard_pairs = [p for p, _ in pair_scores[:500]]\n\nprint(f'Selected {len(tier1_hard_pairs)} hardest pairs for human annotation')\nprint(f'Uncertainty range: {pair_scores[0][1]:.3f} - {pair_scores[499][1]:.3f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 4: TIER 2 - GENERATE OBVIOUS VIOLATION PAIRS\nprint('=' * 70)\nprint('TIER 2: GENERATING OBVIOUS VIOLATION PAIRS')\nprint('=' * 70)\n\n# Obvious violation generators\nOBVIOUS_VIOLATIONS = {\n    'offtopic': [\n        \"Let me tell you about my favorite pizza toppings instead.\",\n        \"Speaking of cats, have you ever seen a rainbow?\",\n        \"The weather is nice today. I like blue.\",\n        \"Random thoughts about nothing related to what you asked.\",\n    ],\n    'contradiction': [\n        \" Actually, everything I just said is false.\",\n        \" Wait, no. The opposite is true.\",\n        \" But then again, none of this is accurate.\",\n    ],\n    'nonsense': [\n        \"Asdf jkl qwer uiop zxcv bnm.\",\n        \"The purple elephant danced with quantum cheese.\",\n        \"Blah blah blah blah blah blah blah.\",\n        \"$$%%^^&&**!!@@##\",\n    ],\n    'extreme_verbose': [\n        \"Let me explain this in EXTREME detail. First, I want to provide extensive background. Then, I'll elaborate on every single point. Additionally, I'll add more context. Furthermore, there's more to discuss. Moreover, we should consider all angles. In conclusion, after all this, \",\n    ],\n}\n\ndef create_obvious_pair(context, good_response, violation_type):\n    \"\"\"Create a pair where good_response is obviously better.\"\"\"\n    if violation_type == 'offtopic':\n        bad_response = random.choice(OBVIOUS_VIOLATIONS['offtopic'])\n    elif violation_type == 'contradiction':\n        bad_response = good_response + random.choice(OBVIOUS_VIOLATIONS['contradiction'])\n    elif violation_type == 'nonsense':\n        bad_response = random.choice(OBVIOUS_VIOLATIONS['nonsense'])\n    elif violation_type == 'extreme_verbose':\n        bad_response = random.choice(OBVIOUS_VIOLATIONS['extreme_verbose']) + good_response + \" \" + good_response\n    else:\n        bad_response = \"I don't know.\"\n    \n    return {\n        'context': context,\n        'response_A': good_response,  # Always the good one\n        'response_B': bad_response,   # Always the bad one\n        'preference': 'A_much',       # Pre-labeled\n        'violation_type': violation_type,\n        'tier': 2,\n        'auto_labeled': True,\n    }\n\n# Generate 500 obvious pairs\ntier2_obvious_pairs = []\nviolation_types = ['offtopic', 'contradiction', 'nonsense', 'extreme_verbose']\n\n# Use remaining pairs (not in tier 1) as source\nremaining_pairs = [p for p, _ in pair_scores[500:]]\n\nfor i in range(500):\n    source_pair = remaining_pairs[i % len(remaining_pairs)]\n    context = source_pair.get('context', '')\n    good_response = source_pair.get('response_A', '')  # Use A as good\n    \n    violation_type = violation_types[i % 4]\n    obvious_pair = create_obvious_pair(context, good_response, violation_type)\n    obvious_pair['id'] = f'tier2_{i}'\n    tier2_obvious_pairs.append(obvious_pair)\n\nprint(f'Generated {len(tier2_obvious_pairs)} obvious violation pairs')\n\n# Show distribution\ntype_counts = defaultdict(int)\nfor p in tier2_obvious_pairs:\n    type_counts[p['violation_type']] += 1\nprint('Distribution:', dict(type_counts))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 5: PREPARE TIER 1 FOR ANNOTATION\nprint('=' * 70)\nprint('PREPARING TIER 1 FOR ANNOTATION')\nprint('=' * 70)\n\n# Create annotation-ready format\ntier1_for_annotation = []\nfor i, pair in enumerate(tier1_hard_pairs):\n    tier1_for_annotation.append({\n        'id': f'tier1_{i}',\n        'context': pair.get('context', ''),\n        'response_A': pair.get('response_A', ''),\n        'response_B': pair.get('response_B', ''),\n        'preference': '',  # YOU fill: A_much, A_slight, equal, B_slight, B_much\n        'reason': '',      # Optional: informative, accurate, on_topic, clear\n        'tier': 1,\n        'annotated': False,\n    })\n\nprint(f'Prepared {len(tier1_for_annotation)} pairs for annotation')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 6: SHOW SAMPLES\nprint('=' * 70)\nprint('SAMPLE HARD PAIRS (Tier 1)')\nprint('=' * 70)\n\nfor i, pair in enumerate(tier1_for_annotation[:2]):\n    print(f\"\\n--- Hard Pair {i+1} ---\")\n    print(f\"Context: {pair['context'][:80]}...\")\n    print(f\"A: {pair['response_A'][:60]}...\")\n    print(f\"B: {pair['response_B'][:60]}...\")\n    print(\"[These are HARD - responses look similar]\")\n\nprint('\\n' + '=' * 70)\nprint('SAMPLE OBVIOUS PAIRS (Tier 2)')\nprint('=' * 70)\n\nfor i, pair in enumerate(tier2_obvious_pairs[:2]):\n    print(f\"\\n--- Obvious Pair {i+1} ({pair['violation_type']}) ---\")\n    print(f\"A (good): {pair['response_A'][:60]}...\")\n    print(f\"B (bad):  {pair['response_B'][:60]}...\")\n    print(f\"[OBVIOUS - A is clearly better]\")\n    print(f\"Auto-preference: {pair['preference']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 7: SAVE OUTPUTS\nprint('=' * 70)\nprint('SAVING OUTPUTS')\nprint('=' * 70)\n\n# Save Tier 1 for annotation\ntier1_path = OUTPUT_DIR / 'tier1_hard_pairs.json'\nwith open(tier1_path, 'w', encoding='utf-8') as f:\n    json.dump(tier1_for_annotation, f, indent=2, ensure_ascii=False)\nprint(f'‚úÖ Saved {len(tier1_for_annotation)} hard pairs for annotation')\n\n# Save Tier 2 (already labeled)\ntier2_path = OUTPUT_DIR / 'tier2_obvious_pairs.json'\nwith open(tier2_path, 'w', encoding='utf-8') as f:\n    json.dump(tier2_obvious_pairs, f, indent=2, ensure_ascii=False)\nprint(f'‚úÖ Saved {len(tier2_obvious_pairs)} obvious pairs (auto-labeled)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CELL 8: SUMMARY\nprint('\\n' + '=' * 70)\nprint('3-TIER HYBRID DPO - TIER 1 & 2 COMPLETE')\nprint('=' * 70)\n\nprint(f'\\nüìä SUMMARY:')\nprint(f'   Tier 1 (HARD pairs): {len(tier1_for_annotation)} ‚Üí YOU annotate')\nprint(f'   Tier 2 (OBVIOUS pairs): {len(tier2_obvious_pairs)} ‚Üí Auto-labeled ‚úÖ')\nprint(f'   Tier 3 (BOOTSTRAPPED): 500 ‚Üí After your annotation')\n\nprint(f'\\nüìÅ OUTPUT FILES:')\nprint(f'   tier1_hard_pairs.json - Download & annotate with HTML interface')\nprint(f'   tier2_obvious_pairs.json - Already labeled, ready for training')\n\nprint(f'\\nüìã YOUR NEXT STEPS:')\nprint(f'   1. Download tier1_hard_pairs.json')\nprint(f'   2. Open annotation_interface.html')\nprint(f'   3. Load the file and annotate (~500 pairs, ~4 hours)')\nprint(f'   4. Save as tier1_annotated.json')\nprint(f'   5. Upload to Kaggle dataset')\nprint(f'   6. Run Phase 5 Training notebook')\n\nprint(f'\\n‚è±Ô∏è TIME ESTIMATE:')\nprint(f'   Annotation: ~30 seconds/pair √ó 500 = ~4 hours')\nprint(f'   Tip: Do 100 pairs/day over 5 days')\n\nprint('\\n' + '=' * 70)"
            ]
        }
    ]
}