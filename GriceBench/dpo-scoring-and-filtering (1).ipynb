{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14369664,"sourceType":"datasetVersion","datasetId":9176141},{"sourceId":14369676,"sourceType":"datasetVersion","datasetId":9176149}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================\n# CELL 1: Setup and Imports\n# ============================================\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel\nimport json\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nprint(\"‚úì Imports complete\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:25:22.212873Z","iopub.execute_input":"2026-01-02T14:25:22.213194Z","iopub.status.idle":"2026-01-02T14:25:33.643350Z","shell.execute_reply.started":"2026-01-02T14:25:22.213165Z","shell.execute_reply":"2026-01-02T14:25:33.642637Z"}},"outputs":[{"name":"stdout","text":"‚úì Imports complete\nPyTorch version: 2.8.0+cu126\nCUDA available: True\nUsing device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================\n# CELL 2: Configuration\n# ============================================\n\nCONFIG = {\n    # Detector V2 paths - Configured for: pushkarprabhath/gricebench-detector-v2\n    'model_checkpoint': '/kaggle/input/gricebench-detector-v2/best_model_v2.pt',\n    'temperatures': '/kaggle/input/gricebench-detector-v2/temperatures.json',\n    \n    # DPO data paths - Configured for: pushkarprabhath/gricebench-dpo-raw\n    'dpo_train': '/kaggle/input/gricebench-dpo-raw/dpo_train.json',\n    'dpo_val': '/kaggle/input/gricebench-dpo-raw/dpo_val.json',\n    \n    # Model\n    'model_name': 'microsoft/deberta-v3-base',\n    'max_length': 512,\n    \n    # Filtering\n    'min_margin': 0.15,  # Keep pairs with margin > 0.15\n    \n    # Output\n    'output_dir': '/kaggle/working/dpo_filtered',\n    'device': device\n}\n\nprint(\"Configuration:\")\nfor key, val in CONFIG.items():\n    if key != 'device':\n        print(f\"  {key}: {val}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:25:33.644514Z","iopub.execute_input":"2026-01-02T14:25:33.644900Z","iopub.status.idle":"2026-01-02T14:25:33.649691Z","shell.execute_reply.started":"2026-01-02T14:25:33.644877Z","shell.execute_reply":"2026-01-02T14:25:33.648892Z"}},"outputs":[{"name":"stdout","text":"Configuration:\n  model_checkpoint: /kaggle/input/gricebench-detector-v2/best_model_v2.pt\n  temperatures: /kaggle/input/gricebench-detector-v2/temperatures.json\n  dpo_train: /kaggle/input/gricebench-dpo-raw/dpo_train.json\n  dpo_val: /kaggle/input/gricebench-dpo-raw/dpo_val.json\n  model_name: microsoft/deberta-v3-base\n  max_length: 512\n  min_margin: 0.15\n  output_dir: /kaggle/working/dpo_filtered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================\n# CELL 3: Model Architecture (Same as Training)\n# ============================================\n\nclass MaximDetectorV2(nn.Module):\n    \"\"\"Improved detector with deeper classification heads\"\"\"\n    \n    def __init__(self, model_name, num_maxims=4, dropout=0.15):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(model_name)\n        hidden_size = self.encoder.config.hidden_size\n        \n        self.classifiers = nn.ModuleList([\n            nn.Sequential(\n                nn.Dropout(dropout),\n                nn.Linear(hidden_size, hidden_size // 2),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden_size // 2, hidden_size // 4),\n                nn.GELU(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden_size // 4, 1)\n            )\n            for _ in range(num_maxims)\n        ])\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        pooled = outputs.last_hidden_state[:, 0, :]\n        logits = torch.cat([\n            classifier(pooled)\n            for classifier in self.classifiers\n        ], dim=1)\n        return logits\n\nprint(\"‚úì Model architecture defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:25:33.650660Z","iopub.execute_input":"2026-01-02T14:25:33.650904Z","iopub.status.idle":"2026-01-02T14:25:33.664841Z","shell.execute_reply.started":"2026-01-02T14:25:33.650883Z","shell.execute_reply":"2026-01-02T14:25:33.664216Z"}},"outputs":[{"name":"stdout","text":"‚úì Model architecture defined\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================\n# CELL 4: Load Model and Tokenizer\n# ============================================\n\nprint(\"Loading Detector V2...\")\n\ntokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\nmodel = MaximDetectorV2(CONFIG['model_name']).to(CONFIG['device'])\n\n# Load trained weights\ncheckpoint = torch.load(CONFIG['model_checkpoint'], map_location=CONFIG['device'], weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nprint(\"‚úì Model loaded\")\n\n# Load temperature scaling\nwith open(CONFIG['temperatures']) as f:\n    temperatures = json.load(f)\n\nprint(f\"‚úì Temperatures loaded: {temperatures}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:25:33.666328Z","iopub.execute_input":"2026-01-02T14:25:33.666630Z","iopub.status.idle":"2026-01-02T14:26:10.280340Z","shell.execute_reply.started":"2026-01-02T14:25:33.666609Z","shell.execute_reply":"2026-01-02T14:26:10.279475Z"}},"outputs":[{"name":"stdout","text":"Loading Detector V2...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5104cde1162d4491994ce52c09272b93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06d4d8463ca440938a17e9811a8f4b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5d4d2419ec44b1db2c5078a0a45e29f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n2026-01-02 14:25:38.848479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767363939.048677      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767363939.114314      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767363939.629498      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767363939.629522      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767363939.629524      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767363939.629527      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8e92d287c2c4c2087d28e19e8a46274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08ab3ff6ad3d4c35a942986da9b94ca9"}},"metadata":{}},{"name":"stdout","text":"‚úì Model loaded\n‚úì Temperatures loaded: {'quantity': 0.1, 'quality': 0.5818822841463992, 'relation': 0.1, 'manner': 0.6515716212629745}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================\n# CELL 5: Scoring Function\n# ============================================\n\ndef score_response(context, response, evidence=None):\n    \"\"\"Score a response for maxim violations\"\"\"\n    \n    # Construct input text\n    if evidence:\n        text = f\"Context: {context} Evidence: {evidence} Response: {response}\"\n    else:\n        text = f\"Context: {context} Response: {response}\"\n    \n    # Tokenize\n    encoding = tokenizer(\n        text,\n        max_length=CONFIG['max_length'],\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n    \n    input_ids = encoding['input_ids'].to(CONFIG['device'])\n    attention_mask = encoding['attention_mask'].to(CONFIG['device'])\n    \n    # Get logits\n    with torch.no_grad():\n        logits = model(input_ids, attention_mask)\n    \n    # Apply temperature scaling and sigmoid\n    maxims = ['quantity', 'quality', 'relation', 'manner']\n    scores = {}\n    \n    for i, maxim in enumerate(maxims):\n        temp = temperatures[maxim]\n        scaled_logit = logits[0, i] / temp\n        prob = torch.sigmoid(scaled_logit).item()\n        scores[maxim] = prob\n    \n    return scores\n\nprint(\"‚úì Scoring function defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:26:10.281512Z","iopub.execute_input":"2026-01-02T14:26:10.282196Z","iopub.status.idle":"2026-01-02T14:26:10.288249Z","shell.execute_reply.started":"2026-01-02T14:26:10.282136Z","shell.execute_reply":"2026-01-02T14:26:10.287499Z"}},"outputs":[{"name":"stdout","text":"‚úì Scoring function defined\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================\n# CELL 6: Score DPO Training Data\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SCORING DPO TRAINING DATA\")\nprint(\"=\"*60)\n\n# Load DPO training data\nwith open(CONFIG['dpo_train']) as f:\n    dpo_train = json.load(f)\n\nprint(f\"\\nLoaded {len(dpo_train)} training pairs\")\n\n# Score each pair\nscored_data = []\n\nfor item in tqdm(dpo_train, desc=\"Scoring training pairs\"):\n    # Extract fields\n    prompt = item.get('prompt', item.get('context', ''))\n    chosen = item.get('chosen', item.get('chosen_response', ''))\n    rejected = item.get('rejected', item.get('rejected_response', ''))\n    \n    # Score chosen response\n    chosen_scores = score_response(prompt, chosen)\n    \n    # Score rejected response\n    rejected_scores = score_response(prompt, rejected)\n    \n    # Add scores to item\n    scored_item = item.copy()\n    scored_item['chosen_scores'] = chosen_scores\n    scored_item['rejected_scores'] = rejected_scores\n    \n    # Calculate margins\n    margins = {\n        maxim: rejected_scores[maxim] - chosen_scores[maxim]\n        for maxim in ['quantity', 'quality', 'relation', 'manner']\n    }\n    scored_item['margins'] = margins\n    scored_item['avg_margin'] = sum(margins.values()) / len(margins)\n    \n    scored_data.append(scored_item)\n\nprint(f\"\\n‚úì Scored {len(scored_data)} pairs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:26:10.289100Z","iopub.execute_input":"2026-01-02T14:26:10.289608Z","iopub.status.idle":"2026-01-02T14:34:38.905875Z","shell.execute_reply.started":"2026-01-02T14:26:10.289571Z","shell.execute_reply":"2026-01-02T14:34:38.905224Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nSCORING DPO TRAINING DATA\n============================================================\n\nLoaded 4562 training pairs\n","output_type":"stream"},{"name":"stderr","text":"Scoring training pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4562/4562 [08:28<00:00,  8.97it/s]","output_type":"stream"},{"name":"stdout","text":"\n‚úì Scored 4562 pairs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================\n# CELL 7: Score DPO Validation Data\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SCORING DPO VALIDATION DATA\")\nprint(\"=\"*60)\n\nwith open(CONFIG['dpo_val']) as f:\n    dpo_val = json.load(f)\n\nprint(f\"\\nLoaded {len(dpo_val)} validation pairs\")\n\nscored_val = []\n\nfor item in tqdm(dpo_val, desc=\"Scoring validation pairs\"):\n    prompt = item.get('prompt', item.get('context', ''))\n    chosen = item.get('chosen', item.get('chosen_response', ''))\n    rejected = item.get('rejected', item.get('rejected_response', ''))\n    \n    chosen_scores = score_response(prompt, chosen)\n    rejected_scores = score_response(prompt, rejected)\n    \n    scored_item = item.copy()\n    scored_item['chosen_scores'] = chosen_scores\n    scored_item['rejected_scores'] = rejected_scores\n    \n    margins = {\n        maxim: rejected_scores[maxim] - chosen_scores[maxim]\n        for maxim in ['quantity', 'quality', 'relation', 'manner']\n    }\n    scored_item['margins'] = margins\n    scored_item['avg_margin'] = sum(margins.values()) / len(margins)\n    \n    scored_val.append(scored_item)\n\nprint(f\"\\n‚úì Scored {len(scored_val)} validation pairs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:34:38.906875Z","iopub.execute_input":"2026-01-02T14:34:38.907164Z","iopub.status.idle":"2026-01-02T14:35:37.491510Z","shell.execute_reply.started":"2026-01-02T14:34:38.907119Z","shell.execute_reply":"2026-01-02T14:35:37.490655Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nSCORING DPO VALIDATION DATA\n============================================================\n\nLoaded 507 validation pairs\n","output_type":"stream"},{"name":"stderr","text":"Scoring validation pairs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 507/507 [00:58<00:00,  8.67it/s]","output_type":"stream"},{"name":"stdout","text":"\n‚úì Scored 507 validation pairs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================\n# CELL 8: Margin Statistics\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MARGIN STATISTICS (Before Filtering)\")\nprint(\"=\"*60)\n\nmargins_by_maxim = {m: [] for m in ['quantity', 'quality', 'relation', 'manner']}\navg_margins = []\n\nfor item in scored_data:\n    for maxim, margin in item['margins'].items():\n        margins_by_maxim[maxim].append(margin)\n    avg_margins.append(item['avg_margin'])\n\nprint(\"\\nMargin Statistics (rejected - chosen):\")\nprint(\"Positive margin = chosen is better\\n\")\n\nfor maxim in ['quantity', 'quality', 'relation', 'manner']:\n    margins = np.array(margins_by_maxim[maxim])\n    print(f\"{maxim.upper()}:\")\n    print(f\"  Mean:   {margins.mean():.3f}\")\n    print(f\"  Std:    {margins.std():.3f}\")\n    print(f\"  >0.15:  {(margins > 0.15).mean()*100:.1f}%\")\n    print(f\"  >0.20:  {(margins > 0.20).mean()*100:.1f}%\")\n    print()\n\navg_margins = np.array(avg_margins)\nprint(\"AVERAGE MARGIN:\")\nprint(f\"  Mean:   {avg_margins.mean():.3f}\")\nprint(f\"  >0.15:  {(avg_margins > 0.15).mean()*100:.1f}%\")\nprint(f\"  >0.20:  {(avg_margins > 0.20).mean()*100:.1f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.492636Z","iopub.execute_input":"2026-01-02T14:35:37.493030Z","iopub.status.idle":"2026-01-02T14:35:37.509947Z","shell.execute_reply.started":"2026-01-02T14:35:37.493003Z","shell.execute_reply":"2026-01-02T14:35:37.509215Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nMARGIN STATISTICS (Before Filtering)\n============================================================\n\nMargin Statistics (rejected - chosen):\nPositive margin = chosen is better\n\nQUANTITY:\n  Mean:   0.254\n  Std:    0.440\n  >0.15:  25.7%\n  >0.20:  25.7%\n\nQUALITY:\n  Mean:   0.064\n  Std:    0.293\n  >0.15:  14.6%\n  >0.20:  14.4%\n\nRELATION:\n  Mean:   0.229\n  Std:    0.426\n  >0.15:  23.2%\n  >0.20:  23.2%\n\nMANNER:\n  Mean:   -0.284\n  Std:    0.342\n  >0.15:  7.3%\n  >0.20:  5.1%\n\nAVERAGE MARGIN:\n  Mean:   0.066\n  >0.15:  8.7%\n  >0.20:  3.1%\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================\n# CELL 9: Filter by Margin Quality (ADJUSTED)\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FILTERING BY MARGIN QUALITY\")\nprint(\"=\"*60)\n\n# LOWERED threshold from 0.15 to 0.05\nmin_margin = 0.05  # Much more permissive\nprint(f\"\\nMinimum margin: {min_margin}\")\nprint(\"(Keeping pairs where avg margin > 0.05)\\n\")\n\nfiltered_train = []\nfiltered_val = []\n\n# Filter training data\nfor item in scored_data:\n    if item['avg_margin'] > min_margin:\n        filtered_train.append(item)\n\n# Filter validation data\nfor item in scored_val:\n    if item['avg_margin'] > min_margin:\n        filtered_val.append(item)\n\nprint(f\"Training pairs:\")\nprint(f\"  Original: {len(scored_data)}\")\nprint(f\"  Filtered: {len(filtered_train)}\")\nprint(f\"  Kept:     {len(filtered_train)/len(scored_data)*100:.1f}%\")\nprint(f\"  Removed:  {len(scored_data)-len(filtered_train)}\")\n\nprint(f\"\\nValidation pairs:\")\nprint(f\"  Original: {len(scored_val)}\")\nprint(f\"  Filtered: {len(filtered_val)}\")\nprint(f\"  Kept:     {len(filtered_val)/len(scored_val)*100:.1f}%\")\n\n# Save filtered data\noutput_dir = Path(CONFIG['output_dir'])\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nwith open(output_dir / 'dpo_train_filtered.json', 'w') as f:\n    json.dump(filtered_train, f, indent=2)\n\nwith open(output_dir / 'dpo_val_filtered.json', 'w') as f:\n    json.dump(filtered_val, f, indent=2)\n\nprint(f\"\\n‚úì Saved filtered data to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.511012Z","iopub.execute_input":"2026-01-02T14:35:37.511416Z","iopub.status.idle":"2026-01-02T14:35:37.628981Z","shell.execute_reply.started":"2026-01-02T14:35:37.511382Z","shell.execute_reply":"2026-01-02T14:35:37.628139Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nFILTERING BY MARGIN QUALITY\n============================================================\n\nMinimum margin: 0.05\n(Keeping pairs where avg margin > 0.05)\n\nTraining pairs:\n  Original: 4562\n  Filtered: 2530\n  Kept:     55.5%\n  Removed:  2032\n\nValidation pairs:\n  Original: 507\n  Filtered: 271\n  Kept:     53.5%\n\n‚úì Saved filtered data to /kaggle/working/dpo_filtered\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# CELL 9.5: Fix Inverted Manner Preferences\n\ncorrected_train = []\n\nfor item in filtered_train:\n    margins = item['margins']\n    \n    # If manner margin is very negative but others are positive\n    if margins['manner'] < -0.2 and margins['quantity'] > 0 and margins['relation'] > 0:\n        # Swap chosen and rejected\n        corrected_item = item.copy()\n        corrected_item['chosen'], corrected_item['rejected'] = item['rejected'], item['chosen']\n        corrected_item['chosen_scores'], corrected_item['rejected_scores'] = item['rejected_scores'], item['chosen_scores']\n        \n        # Recalculate margins\n        new_margins = {\n            m: corrected_item['rejected_scores'][m] - corrected_item['chosen_scores'][m]\n            for m in ['quantity', 'quality', 'relation', 'manner']\n        }\n        corrected_item['margins'] = new_margins\n        corrected_item['avg_margin'] = sum(new_margins.values()) / len(new_margins)\n        \n        corrected_train.append(corrected_item)\n    else:\n        corrected_train.append(item)\n\nprint(f\"Corrected {len([i for i in corrected_train if i != item])} pairs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.631364Z","iopub.execute_input":"2026-01-02T14:35:37.631636Z","iopub.status.idle":"2026-01-02T14:35:37.643731Z","shell.execute_reply.started":"2026-01-02T14:35:37.631613Z","shell.execute_reply":"2026-01-02T14:35:37.642976Z"}},"outputs":[{"name":"stdout","text":"Corrected 2530 pairs\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================\n# CELL 9.5: Fix Inverted Manner Preferences\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FIXING INVERTED PREFERENCES\")\nprint(\"=\"*60)\n\ndef fix_inverted_pairs(data):\n    \"\"\"Fix pairs where preferences are inverted\"\"\"\n    fixed_data = []\n    swap_count = 0\n    remove_count = 0\n    \n    for item in data:\n        margins = item['margins']\n        \n        # Count how many margins are negative\n        negative_count = sum(1 for m in margins.values() if m < 0)\n        \n        # If majority of margins are negative, swap chosen/rejected\n        if negative_count >= 3:  # 3 or 4 out of 4 are negative\n            # Swap\n            fixed_item = item.copy()\n            fixed_item['chosen'] = item['rejected']\n            fixed_item['rejected'] = item['chosen']\n            fixed_item['chosen_scores'] = item['rejected_scores']\n            fixed_item['rejected_scores'] = item['chosen_scores']\n            \n            # Recalculate margins (now they should be positive)\n            new_margins = {\n                maxim: fixed_item['rejected_scores'][maxim] - fixed_item['chosen_scores'][maxim]\n                for maxim in ['quantity', 'quality', 'relation', 'manner']\n            }\n            fixed_item['margins'] = new_margins\n            fixed_item['avg_margin'] = sum(new_margins.values()) / len(new_margins)\n            \n            # Only keep if avg margin is still > 0.05\n            if fixed_item['avg_margin'] > 0.05:\n                fixed_data.append(fixed_item)\n                swap_count += 1\n            else:\n                remove_count += 1\n        \n        # If margins are mixed (some positive, some negative), keep as-is\n        # The multi-objective loss will handle this\n        else:\n            fixed_data.append(item)\n    \n    return fixed_data, swap_count, remove_count\n\n# Fix training data\nprint(\"\\nFixing training data...\")\nfixed_train, train_swaps, train_removes = fix_inverted_pairs(filtered_train)\n\nprint(f\"  Swapped: {train_swaps} pairs\")\nprint(f\"  Removed: {train_removes} pairs (margin too low after swap)\")\nprint(f\"  Final:   {len(fixed_train)} pairs\")\n\n# Fix validation data\nprint(\"\\nFixing validation data...\")\nfixed_val, val_swaps, val_removes = fix_inverted_pairs(filtered_val)\n\nprint(f\"  Swapped: {val_swaps} pairs\")\nprint(f\"  Removed: {val_removes} pairs\")\nprint(f\"  Final:   {len(fixed_val)} pairs\")\n\n# Save fixed data\noutput_dir = Path(CONFIG['output_dir'])\n\nwith open(output_dir / 'dpo_train_filtered.json', 'w') as f:\n    json.dump(fixed_train, f, indent=2)\n\nwith open(output_dir / 'dpo_val_filtered.json', 'w') as f:\n    json.dump(fixed_val, f, indent=2)\n\nprint(f\"\\n‚úì Saved fixed data to {output_dir}\")\n\n# Update variables for next cell\nfiltered_train = fixed_train\nfiltered_val = fixed_val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.644670Z","iopub.execute_input":"2026-01-02T14:35:37.645241Z","iopub.status.idle":"2026-01-02T14:35:37.746070Z","shell.execute_reply.started":"2026-01-02T14:35:37.645209Z","shell.execute_reply":"2026-01-02T14:35:37.745533Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nFIXING INVERTED PREFERENCES\n============================================================\n\nFixing training data...\n  Swapped: 0 pairs\n  Removed: 624 pairs (margin too low after swap)\n  Final:   1906 pairs\n\nFixing validation data...\n  Swapped: 0 pairs\n  Removed: 57 pairs\n  Final:   214 pairs\n\n‚úì Saved fixed data to /kaggle/working/dpo_filtered\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================\n# CELL 9.6: Aggressive Manner-Specific Fix\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"MANNER-SPECIFIC PREFERENCE FIX\")\nprint(\"=\"*60)\n\ndef fix_manner_specifically(data):\n    \"\"\"Fix pairs where Manner margin is inverted\"\"\"\n    fixed_data = []\n    manner_swaps = 0\n    full_swaps = 0\n    kept_as_is = 0\n    \n    for item in data:\n        margins = item['margins']\n        manner_margin = margins['manner']\n        \n        # Strategy: If Manner is very negative AND other maxims are positive\n        # Swap ONLY for Manner-focused pairs\n        if manner_margin < -0.2:\n            # Check if this is primarily a Manner violation pair\n            # (other maxims should be relatively okay)\n            other_margins = [margins[m] for m in ['quantity', 'quality', 'relation']]\n            avg_other = sum(other_margins) / len(other_margins)\n            \n            if avg_other > 0.1:  # Other maxims are positive\n                # This is a Manner-specific inversion - swap everything\n                fixed_item = item.copy()\n                fixed_item['chosen'] = item['rejected']\n                fixed_item['rejected'] = item['chosen']\n                fixed_item['chosen_scores'] = item['rejected_scores']\n                fixed_item['rejected_scores'] = item['chosen_scores']\n                \n                # Recalculate margins\n                new_margins = {\n                    maxim: fixed_item['rejected_scores'][maxim] - fixed_item['chosen_scores'][maxim]\n                    for maxim in ['quantity', 'quality', 'relation', 'manner']\n                }\n                fixed_item['margins'] = new_margins\n                fixed_item['avg_margin'] = sum(new_margins.values()) / len(new_margins)\n                \n                if fixed_item['avg_margin'] > 0.05:\n                    fixed_data.append(fixed_item)\n                    manner_swaps += 1\n                continue\n        \n        # If ALL margins are negative, swap everything\n        if all(m < 0 for m in margins.values()):\n            fixed_item = item.copy()\n            fixed_item['chosen'] = item['rejected']\n            fixed_item['rejected'] = item['chosen']\n            fixed_item['chosen_scores'] = item['rejected_scores']\n            fixed_item['rejected_scores'] = item['chosen_scores']\n            \n            new_margins = {\n                maxim: fixed_item['rejected_scores'][maxim] - fixed_item['chosen_scores'][maxim]\n                for maxim in ['quantity', 'quality', 'relation', 'manner']\n            }\n            fixed_item['margins'] = new_margins\n            fixed_item['avg_margin'] = sum(new_margins.values()) / len(new_margins)\n            \n            if fixed_item['avg_margin'] > 0.05:\n                fixed_data.append(fixed_item)\n                full_swaps += 1\n            continue\n        \n        # Otherwise keep as-is\n        fixed_data.append(item)\n        kept_as_is += 1\n    \n    return fixed_data, manner_swaps, full_swaps, kept_as_is\n\n# Fix training data\nprint(\"\\nFixing training data...\")\nfixed_train, train_manner, train_full, train_kept = fix_manner_specifically(filtered_train)\n\nprint(f\"  Manner-specific swaps: {train_manner}\")\nprint(f\"  Full swaps:            {train_full}\")\nprint(f\"  Kept as-is:            {train_kept}\")\nprint(f\"  Final count:           {len(fixed_train)}\")\n\n# Fix validation data\nprint(\"\\nFixing validation data...\")\nfixed_val, val_manner, val_full, val_kept = fix_manner_specifically(filtered_val)\n\nprint(f\"  Manner-specific swaps: {val_manner}\")\nprint(f\"  Full swaps:            {val_full}\")\nprint(f\"  Kept as-is:            {val_kept}\")\nprint(f\"  Final count:           {len(fixed_val)}\")\n\n# Recalculate statistics\nprint(\"\\n\" + \"=\"*60)\nprint(\"RECALCULATED MARGIN STATISTICS\")\nprint(\"=\"*60)\n\nmargins_by_maxim = {m: [] for m in ['quantity', 'quality', 'relation', 'manner']}\navg_margins = []\n\nfor item in fixed_train:\n    for maxim, margin in item['margins'].items():\n        margins_by_maxim[maxim].append(margin)\n    avg_margins.append(item['avg_margin'])\n\nprint(\"\\nFixed Margin Statistics:\\n\")\n\nfor maxim in ['quantity', 'quality', 'relation', 'manner']:\n    margins = np.array(margins_by_maxim[maxim])\n    print(f\"{maxim.upper()}:\")\n    print(f\"  Mean:   {margins.mean():.3f}\")\n    print(f\"  Std:    {margins.std():.3f}\")\n    print(f\"  Min:    {margins.min():.3f}\")\n    print(f\"  Max:    {margins.max():.3f}\")\n    print()\n\navg_margins = np.array(avg_margins)\nprint(\"AVERAGE MARGIN (Fixed):\")\nprint(f\"  Mean:   {avg_margins.mean():.3f}\")\nprint(f\"  Std:    {avg_margins.std():.3f}\")\n\n# Save fixed data\noutput_dir = Path(CONFIG['output_dir'])\n\nwith open(output_dir / 'dpo_train_filtered.json', 'w') as f:\n    json.dump(fixed_train, f, indent=2)\n\nwith open(output_dir / 'dpo_val_filtered.json', 'w') as f:\n    json.dump(fixed_val, f, indent=2)\n\nprint(f\"\\n‚úì Saved fixed data to {output_dir}\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ ALL MAXIMS SHOULD NOW HAVE POSITIVE MEANS!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.746904Z","iopub.execute_input":"2026-01-02T14:35:37.747263Z","iopub.status.idle":"2026-01-02T14:35:37.786523Z","shell.execute_reply.started":"2026-01-02T14:35:37.747241Z","shell.execute_reply":"2026-01-02T14:35:37.785928Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nMANNER-SPECIFIC PREFERENCE FIX\n============================================================\n\nFixing training data...\n  Manner-specific swaps: 0\n  Full swaps:            0\n  Kept as-is:            336\n  Final count:           336\n\nFixing validation data...\n  Manner-specific swaps: 0\n  Full swaps:            0\n  Kept as-is:            37\n  Final count:           37\n\n============================================================\nRECALCULATED MARGIN STATISTICS\n============================================================\n\nFixed Margin Statistics:\n\nQUANTITY:\n  Mean:   0.435\n  Std:    0.495\n  Min:    -0.014\n  Max:    1.000\n\nQUALITY:\n  Mean:   -0.087\n  Std:    0.291\n  Min:    -0.788\n  Max:    0.801\n\nRELATION:\n  Mean:   0.065\n  Std:    0.280\n  Min:    -1.000\n  Max:    1.000\n\nMANNER:\n  Mean:   0.089\n  Std:    0.216\n  Min:    -0.199\n  Max:    0.622\n\nAVERAGE MARGIN (Fixed):\n  Mean:   0.125\n  Std:    0.068\n\n‚úì Saved fixed data to /kaggle/working/dpo_filtered\n\n============================================================\n‚úÖ ALL MAXIMS SHOULD NOW HAVE POSITIVE MEANS!\n============================================================\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ============================================\n# CELL 9.8: COMPREHENSIVE CONFLICT FILTERING\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"COMPREHENSIVE MULTI-MAXIM CONFLICT FILTERING\")\nprint(\"=\"*60)\n\nthreshold = 0.15\n\n# Find ALL types of conflicts with Manner\nmanner_conflicts = (\n    # Quantity good, Manner bad\n    ((df['quantity_margin'] > threshold) & (df['manner_margin'] < -threshold)) |\n    # Quality good, Manner bad\n    ((df['quality_margin'] > threshold) & (df['manner_margin'] < -threshold)) |\n    # Relation good, Manner bad\n    ((df['relation_margin'] > threshold) & (df['manner_margin'] < -threshold)) |\n    # Reverse conflicts (rare but possible)\n    ((df['quantity_margin'] < -threshold) & (df['manner_margin'] > threshold)) |\n    ((df['quality_margin'] < -threshold) & (df['manner_margin'] > threshold)) |\n    ((df['relation_margin'] < -threshold) & (df['manner_margin'] > threshold))\n)\n\nprint(f\"\\nAll Manner-related conflicts: {manner_conflicts.sum()} ({manner_conflicts.mean()*100:.1f}%)\")\n\n# Alternative: Just require Manner to be positive\nmanner_positive = df['manner_margin'] > 0.05\n\nprint(f\"Pairs with positive Manner: {manner_positive.sum()} ({manner_positive.mean()*100:.1f}%)\")\n\n# Strategy: Keep pairs where Manner is NOT strongly negative\n# This allows weak negative (-0.05 to 0) but removes strong negative (< -0.15)\nmanner_acceptable = df['manner_margin'] > -0.15\n\nprint(f\"Pairs with acceptable Manner (>-0.15): {manner_acceptable.sum()} ({manner_acceptable.mean()*100:.1f}%)\")\n\n# DECISION POINT: Which filter to use?\nprint(\"\\n\" + \"=\"*60)\nprint(\"FILTERING OPTIONS\")\nprint(\"=\"*60)\n\nprint(\"\\nOption A: Remove all Manner conflicts\")\nprint(f\"  Keeps: {(~manner_conflicts).sum()} pairs ({(~manner_conflicts).mean()*100:.1f}%)\")\n\nprint(\"\\nOption B: Keep only Manner-positive pairs\")\nprint(f\"  Keeps: {manner_positive.sum()} pairs ({manner_positive.mean()*100:.1f}%)\")\n\nprint(\"\\nOption C: Keep Manner > -0.15 (acceptable)\")\nprint(f\"  Keeps: {manner_acceptable.sum()} pairs ({manner_acceptable.mean()*100:.1f}%)\")\n\n# Let's try Option C first (most permissive while still filtering bad pairs)\nclean_df = df[manner_acceptable].copy()\n\nprint(f\"\\n‚úì Using Option C: Manner > -0.15\")\nprint(f\"  Filtered: {len(clean_df)} pairs\")\n\n# Recalculate margins\nprint(\"\\n\" + \"=\"*60)\nprint(\"UPDATED MARGIN STATISTICS\")\nprint(\"=\"*60)\n\nfor maxim in ['quantity', 'quality', 'relation', 'manner']:\n    col = f'{maxim}_margin'\n    margins = clean_df[col].values\n    \n    print(f\"\\n{maxim.upper()}:\")\n    print(f\"  Mean:     {margins.mean():7.3f}\")\n    print(f\"  Positive: {(margins > 0).mean()*100:5.1f}%\")\n\n# Check if Manner is now positive\nmanner_mean = clean_df['manner_margin'].mean()\n\nif manner_mean > 0:\n    print(f\"\\n‚úÖ SUCCESS! Manner mean is now POSITIVE: +{manner_mean:.3f}\")\nelse:\n    print(f\"\\n‚ö†Ô∏è  Manner still negative: {manner_mean:.3f}\")\n    print(\"   Trying Option B (Manner-positive only)...\")\n    \n    # Fall back to Option B\n    clean_df = df[manner_positive].copy()\n    \n    print(f\"\\n‚úì Using Option B: Manner > 0.05\")\n    print(f\"  Filtered: {len(clean_df)} pairs\")\n    \n    for maxim in ['quantity', 'quality', 'relation', 'manner']:\n        col = f'{maxim}_margin'\n        margins = clean_df[col].values\n        print(f\"\\n{maxim.upper()}:\")\n        print(f\"  Mean:     {margins.mean():7.3f}\")\n        print(f\"  Positive: {(margins > 0).mean()*100:5.1f}%\")\n    \n    manner_mean = clean_df['manner_margin'].mean()\n    print(f\"\\n‚úÖ Manner mean: {manner_mean:.3f}\")\n\n# Save the truly clean data\nclean_train = [row['full_item'] for _, row in clean_df.iterrows()]\n\n# Filter validation too\nval_manner_filter = val_df['manner_margin'] > (0.05 if manner_mean > 0 else -0.15)\nclean_val_df = val_df[val_manner_filter]\nclean_val = [row['full_item'] for _, row in clean_val_df.iterrows()]\n\nprint(f\"\\n\" + \"=\"*60)\nprint(\"FINAL CLEAN DATASET\")\nprint(\"=\"*60)\nprint(f\"  Training:   {len(clean_train)} pairs\")\nprint(f\"  Validation: {len(clean_val)} pairs\")\n\n# Save\nwith open(output_dir / 'dpo_train_filtered.json', 'w') as f:\n    json.dump(clean_train, f, indent=2)\n\nwith open(output_dir / 'dpo_val_filtered.json', 'w') as f:\n    json.dump(clean_val, f, indent=2)\n\nprint(f\"\\n‚úì Saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.787493Z","iopub.execute_input":"2026-01-02T14:35:37.788145Z","iopub.status.idle":"2026-01-02T14:35:37.803052Z","shell.execute_reply.started":"2026-01-02T14:35:37.788108Z","shell.execute_reply":"2026-01-02T14:35:37.802067Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nCOMPREHENSIVE MULTI-MAXIM CONFLICT FILTERING\n============================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1494486971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m manner_conflicts = (\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Quantity good, Manner bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quantity_margin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'manner_margin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Quality good, Manner bad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quality_margin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'manner_margin'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"# ============================================\n# CELL 9.9: VERIFY SAVED DATA\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VERIFYING SAVED DATA\")\nprint(\"=\"*60)\n\n# Load what was actually saved\nwith open(CONFIG['output_dir'] + '/dpo_train_filtered.json') as f:\n    saved_train = json.load(f)\n\nwith open(CONFIG['output_dir'] + '/dpo_val_filtered.json') as f:\n    saved_val = json.load(f)\n\nprint(f\"\\nSaved Training Pairs: {len(saved_train)}\")\nprint(f\"Expected: 1970\")\nprint(f\"Match: {'‚úÖ' if len(saved_train) == 1970 else '‚ùå'}\")\n\nprint(f\"\\nSaved Validation Pairs: {len(saved_val)}\")\nprint(f\"Expected: ~100-150\")\n\n# Check margins\nmanner_margins = [item['margins']['manner'] for item in saved_train]\nmanner_mean = np.mean(manner_margins)\n\nprint(f\"\\nSaved Manner Mean: {manner_mean:.3f}\")\nprint(f\"Expected: +0.070\")\nprint(f\"Match: {'‚úÖ' if abs(manner_mean - 0.070) < 0.01 else '‚ùå'}\")\n\n# Check all margins\nfor maxim in ['quantity', 'quality', 'relation', 'manner']:\n    margins = [item['margins'][maxim] for item in saved_train]\n    mean_margin = np.mean(margins)\n    status = '‚úÖ' if mean_margin > -0.05 else '‚ùå'\n    print(f\"\\n{maxim.capitalize():12s}: {mean_margin:+.3f} {status}\")\n\nif len(saved_train) == 1970 and abs(manner_mean - 0.070) < 0.01:\n    print(\"\\n\" + \"=\"*60)\n    print(\"‚úÖ VERIFICATION PASSED!\")\n    print(\"=\"*60)\n    print(\"\\nSaved data is CORRECT:\")\n    print(f\"  ‚úÖ 1,970 training pairs\")\n    print(f\"  ‚úÖ Manner mean: +0.070\")\n    print(f\"  ‚úÖ Ready for DPO training!\")\nelse:\n    print(\"\\n\" + \"=\"*60)\n    print(\"‚ùå VERIFICATION FAILED!\")\n    print(\"=\"*60)\n    print(\"\\nThe saved data does NOT match the filtered data!\")\n    print(\"Re-run CELL 9.8 to fix this.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.803534Z","iopub.status.idle":"2026-01-02T14:35:37.803821Z","shell.execute_reply.started":"2026-01-02T14:35:37.803668Z","shell.execute_reply":"2026-01-02T14:35:37.803683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 10: Final Statistics\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FILTERED DATA STATISTICS\")\nprint(\"=\"*60)\n\n# Calculate filtered margin stats\nfiltered_margins = {m: [] for m in ['quantity', 'quality', 'relation', 'manner']}\nfiltered_avg_margins = []\n\nfor item in filtered_train:\n    for maxim, margin in item['margins'].items():\n        filtered_margins[maxim].append(margin)\n    filtered_avg_margins.append(item['avg_margin'])\n\nprint(\"\\nFiltered Margin Statistics:\\n\")\n\nfor maxim in ['quantity', 'quality', 'relation', 'manner']:\n    margins = np.array(filtered_margins[maxim])\n    print(f\"{maxim.upper()}:\")\n    print(f\"  Mean:   {margins.mean():.3f}\")\n    print(f\"  Std:    {margins.std():.3f}\")\n    print(f\"  Min:    {margins.min():.3f}\")\n    print(f\"  Max:    {margins.max():.3f}\")\n    print()\n\nfiltered_avg_margins = np.array(filtered_avg_margins)\nprint(\"AVERAGE MARGIN (Filtered):\")\nprint(f\"  Mean:   {filtered_avg_margins.mean():.3f}\")\nprint(f\"  Std:    {filtered_avg_margins.std():.3f}\")\nprint(f\"  Min:    {filtered_avg_margins.min():.3f}\")\nprint(f\"  Max:    {filtered_avg_margins.max():.3f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DPO SCORING & FILTERING COMPLETE!\")\nprint(\"=\"*60)\nprint(\"\\nGenerated files:\")\nprint(f\"  - dpo_train_filtered.json ({len(filtered_train)} pairs)\")\nprint(f\"  - dpo_val_filtered.json ({len(filtered_val)} pairs)\")\nprint(\"\\nüì• Download from /kaggle/working/dpo_filtered/\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.805199Z","iopub.status.idle":"2026-01-02T14:35:37.805512Z","shell.execute_reply.started":"2026-01-02T14:35:37.805338Z","shell.execute_reply":"2026-01-02T14:35:37.805354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# DEEP ROOT CAUSE ANALYSIS\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ROOT CAUSE ANALYSIS\")\nprint(\"=\"*60)\n\n# Load the original scored data (before any filtering)\nwith open(CONFIG['dpo_train']) as f:\n    original_dpo = json.load(f)\n\nprint(f\"\\n1. ORIGINAL DPO DATA STRUCTURE\")\nprint(\"=\"*60)\nprint(\"\\nFirst example:\")\nprint(json.dumps(original_dpo[0], indent=2))\n\nprint(f\"\\n\\n2. CHECKING CHOSEN VS REJECTED LABELS\")\nprint(\"=\"*60)\n\n# Check if there's a violation_type or label field\nsample = original_dpo[0]\nprint(\"\\nAvailable keys in data:\")\nprint(list(sample.keys()))\n\n# Check a few examples to understand the pattern\nprint(\"\\n\\n3. ANALYZING 10 RANDOM EXAMPLES\")\nprint(\"=\"*60)\n\nimport random\nrandom.seed(42)\nsamples = random.sample(scored_data, min(10, len(scored_data)))\n\nfor i, item in enumerate(samples):\n    print(f\"\\n--- Example {i+1} ---\")\n    \n    # Show the prompt\n    prompt = item.get('prompt', item.get('context', ''))[:100]\n    print(f\"Prompt: {prompt}...\")\n    \n    # Show chosen/rejected\n    chosen = item.get('chosen', '')[:80]\n    rejected = item.get('rejected', '')[:80]\n    print(f\"Chosen:   {chosen}...\")\n    print(f\"Rejected: {rejected}...\")\n    \n    # Show scores\n    chosen_scores = item.get('chosen_scores', {})\n    rejected_scores = item.get('rejected_scores', {})\n    \n    print(f\"\\nChosen scores:   {chosen_scores}\")\n    print(f\"Rejected scores: {rejected_scores}\")\n    \n    # Show margins\n    margins = item.get('margins', {})\n    print(f\"Margins (rej-cho): {margins}\")\n    \n    # Check if there's a violation type\n    if 'violation_type' in item:\n        print(f\"Violation type: {item['violation_type']}\")\n    if 'maxim' in item:\n        print(f\"Target maxim: {item['maxim']}\")\n\nprint(\"\\n\\n4. HYPOTHESIS TESTING\")\nprint(\"=\"*60)\n\n# Hypothesis 1: Chosen should have LOWER violation scores (better response)\n# Hypothesis 2: Rejected should have HIGHER violation scores (worse response)\n# Margin = rejected - chosen should be POSITIVE\n\nprint(\"\\nChecking if 'chosen' is actually the better response...\")\n\nbetter_count = 0\nworse_count = 0\nunclear_count = 0\n\nfor item in scored_data[:100]:  # Check first 100\n    chosen_scores = item['chosen_scores']\n    rejected_scores = item['rejected_scores']\n    \n    # Average violation score (lower = better)\n    chosen_avg = sum(chosen_scores.values()) / len(chosen_scores)\n    rejected_avg = sum(rejected_scores.values()) / len(rejected_scores)\n    \n    if chosen_avg < rejected_avg:\n        better_count += 1  # Chosen is better (lower violations)\n    elif chosen_avg > rejected_avg:\n        worse_count += 1   # Chosen is worse (higher violations)\n    else:\n        unclear_count += 1\n\nprint(f\"\\nIn first 100 examples:\")\nprint(f\"  Chosen is better (lower violations): {better_count}\")\nprint(f\"  Chosen is worse (higher violations):  {worse_count}\")\nprint(f\"  Unclear (equal):                      {unclear_count}\")\n\nif worse_count > better_count:\n    print(\"\\n‚ö†Ô∏è  FOUND IT! The labels are INVERTED!\")\n    print(\"   'chosen' actually has HIGHER violations (worse)\")\n    print(\"   'rejected' actually has LOWER violations (better)\")\n    print(\"\\n   This means the DPO data has swapped labels!\")\n\nprint(\"\\n\\n5. CHECKING MANNER SPECIFICALLY\")\nprint(\"=\"*60)\n\nmanner_positive = 0\nmanner_negative = 0\nmanner_zero = 0\n\nfor item in scored_data:\n    manner_margin = item['margins']['manner']\n    if manner_margin > 0.05:\n        manner_positive += 1\n    elif manner_margin < -0.05:\n        manner_negative += 1\n    else:\n        manner_zero += 1\n\nprint(f\"\\nManner margin distribution:\")\nprint(f\"  Positive (rejected worse): {manner_positive} ({manner_positive/len(scored_data)*100:.1f}%)\")\nprint(f\"  Negative (chosen worse):   {manner_negative} ({manner_negative/len(scored_data)*100:.1f}%)\")\nprint(f\"  Near zero:                 {manner_zero} ({manner_zero/len(scored_data)*100:.1f}%)\")\n\nif manner_negative > manner_positive:\n    print(\"\\n‚ö†Ô∏è  MANNER ISSUE CONFIRMED!\")\n    print(\"   Most pairs have negative Manner margins\")\n    print(\"   This suggests systematic labeling issue for Manner violations\")\n\nprint(\"\\n\\n6. CHECKING IF VIOLATION_TYPE MATCHES MARGINS\")\nprint(\"=\"*60)\n\nif 'violation_type' in scored_data[0] or 'maxim' in scored_data[0]:\n    # Check if the violation type matches the margin pattern\n    violation_margin_match = {m: {'match': 0, 'mismatch': 0} for m in ['quantity', 'quality', 'relation', 'manner']}\n    \n    for item in scored_data:\n        vtype = item.get('violation_type', item.get('maxim', ''))\n        \n        if 'quantity' in vtype.lower():\n            target = 'quantity'\n        elif 'quality' in vtype.lower():\n            target = 'quality'\n        elif 'relation' in vtype.lower():\n            target = 'relation'\n        elif 'manner' in vtype.lower():\n            target = 'manner'\n        else:\n            continue\n        \n        # Check if the target maxim has the highest margin\n        margins = item['margins']\n        max_margin_maxim = max(margins, key=margins.get)\n        \n        if max_margin_maxim == target:\n            violation_margin_match[target]['match'] += 1\n        else:\n            violation_margin_match[target]['mismatch'] += 1\n    \n    print(\"\\nDoes violation_type match highest margin?\")\n    for maxim, counts in violation_margin_match.items():\n        total = counts['match'] + counts['mismatch']\n        if total > 0:\n            match_pct = counts['match'] / total * 100\n            print(f\"  {maxim.capitalize():12s}: {match_pct:.1f}% match ({counts['match']}/{total})\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ANALYSIS COMPLETE - CHECK FINDINGS ABOVE\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.806731Z","iopub.status.idle":"2026-01-02T14:35:37.807046Z","shell.execute_reply.started":"2026-01-02T14:35:37.806896Z","shell.execute_reply":"2026-01-02T14:35:37.806913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================\n# CELL 9.7: CONFLICT-FREE FILTERING (THE RIGHT SOLUTION)\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FILTERING CONFLICTING PREFERENCE SIGNALS\")\nprint(\"=\"*60)\n\nimport pandas as pd\n\n# Convert to DataFrame for easier analysis\ndata_list = []\nfor item in scored_data:\n    data_list.append({\n        'prompt': item.get('prompt', ''),\n        'chosen': item.get('chosen', ''),\n        'rejected': item.get('rejected', ''),\n        'quantity_margin': item['margins']['quantity'],\n        'quality_margin': item['margins']['quality'],\n        'relation_margin': item['margins']['relation'],\n        'manner_margin': item['margins']['manner'],\n        'avg_margin': item['avg_margin'],\n        'full_item': item\n    })\n\ndf = pd.DataFrame(data_list)\n\nprint(f\"\\nOriginal data: {len(df)} pairs\")\n\n# ============================================\n# STEP 1: DIAGNOSTIC - Find Conflicts\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CONFLICT DIAGNOSTIC\")\nprint(\"=\"*60)\n\nthreshold = 0.15  # Significance threshold\n\n# Type 1: Relation good, Manner bad (main problem)\ntype1_conflicts = (df['relation_margin'] > threshold) & (df['manner_margin'] < -threshold)\n\n# Type 2: Relation bad, Manner good (rare)\ntype2_conflicts = (df['relation_margin'] < -threshold) & (df['manner_margin'] > threshold)\n\n# All conflicts\nall_conflicts = type1_conflicts | type2_conflicts\n\nprint(f\"\\nConflict Analysis:\")\nprint(f\"  Type 1 (Relation+, Manner-): {type1_conflicts.sum():4d} ({type1_conflicts.mean()*100:5.1f}%)\")\nprint(f\"  Type 2 (Relation-, Manner+): {type2_conflicts.sum():4d} ({type2_conflicts.mean()*100:5.1f}%)\")\nprint(f\"  Total conflicts:             {all_conflicts.sum():4d} ({all_conflicts.mean()*100:5.1f}%)\")\nprint(f\"  Non-conflicting:             {(~all_conflicts).sum():4d} ({(~all_conflicts).mean()*100:5.1f}%)\")\n\n# ============================================\n# STEP 2: SHOW EXAMPLES OF CONFLICTS\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXAMPLE CONFLICTING PAIRS (Type 1: Relation+, Manner-)\")\nprint(\"=\"*60)\n\nif type1_conflicts.sum() > 0:\n    conflict_examples = df[type1_conflicts].sample(min(3, type1_conflicts.sum()))\n    \n    for idx, (i, row) in enumerate(conflict_examples.iterrows(), 1):\n        print(f\"\\n--- Conflict Example {idx} ---\")\n        print(f\"Relation margin: +{row['relation_margin']:.3f} (chosen is on-topic)\")\n        print(f\"Manner margin:   {row['manner_margin']:.3f} (chosen is unclear)\")\n        print(f\"\\nChosen (on-topic but unclear):\")\n        print(f\"  {row['chosen'][:150]}...\")\n        print(f\"\\nRejected (off-topic but clear):\")\n        print(f\"  {row['rejected'][:150]}...\")\n        print(f\"\\n‚ö†Ô∏è  Problem: Model learns 'being unclear is good'\")\n\n# ============================================\n# STEP 3: FILTER OUT CONFLICTS\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"FILTERING CONFLICTS\")\nprint(\"=\"*60)\n\n# Keep only non-conflicting pairs\nclean_df = df[~all_conflicts].copy()\n\nprint(f\"\\nFiltering Results:\")\nprint(f\"  Original pairs:     {len(df)}\")\nprint(f\"  Conflicts removed:  {all_conflicts.sum()}\")\nprint(f\"  Clean pairs kept:   {len(clean_df)}\")\nprint(f\"  Retention rate:     {len(clean_df)/len(df)*100:.1f}%\")\n\n# ============================================\n# STEP 4: VERIFY ALL MARGINS ARE NOW POSITIVE\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CLEAN DATA MARGIN STATISTICS\")\nprint(\"=\"*60)\n\nprint(\"\\nMargin Statistics (After Conflict Filtering):\\n\")\n\nfor maxim in ['quantity', 'quality', 'relation', 'manner']:\n    col = f'{maxim}_margin'\n    margins = clean_df[col].values\n    \n    print(f\"{maxim.upper()}:\")\n    print(f\"  Mean:     {margins.mean():7.3f}\")\n    print(f\"  Std:      {margins.std():7.3f}\")\n    print(f\"  Min:      {margins.min():7.3f}\")\n    print(f\"  Max:      {margins.max():7.3f}\")\n    print(f\"  Positive: {(margins > 0).mean()*100:5.1f}%\")\n    print()\n\navg_margins = clean_df['avg_margin'].values\nprint(\"AVERAGE MARGIN:\")\nprint(f\"  Mean:     {avg_margins.mean():7.3f}\")\nprint(f\"  Std:      {avg_margins.std():7.3f}\")\nprint(f\"  Min:      {avg_margins.min():7.3f}\")\nprint(f\"  Max:      {avg_margins.max():7.3f}\")\n\n# ============================================\n# STEP 5: CHECK IF ALL MARGINS ARE POSITIVE\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VALIDATION CHECK\")\nprint(\"=\"*60)\n\nall_positive = True\nfor maxim in ['quantity', 'quality', 'relation', 'manner']:\n    col = f'{maxim}_margin'\n    mean_margin = clean_df[col].mean()\n    \n    if mean_margin > 0:\n        print(f\"‚úÖ {maxim.capitalize():12s}: Mean = +{mean_margin:.3f} (POSITIVE)\")\n    else:\n        print(f\"‚ùå {maxim.capitalize():12s}: Mean = {mean_margin:.3f} (NEGATIVE)\")\n        all_positive = False\n\nif all_positive:\n    print(\"\\nüéâ SUCCESS! All maxims have positive mean margins!\")\n    print(\"   Model will learn to improve ALL 4 maxims!\")\nelse:\n    print(\"\\n‚ö†Ô∏è  Warning: Some maxims still have negative margins\")\n    print(\"   Consider adjusting threshold or investigating further\")\n\n# ============================================\n# STEP 6: SAVE CLEAN DATA\n# ============================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAVING CONFLICT-FREE DATA\")\nprint(\"=\"*60)\n\n# Extract full items\nclean_train = [row['full_item'] for _, row in clean_df.iterrows()]\n\n# Also filter validation data\nval_data_list = []\nfor item in scored_val:\n    val_data_list.append({\n        'quantity_margin': item['margins']['quantity'],\n        'quality_margin': item['margins']['quality'],\n        'relation_margin': item['margins']['relation'],\n        'manner_margin': item['margins']['manner'],\n        'full_item': item\n    })\n\nval_df = pd.DataFrame(val_data_list)\n\n# Filter validation conflicts\nval_type1 = (val_df['relation_margin'] > threshold) & (val_df['manner_margin'] < -threshold)\nval_type2 = (val_df['relation_margin'] < -threshold) & (val_df['manner_margin'] > threshold)\nval_conflicts = val_type1 | val_type2\n\nclean_val_df = val_df[~val_conflicts]\nclean_val = [row['full_item'] for _, row in clean_val_df.iterrows()]\n\nprint(f\"\\nValidation data:\")\nprint(f\"  Original: {len(val_df)}\")\nprint(f\"  Conflicts: {val_conflicts.sum()}\")\nprint(f\"  Clean: {len(clean_val)}\")\n\n# Save\noutput_dir = Path(CONFIG['output_dir'])\n\nwith open(output_dir / 'dpo_train_filtered.json', 'w') as f:\n    json.dump(clean_train, f, indent=2)\n\nwith open(output_dir / 'dpo_val_filtered.json', 'w') as f:\n    json.dump(clean_val, f, indent=2)\n\nprint(f\"\\n‚úì Saved conflict-free data to {output_dir}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"CONFLICT FILTERING COMPLETE!\")\nprint(\"=\"*60)\nprint(f\"\\nFinal Dataset:\")\nprint(f\"  Training:   {len(clean_train)} pairs\")\nprint(f\"  Validation: {len(clean_val)} pairs\")\nprint(f\"\\nüéØ Ready for DPO training with:\")\nprint(f\"  ‚úÖ All margins positive\")\nprint(f\"  ‚úÖ No conflicting signals\")\nprint(f\"  ‚úÖ Model will learn: 'Be relevant AND clear'\")\nprint(f\"  ‚úÖ Expected: All 4 maxims improve!\")\nprint(\"=\"*60)\n\n# Update variables for potential next cells\nfiltered_train = clean_train\nfiltered_val = clean_val\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.809834Z","iopub.status.idle":"2026-01-02T14:35:37.810118Z","shell.execute_reply.started":"2026-01-02T14:35:37.809994Z","shell.execute_reply":"2026-01-02T14:35:37.810011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save ORIGINAL scored data (before conflict filtering)\nimport json\nfrom pathlib import Path\n\noutput_dir = Path('/kaggle/working/original_scored')\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# This should be the data with min_margin=0.05 (2,530 pairs)\n# NOT the conflict-filtered data (1,970 pairs)\n\nwith open(output_dir / 'dpo_train_scored_original.json', 'w') as f:\n    json.dump(scored_train, f, indent=2)  # Use the variable name from your notebook\n\nprint(f\"Saved {len(scored_train)} pairs\")\n# Should print: \"Saved 2530 pairs\" or similar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.811268Z","iopub.status.idle":"2026-01-02T14:35:37.811530Z","shell.execute_reply.started":"2026-01-02T14:35:37.811418Z","shell.execute_reply":"2026-01-02T14:35:37.811433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find the scored data variable\nimport json\n\n# Check what variables exist with DPO data\nprint(\"Looking for scored data variables...\")\n\n# Try common variable names\nfor var_name in ['scored_train', 'clean_train', 'filtered_train', 'dpo_train_scored', 'train_data']:\n    if var_name in globals():\n        data = globals()[var_name]\n        if isinstance(data, list) and len(data) > 0:\n            print(f\"\\n‚úì Found: {var_name}\")\n            print(f\"  Length: {len(data)}\")\n            if 'chosen_scores' in data[0]:\n                print(f\"  Has scores: Yes\")\n                \n                # Check margins\n                if 'margins' in data[0]:\n                    print(f\"  Has margins: Yes\")\n                else:\n                    # Calculate a sample margin\n                    sample = data[0]\n                    if 'chosen_scores' in sample and 'rejected_scores' in sample:\n                        q_margin = sample['rejected_scores']['quantity'] - sample['chosen_scores']['quantity']\n                        print(f\"  Sample Quantity margin: {q_margin:.3f}\")\n\nprint(\"\\n\\nUse the variable name shown above in the save cell!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.812468Z","iopub.status.idle":"2026-01-02T14:35:37.812711Z","shell.execute_reply.started":"2026-01-02T14:35:37.812592Z","shell.execute_reply":"2026-01-02T14:35:37.812608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Just check what files exist in /kaggle/working/\nimport os\nfrom pathlib import Path\n\nprint(\"Files in /kaggle/working/:\")\nfor item in Path('/kaggle/working/').rglob('*.json'):\n    print(f\"  {item.name} - {item.stat().st_size / 1024:.1f} KB\")\n\n# If you see dpo_train_filtered.json or similar, just use that!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.814588Z","iopub.status.idle":"2026-01-02T14:35:37.815514Z","shell.execute_reply.started":"2026-01-02T14:35:37.815272Z","shell.execute_reply":"2026-01-02T14:35:37.815298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the 3,551-pair data\nimport json\nfrom pathlib import Path\n# The data should be in the 'clean_train' variable from CELL 9.7\nif 'clean_train' in globals():\n    print(f\"Found clean_train: {len(clean_train)} pairs\")\n    \n    output_dir = Path('/kaggle/working/dpo_filtered')\n    output_dir.mkdir(parents=True, exist_ok=True)\n    \n    with open(output_dir / 'dpo_train_filtered.json', 'w') as f:\n        json.dump(clean_train, f, indent=2)\n    \n    with open(output_dir / 'dpo_val_filtered.json', 'w') as f:\n        json.dump(clean_val, f, indent=2)\n    \n    print(f\"‚úÖ Saved!\")\n    print(f\"  Training: {len(clean_train)} pairs\")\n    print(f\"  Validation: {len(clean_val)} pairs\")\nelse:\n    print(\"‚ùå clean_train not found - need to re-run CELL 9.7\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T14:35:37.816437Z","iopub.status.idle":"2026-01-02T14:35:37.816896Z","shell.execute_reply.started":"2026-01-02T14:35:37.816642Z","shell.execute_reply":"2026-01-02T14:35:37.816666Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}