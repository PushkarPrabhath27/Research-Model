{
    "nbformat": 4,
    "nbformat_minor": 4,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GriceBench Phase 2: Complete Critical Validation\n",
                "\n",
                "## What This Notebook Does\n",
                "\n",
                "This notebook executes **Phase 2: Critical Validation** from the morechanges.md plan:\n",
                "\n",
                "1. **Create Relation Evaluation Set** - Sample 200 Relation violation examples\n",
                "2. **MRR Evaluation** - Measure retrieval quality (Mean Reciprocal Rank)\n",
                "3. **Relevance Scoring** - Semantic similarity metrics\n",
                "4. **Create Annotation Sample** - Stratified 1000 examples for human annotation\n",
                "5. **Generate All Outputs** - Ready for Phase 3\n",
                "\n",
                "---\n",
                "\n",
                "## ‚ö†Ô∏è REQUIRED DATASET\n",
                "\n",
                "You need to add **ONE dataset** to this notebook:\n",
                "\n",
                "### Dataset: `gricebench-scientific-fix`\n",
                "\n",
                "**Files required in this dataset:**\n",
                "\n",
                "| File | Local Path on Your Computer |\n",
                "|------|-----------------------------|\n",
                "| `repair_test.json` | `c:\\Users\\pushk\\OneDrive\\Documents\\Research Model\\GriceBench\\data_processed\\repair_data\\repair_test.json` |\n",
                "| `gold_annotation_set.json` | `c:\\Users\\pushk\\OneDrive\\Documents\\Research Model\\GriceBench\\data_processed\\gold_annotation_set.json` |\n",
                "| `val_examples.json` | `c:\\Users\\pushk\\OneDrive\\Documents\\Research Model\\GriceBench\\data_processed\\val_examples.json` |\n",
                "| `topical_corpus.json` | `c:\\Users\\pushk\\OneDrive\\Documents\\Research Model\\GriceBench\\data_processed\\topical_corpus.json` |\n",
                "\n",
                "**How to add dataset:**\n",
                "1. Right panel ‚Üí Click \"Add Data\" button\n",
                "2. Search for your dataset: `gricebench-scientific-fix`\n",
                "3. Click \"Add\" to add it\n",
                "\n",
                "**Note:** The sentence-transformers model will be downloaded automatically - no need to add it as a dataset!\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è Settings\n",
                "\n",
                "**Recommended:**\n",
                "- GPU: Enable (Settings ‚Üí Accelerator ‚Üí GPU T4 x2)\n",
                "- Internet: ON (needed to download model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 1: INSTALL DEPENDENCIES\n",
                "# ============================================================================\n",
                "# This installs the sentence-transformers library for semantic similarity\n",
                "\n",
                "!pip install -q sentence-transformers\n",
                "\n",
                "print(\"‚úÖ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 2: IMPORTS AND CONFIGURATION\n",
                "# ============================================================================\n",
                "\n",
                "import os\n",
                "import json\n",
                "import numpy as np\n",
                "import random\n",
                "import re\n",
                "from pathlib import Path\n",
                "from typing import Dict, List\n",
                "from collections import defaultdict\n",
                "from datetime import datetime\n",
                "\n",
                "# Paths\n",
                "DATA_INPUT = Path(\"/kaggle/input/gricebench-scientific-fix\")\n",
                "OUTPUT_DIR = Path(\"/kaggle/working\")\n",
                "\n",
                "# Create output directory\n",
                "OUTPUT_DIR.mkdir(exist_ok=True)\n",
                "\n",
                "print(\"Configuration:\")\n",
                "print(f\"  Input: {DATA_INPUT}\")\n",
                "print(f\"  Output: {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 3: VERIFY DATASET\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"VERIFYING DATASET\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "required_files = [\n",
                "    \"repair_test.json\",           # OR repair_data/repair_test.json\n",
                "    \"gold_annotation_set.json\",\n",
                "    \"val_examples.json\",\n",
                "    \"topical_corpus.json\"\n",
                "]\n",
                "\n",
                "# Check if dataset is mounted\n",
                "if not DATA_INPUT.exists():\n",
                "    print(\"\\n‚ùå ERROR: Dataset not found!\")\n",
                "    print(\"\\nPlease add the 'gricebench-scientific-fix' dataset:\")\n",
                "    print(\"1. Click 'Add Data' in the right panel\")\n",
                "    print(\"2. Search for 'gricebench-scientific-fix'\")\n",
                "    print(\"3. Click 'Add'\")\n",
                "    print(\"4. Re-run this cell\")\n",
                "else:\n",
                "    print(f\"\\n‚úÖ Dataset found at: {DATA_INPUT}\")\n",
                "    print(\"\\nContents:\")\n",
                "    for item in DATA_INPUT.iterdir():\n",
                "        if item.is_file():\n",
                "            size_mb = item.stat().st_size / (1024*1024)\n",
                "            print(f\"  üìÑ {item.name} ({size_mb:.2f} MB)\")\n",
                "        else:\n",
                "            print(f\"  üìÅ {item.name}/\")\n",
                "            for subitem in item.iterdir():\n",
                "                size_mb = subitem.stat().st_size / (1024*1024)\n",
                "                print(f\"      üìÑ {subitem.name} ({size_mb:.2f} MB)\")\n",
                "\n",
                "# Find repair_test.json (could be in root or repair_data/)\n",
                "repair_test_path = None\n",
                "if (DATA_INPUT / \"repair_test.json\").exists():\n",
                "    repair_test_path = DATA_INPUT / \"repair_test.json\"\n",
                "elif (DATA_INPUT / \"repair_data\" / \"repair_test.json\").exists():\n",
                "    repair_test_path = DATA_INPUT / \"repair_data\" / \"repair_test.json\"\n",
                "\n",
                "if repair_test_path:\n",
                "    print(f\"\\n‚úÖ repair_test.json found at: {repair_test_path}\")\n",
                "else:\n",
                "    print(\"\\n‚ùå repair_test.json NOT FOUND - check your dataset\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 4: DOWNLOAD AND LOAD MODEL\n",
                "# ============================================================================\n",
                "# The model is downloaded automatically from HuggingFace - no dataset needed!\n",
                "\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"LOADING SENTENCE ENCODER\")\n",
                "print(\"=\" * 70)\n",
                "print(\"\\nDownloading model from HuggingFace (first run only)...\")\n",
                "print(\"Model: sentence-transformers/all-MiniLM-L6-v2\")\n",
                "print(\"This is a lightweight but effective model for semantic similarity.\\n\")\n",
                "\n",
                "encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                "\n",
                "print(\"\\n‚úÖ Model loaded successfully!\")\n",
                "print(f\"   Embedding dimension: 384\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 1: Create Relation Evaluation Set\n",
                "\n",
                "Sample 200 examples with Relation violations for MRR evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 5: CREATE RELATION EVALUATION SET\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"CREATING RELATION EVALUATION SET (200 examples)\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "random.seed(42)\n",
                "\n",
                "# Load repair test data\n",
                "print(f\"\\nLoading from: {repair_test_path}\")\n",
                "with open(repair_test_path, 'r', encoding='utf-8') as f:\n",
                "    test_data = json.load(f)\n",
                "\n",
                "print(f\"Total examples in file: {len(test_data)}\")\n",
                "\n",
                "# Filter for Relation violations\n",
                "relation_examples = []\n",
                "for i, item in enumerate(test_data):\n",
                "    input_text = item.get(\"input_text\", \"\")\n",
                "    if \"[VIOLATION=RELATION]\" in input_text:\n",
                "        example = {\n",
                "            \"id\": f\"relation_eval_{i}\",\n",
                "            \"input_text\": input_text,\n",
                "            \"target_text\": item.get(\"target_text\", \"\"),\n",
                "            \"source_index\": i\n",
                "        }\n",
                "        \n",
                "        # Extract context and response\n",
                "        context_match = re.search(r'\\[CONTEXT\\](.*?)\\[', input_text, re.DOTALL)\n",
                "        response_match = re.search(r'\\[RESPONSE\\](.*?)$', input_text, re.DOTALL)\n",
                "        \n",
                "        if context_match:\n",
                "            example[\"context\"] = context_match.group(1).strip()\n",
                "        if response_match:\n",
                "            example[\"response\"] = response_match.group(1).strip()\n",
                "        \n",
                "        relation_examples.append(example)\n",
                "\n",
                "print(f\"Relation violations found: {len(relation_examples)}\")\n",
                "\n",
                "# Sample 200\n",
                "num_samples = min(200, len(relation_examples))\n",
                "relation_eval_set = random.sample(relation_examples, num_samples)\n",
                "\n",
                "# Save\n",
                "eval_set_path = OUTPUT_DIR / \"relation_eval_set.json\"\n",
                "with open(eval_set_path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(relation_eval_set, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"\\n‚úÖ Saved {len(relation_eval_set)} examples to: {eval_set_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 2: Load Corpus for Retrieval\n",
                "\n",
                "Load and encode the topical corpus for MRR evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 6: LOAD AND ENCODE CORPUS\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"LOADING AND ENCODING CORPUS\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "# Load corpus\n",
                "corpus_path = DATA_INPUT / \"topical_corpus.json\"\n",
                "print(f\"\\nLoading corpus from: {corpus_path}\")\n",
                "\n",
                "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
                "    corpus = json.load(f)\n",
                "\n",
                "print(f\"Total corpus size: {len(corpus)}\")\n",
                "\n",
                "# Extract text responses\n",
                "if isinstance(corpus[0], dict):\n",
                "    corpus_responses = [item.get('response', str(item)) for item in corpus]\n",
                "else:\n",
                "    corpus_responses = corpus\n",
                "\n",
                "# Subsample for efficiency (10k is good balance of speed vs coverage)\n",
                "MAX_CORPUS = 10000\n",
                "if len(corpus_responses) > MAX_CORPUS:\n",
                "    random.seed(42)\n",
                "    corpus_sample = random.sample(corpus_responses, MAX_CORPUS)\n",
                "    print(f\"Subsampled to: {len(corpus_sample)} responses\")\n",
                "else:\n",
                "    corpus_sample = corpus_responses\n",
                "\n",
                "# Encode corpus\n",
                "print(\"\\nEncoding corpus responses (this may take a few minutes)...\")\n",
                "corpus_embeddings = encoder.encode(\n",
                "    corpus_sample,\n",
                "    convert_to_numpy=True,\n",
                "    normalize_embeddings=True,\n",
                "    show_progress_bar=True,\n",
                "    batch_size=64\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úÖ Corpus encoded!\")\n",
                "print(f\"   Shape: {corpus_embeddings.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 3: MRR Evaluation\n",
                "\n",
                "Mean Reciprocal Rank measures how well retrieval finds relevant responses:\n",
                "- For each context, retrieve top-10 from corpus\n",
                "- Find rank of semantically similar response\n",
                "- MRR = mean(1/rank)\n",
                "\n",
                "**Target:** MRR ‚â• 0.5 (per morechanges.md)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 7: MRR EVALUATION\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"MRR EVALUATION\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "mrr_scores = []\n",
                "top1_hits = 0\n",
                "top3_hits = 0\n",
                "top10_hits = 0\n",
                "\n",
                "print(f\"\\nEvaluating {len(relation_eval_set)} examples...\\n\")\n",
                "\n",
                "for i, item in enumerate(relation_eval_set):\n",
                "    if (i + 1) % 50 == 0:\n",
                "        print(f\"  Processed {i + 1}/{len(relation_eval_set)}\")\n",
                "    \n",
                "    # Get context\n",
                "    context = item.get('context', '')\n",
                "    if not context:\n",
                "        mrr_scores.append(0.0)\n",
                "        continue\n",
                "    \n",
                "    # Get true response (the on-topic reference)\n",
                "    true_response = item.get('target_text', item.get('response', ''))\n",
                "    \n",
                "    # Encode context for retrieval\n",
                "    context_embedding = encoder.encode(\n",
                "        [context],\n",
                "        convert_to_numpy=True,\n",
                "        normalize_embeddings=True\n",
                "    )[0]\n",
                "    \n",
                "    # Find top-10 from corpus based on context similarity\n",
                "    similarities = np.dot(corpus_embeddings, context_embedding)\n",
                "    top_indices = np.argsort(similarities)[-10:][::-1]\n",
                "    \n",
                "    # Encode true response for comparison\n",
                "    true_embedding = encoder.encode(\n",
                "        [true_response],\n",
                "        convert_to_numpy=True,\n",
                "        normalize_embeddings=True\n",
                "    )[0]\n",
                "    \n",
                "    # Find rank of semantically similar response\n",
                "    rank = None\n",
                "    for j, idx in enumerate(top_indices):\n",
                "        candidate_embedding = corpus_embeddings[idx]\n",
                "        sim_to_true = np.dot(candidate_embedding, true_embedding)\n",
                "        if sim_to_true > 0.7:  # Threshold for \"relevant\"\n",
                "            rank = j + 1\n",
                "            break\n",
                "    \n",
                "    if rank:\n",
                "        mrr_scores.append(1.0 / rank)\n",
                "        if rank == 1:\n",
                "            top1_hits += 1\n",
                "        if rank <= 3:\n",
                "            top3_hits += 1\n",
                "        if rank <= 10:\n",
                "            top10_hits += 1\n",
                "    else:\n",
                "        mrr_scores.append(0.0)\n",
                "\n",
                "# Calculate final metrics\n",
                "n = len(relation_eval_set)\n",
                "mrr = np.mean(mrr_scores)\n",
                "\n",
                "mrr_results = {\n",
                "    'mrr': float(mrr),\n",
                "    'top1_accuracy': top1_hits / n if n > 0 else 0,\n",
                "    'top3_accuracy': top3_hits / n if n > 0 else 0,\n",
                "    'top10_accuracy': top10_hits / n if n > 0 else 0,\n",
                "    'n_examples': n,\n",
                "    'timestamp': datetime.now().isoformat()\n",
                "}\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"MRR RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nMRR:          {mrr_results['mrr']:.4f}\")\n",
                "print(f\"Top-1:        {mrr_results['top1_accuracy']:.4f} ({top1_hits}/{n})\")\n",
                "print(f\"Top-3:        {mrr_results['top3_accuracy']:.4f} ({top3_hits}/{n})\")\n",
                "print(f\"Top-10:       {mrr_results['top10_accuracy']:.4f} ({top10_hits}/{n})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 8: VERDICT AND DECISION\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"VERDICT (per morechanges.md)\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "if mrr_results['mrr'] >= 0.7:\n",
                "    verdict = \"EXCELLENT\"\n",
                "    emoji = \"‚úÖ\"\n",
                "    action = \"Retrieval system is working well.\"\n",
                "    next_step = \"Proceed to Phase 3 (Annotation)\"\n",
                "elif mrr_results['mrr'] >= 0.5:\n",
                "    verdict = \"ACCEPTABLE\"\n",
                "    emoji = \"‚ö†Ô∏è\"\n",
                "    action = \"Retrieval acceptable but could be improved.\"\n",
                "    next_step = \"Proceed to Phase 3, consider upgrading model later\"\n",
                "else:\n",
                "    verdict = \"NEEDS IMPROVEMENT\"\n",
                "    emoji = \"‚ùå\"\n",
                "    action = \"Retrieval below threshold. Fix before proceeding.\"\n",
                "    next_step = \"Run improvement steps (use all-mpnet-base-v2 or expand corpus)\"\n",
                "\n",
                "print(f\"\\n{emoji} {verdict}\")\n",
                "print(f\"\\nAction: {action}\")\n",
                "print(f\"Next Step: {next_step}\")\n",
                "\n",
                "# Decision point\n",
                "print(\"\\n\" + \"-\" * 50)\n",
                "print(\"DECISION POINT:\")\n",
                "if mrr_results['mrr'] >= 0.5:\n",
                "    print(\"‚úÖ MRR >= 0.5: Continue with this notebook to create annotation sample\")\n",
                "else:\n",
                "    print(\"‚ùå MRR < 0.5: Stop here and fix retrieval first\")\n",
                "    print(\"   Options:\")\n",
                "    print(\"   1. Use better encoder: 'all-mpnet-base-v2'\")\n",
                "    print(\"   2. Expand corpus with more responses\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 9: SAVE MRR RESULTS\n",
                "# ============================================================================\n",
                "\n",
                "mrr_output_path = OUTPUT_DIR / \"relation_repair_mrr.json\"\n",
                "with open(mrr_output_path, 'w') as f:\n",
                "    json.dump(mrr_results, f, indent=2)\n",
                "\n",
                "print(f\"\\n‚úÖ MRR results saved to: {mrr_output_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 4: Create Annotation Sample (1000 examples)\n",
                "\n",
                "Creates stratified sample for human annotation per morechanges.md:\n",
                "- 200 per maxim (detector positives)\n",
                "- 200 clean (detector negatives)\n",
                "- 100 random"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 10: CREATE ANNOTATION SAMPLE\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"CREATING ANNOTATION SAMPLE (1000 examples)\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "random.seed(42)\n",
                "\n",
                "# Load data sources\n",
                "all_examples = []\n",
                "\n",
                "# Load validation data\n",
                "val_path = DATA_INPUT / \"val_examples.json\"\n",
                "if val_path.exists():\n",
                "    with open(val_path, 'r', encoding='utf-8') as f:\n",
                "        val_data = json.load(f)\n",
                "    print(f\"Validation data: {len(val_data)} examples\")\n",
                "    for i, item in enumerate(val_data):\n",
                "        item['source_file'] = 'validation'\n",
                "        item['source_idx'] = i\n",
                "    all_examples.extend(val_data)\n",
                "\n",
                "# Load gold annotation data\n",
                "gold_path = DATA_INPUT / \"gold_annotation_set.json\"\n",
                "if gold_path.exists():\n",
                "    with open(gold_path, 'r', encoding='utf-8') as f:\n",
                "        gold_data = json.load(f)\n",
                "    print(f\"Gold data: {len(gold_data)} examples\")\n",
                "    for i, item in enumerate(gold_data):\n",
                "        item['source_file'] = 'gold'\n",
                "        item['source_idx'] = i\n",
                "    all_examples.extend(gold_data)\n",
                "\n",
                "print(f\"\\nTotal pool: {len(all_examples)} examples\")\n",
                "\n",
                "# Categorize by maxim\n",
                "maxims = ['quantity', 'quality', 'relation', 'manner']\n",
                "detector_positives = defaultdict(list)\n",
                "detector_negatives = []\n",
                "\n",
                "for item in all_examples:\n",
                "    labels = item.get('labels', item.get('detector_predictions', {}))\n",
                "    has_violation = any(labels.get(m, 0) for m in maxims)\n",
                "    \n",
                "    if not has_violation:\n",
                "        detector_negatives.append(item)\n",
                "    else:\n",
                "        for maxim in maxims:\n",
                "            if labels.get(maxim, 0):\n",
                "                detector_positives[maxim].append(item)\n",
                "\n",
                "print(f\"\\nCategorization:\")\n",
                "print(f\"  Clean (no violations): {len(detector_negatives)}\")\n",
                "for maxim in maxims:\n",
                "    print(f\"  {maxim} positives: {len(detector_positives[maxim])}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 11: SAMPLE AND SAVE\n",
                "# ============================================================================\n",
                "\n",
                "# Sampling function\n",
                "final_sample = []\n",
                "seen_ids = set()\n",
                "\n",
                "def add_samples(pool, count, category):\n",
                "    global final_sample, seen_ids\n",
                "    shuffled = pool.copy()\n",
                "    random.shuffle(shuffled)\n",
                "    added = 0\n",
                "    for item in shuffled:\n",
                "        item_id = item.get('id', f\"{item.get('source_file', 'unk')}_{item.get('source_idx', 0)}\")\n",
                "        if item_id not in seen_ids:\n",
                "            item['annotation_category'] = category\n",
                "            item['sample_id'] = f\"sample_{len(final_sample)}\"\n",
                "            final_sample.append(item)\n",
                "            seen_ids.add(item_id)\n",
                "            added += 1\n",
                "            if added >= count:\n",
                "                break\n",
                "    return added\n",
                "\n",
                "print(\"Sampling...\")\n",
                "\n",
                "# 200 per maxim\n",
                "for maxim in maxims:\n",
                "    added = add_samples(detector_positives[maxim], 200, f\"{maxim}_positive\")\n",
                "    print(f\"  {maxim}_positive: {added}\")\n",
                "\n",
                "# 200 clean\n",
                "added = add_samples(detector_negatives, 200, \"clean\")\n",
                "print(f\"  clean: {added}\")\n",
                "\n",
                "# 100 random from remaining\n",
                "remaining = [item for item in all_examples \n",
                "             if item.get('id', f\"{item.get('source_file', '')}_{item.get('source_idx', 0)}\") not in seen_ids]\n",
                "added = add_samples(remaining, 100, \"random\")\n",
                "print(f\"  random: {added}\")\n",
                "\n",
                "print(f\"\\nTotal sampled: {len(final_sample)}\")\n",
                "\n",
                "# Shuffle and assign final IDs\n",
                "random.shuffle(final_sample)\n",
                "for i, item in enumerate(final_sample):\n",
                "    item['id'] = f\"annotation_{i:04d}\"\n",
                "\n",
                "# Save\n",
                "annotation_sample_path = OUTPUT_DIR / \"annotation_sample_1000.json\"\n",
                "with open(annotation_sample_path, 'w', encoding='utf-8') as f:\n",
                "    json.dump(final_sample, f, indent=2, ensure_ascii=False)\n",
                "\n",
                "print(f\"\\n‚úÖ Saved {len(final_sample)} examples to: {annotation_sample_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 5: Summary and Downloads"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CELL 12: FINAL SUMMARY\n",
                "# ============================================================================\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"PHASE 2 COMPLETE - SUMMARY\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "print(\"\\nüìä MRR RESULTS:\")\n",
                "print(f\"   MRR Score: {mrr_results['mrr']:.4f}\")\n",
                "print(f\"   Top-1 Accuracy: {mrr_results['top1_accuracy']:.2%}\")\n",
                "print(f\"   Verdict: {verdict}\")\n",
                "\n",
                "print(\"\\nüìÅ OUTPUT FILES (download these):\")\n",
                "outputs = [\n",
                "    OUTPUT_DIR / \"relation_eval_set.json\",\n",
                "    OUTPUT_DIR / \"relation_repair_mrr.json\",\n",
                "    OUTPUT_DIR / \"annotation_sample_1000.json\"\n",
                "]\n",
                "\n",
                "for output_file in outputs:\n",
                "    if output_file.exists():\n",
                "        size_kb = output_file.stat().st_size / 1024\n",
                "        print(f\"   ‚úÖ {output_file.name} ({size_kb:.1f} KB)\")\n",
                "    else:\n",
                "        print(f\"   ‚ùå {output_file.name} - NOT CREATED\")\n",
                "\n",
                "print(\"\\nüìã NEXT STEPS:\")\n",
                "print(\"   1. Download the 3 output files above\")\n",
                "print(\"   2. Add them back to your gricebench-scientific-fix dataset\")\n",
                "print(\"   3. For Phase 3: Use annotation_sample_1000.json for human annotation\")\n",
                "print(\"   4. After annotating, run Phase 4 notebook for agreement analysis\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"Done! üéâ\")\n",
                "print(\"=\" * 70)"
            ]
        }
    ]
}